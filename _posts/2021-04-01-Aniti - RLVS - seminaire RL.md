---
title: "ANITI's first Reinforcement Learning Virtual School"
description: My notes
toc: true
comments: true
layout: post
categories: [reinforcement learning]
image: images/RL.png
---



![](https://d1keuthy5s86c8.cloudfront.net/static/ems/upload/img/72947a097165dcd24a6f700e2f28d690.png)

[https://rlvs.aniti.fr/](https://rlvs.aniti.fr/)

Schedule is 

## RLVS schedule

This condensed schedule does not include class breaks and social events. Times are Central European Summer Time (UTC+2).

| Schedule   |             |                                                              |                                                              |
| ---------- | ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| March 25th | 9:00-9:10   | [Opening remarks](https://rl-vs.github.io/rlvs2021/opening.html) | [S. Gerchinovitz](https://rl-vs.github.io/rlvs2021/sebastien-gerchinovitz.html) |
|            | 9:10-9:30   | [RLVS Overview](https://rl-vs.github.io/rlvs2021/rlvs-overview.html) | [E. Rachelson](https://rl-vs.github.io/rlvs2021/emmanuel-rachelson.html) |
|            | 9:30-13:00  | [RL fundamentals](https://rl-vs.github.io/rlvs2021/rl-fundamentals.html) | [E. Rachelson](https://rl-vs.github.io/rlvs2021/emmanuel-rachelson.html) |
|            | 14:00-16:00 | [Introduction to Deep Learning](https://rl-vs.github.io/rlvs2021/deep-learning.html) | [D. Wilson](https://rl-vs.github.io/rlvs2021/dennis-wilson.html) |
|            | 16:30-17:30 | [Reward Processing Biases in Humans and RL Agents](https://rl-vs.github.io/rlvs2021/human-behavioral-agents.html) | [I. Rish](https://rl-vs.github.io/rlvs2021/irina-rish.html)  |
|            | 17:45-18:45 | [Introduction to Hierarchical Reinforcement Learning](https://rl-vs.github.io/rlvs2021/hierarchical.html) | [D. Precup](https://rl-vs.github.io/rlvs2021/doina-precup.html) |
| March 26th | 10:00-12:00 | [Stochastic bandits](https://rl-vs.github.io/rlvs2021/stochastic-bandits.html) | [T. Lattimore](https://rl-vs.github.io/rlvs2021/tor-lattimore.html) |
|            | 14:00-16:00 | [Monte Carlo Tree Search](https://rl-vs.github.io/rlvs2021/mcts.html) | [T. Lattimore](https://rl-vs.github.io/rlvs2021/tor-lattimore.html) |
|            | 16:30-17:30 | [Multi-armed bandits in clinical trials](https://rl-vs.github.io/rlvs2021/clinical.html) | [D. A. Berry](https://rl-vs.github.io/rlvs2021/donald-berry.html) |
| April 1st  | 9:00-15:00  | [Deep Q-Networks and its variants](https://rl-vs.github.io/rlvs2021/dqn.html) | [B. Piot](https://rl-vs.github.io/rlvs2021/bilal-piot.html), [C. Tallec](https://rl-vs.github.io/rlvs2021/corentin-tallec.html) |
|            | 15:15-16:15 | [Regularized MDPs](https://rl-vs.github.io/rlvs2021/regularized-mdps.html) | [M. Geist](https://rl-vs.github.io/rlvs2021/matthieu-geist.html) |
|            | 16:30-17:30 | [Regret bounds of model-based reinforcement learning](https://rl-vs.github.io/rlvs2021/regret-bound.html) | [M. Wang](https://rl-vs.github.io/rlvs2021/mengdi-wang.html) |
| April 2nd  | 9:00-12:30  | [Policy Gradients and Actor Critic methods](https://rl-vs.github.io/rlvs2021/pg.html) | [O. Sigaud](https://rl-vs.github.io/rlvs2021/olivier-sigaud.html) |
|            | 14:00-15:00 | [Pitfalls in Policy Gradient methods](https://rl-vs.github.io/rlvs2021/pg-pitfalls.html) | [O. Sigaud](https://rl-vs.github.io/rlvs2021/olivier-sigaud.html) |
|            | 15:30-17:30 | [Exploration in Deep RL](https://rl-vs.github.io/rlvs2021/exploration.html) | [M. Pirotta](https://rl-vs.github.io/rlvs2021/matteo-pirotta.html) |
| April 8th  | 9:00-11:00  | [Evolutionary Reinforcement Learning](https://rl-vs.github.io/rlvs2021/evo-rl.html) | [D. Wilson](https://rl-vs.github.io/rlvs2021/dennis-wilson.html), [J.-B. Mouret](https://rl-vs.github.io/rlvs2021/jean-baptiste-mouret.html) |
|            | 11:30-12:30 | [Evolving Agents that Learn More Like Animals](https://rl-vs.github.io/rlvs2021/evolving-agents.html) | [S. Risi](https://rl-vs.github.io/rlvs2021/sebastian-risi.html) |
|            | 14:00-16:00 | [Micro-data Policy Search](https://rl-vs.github.io/rlvs2021/micro-data.html) | [K. Chatzilygeroudis](https://rl-vs.github.io/rlvs2021/konstantinos-chatzilygeroudis.html), [J.-B. Mouret](https://rl-vs.github.io/rlvs2021/jean-baptiste-mouret.html) |
|            | 16:30-17:30 | [Efficient Motor Skills Learning in Robotics](https://rl-vs.github.io/rlvs2021/efficient-motor.html) | [D. Lee](https://rl-vs.github.io/rlvs2021/dongheui-lee.html) |
| April 9th  | 9:00-13:00  | [RL tips and tricks](https://rl-vs.github.io/rlvs2021/tips-and-tricks.html) | [A. Raffin](https://rl-vs.github.io/rlvs2021/antonin-raffin.html) |
|            | 14:30-15:30 | [Symbolic representations and reinforcement learning](https://rl-vs.github.io/rlvs2021/symbolic.html) | [M. Garnelo](https://rl-vs.github.io/rlvs2021/marta-garnelo.html) |
|            | 15:45-16:45 | [Leveraging model-learning for extreme generalization](https://rl-vs.github.io/rlvs2021/model-learning.html) | [L. P. Kaelbling](https://rl-vs.github.io/rlvs2021/leslie-kaelbling.html) |
|            | 17:00-18:00 | RLVS wrap-up                                                 | [E. Rachelson](https://rl-vs.github.io/rlvs2021/emmanuel-rachelson.html) |

## (4/1/21) - [Deep Q-Networks and its variants](https://whova.com/embedded/session/rlstc_202011/1416824/?view=)



Speaker is Bilal Piot.

**Deep Q network** as a solution for a practicable control theory.

Introduction of ALE (Atari Learning Environment)

DQN is (almost) end-to-end: from raw observations to actions. Bilal explains the preprocessing part (from 160x210x3 to 84x84 + stacking 4 frames + downsampling to 15 Hz)

Value Iteration (VI) algorithm: Recurrent algorithm to get Q. $Q_{k+1}=T^*Q$

But it is not practical in a real-world case. What we can do is use interactions with real world. And estimate $Q^*$ using a regression.

Would be interesting to have slides. I like the link between regression notations and VI notation.

From neural Fitted-$Q$ to DQN. Main difference is data collection (in DQN you have updated interactions and it allows exploration, and size of architecture)

With DQN we have acting part and learning part. Acting is the data collection. (using $\epsilon$-greedy policy)



**hands-on based on DQN tutorial notebook.**

had to  `export LD_LIBRARY_PATH=/home/explore/miniconda3/envs/aniti/lib/`

Nice introduction to JAX and haiku. Haiku is similar modules in pytorch and can turn NN into pure version. Which is useful for Jax.



**overview of the literature**

![](https://kstatic.googleusercontent.com/files/f6b5f285173d4449285a8e812b8385f45c03f7104e1c41370a73e0c8558ff82d6a69e60962dd91c4972c444fd73bc4f98a06b5487eff5a037a37bc42f97cef3b)



## (4/2/21) - [From Policy Gradients to Actor Critic methods](https://whova.com/embedded/session/rlstc_202011/1416833/?view=)

Olivier Sigaud is the speaker.

He has pre-recorded his lecture in videos. I have missed the start so I will have to watch them later. 



[Policy Gradient in pratice](https://whova.com/embedded/session/rlstc_202011/1416836/?view=)

Don't become an alchemist ;)

As stochastic policies, squashed gaussian is interesting because it allows continuous variable + bounds.



[Exploration in Deep RL](https://whova.com/embedded/session/rlstc_202011/1416838/?view=#)



## (4/8/21) - [Evolutionary Reinforcement Learning](https://whova.com/embedded/session/rlstc_202011/1416851/?view=)

pdf version of the slides are available [here](https://rl-vs.github.io/rlvs2021/class-material/evolutionary/light-virtual_school_neat_hyperneat.pdf)

then [Evolving Agents that Learn More Like Animals](https://whova.com/embedded/session/rlstc_202011/1416848/?view=)



This morning was more about what we can do when we have infinite calculation power and data.

Afternoon will be the opposite.



## (4/8/21) - [Micro-data Policy Search](https://whova.com/embedded/session/rlstc_202011/1416841/?view=)

Most policy search algorithms require thousands of training episodes to  find an effective policy, which is often infeasible when experiments  takes time or are expensive (for instance, with physical robot or with  an aerodynamics simulator). This class focuses on the extreme other end  of the spectrum: how can an algorithm adapt a policy with only a handful of trials (a dozen) and a few minutes? By analogy with the word  "big-data", we refer to this challenge as "micro-data reinforcement  learning". We will describe two main strategies: (1) leverage prior  knowledge on the policy structure (e.g., dynamic movement primitives),  on the policy parameters (e.g., demonstrations), or on the dynamics  (e.g., simulators), and (2) create data-driven surrogate models of the  expected reward (e.g., Bayesian optimization) or the dynamical model  (e.g., model-based policy search), so that the policy optimizer queries  the model instead of the real system. Most of the examples will be about robotic systems, but the principle apply to any other expensive setup.

all material: [https://rl-vs.github.io/rlvs2021/micro-data.html](https://rl-vs.github.io/rlvs2021/micro-data.html)



## (4/9/21) - [RL in Practice: Tips and Tricks and Practical Session With Stable-Baselines3](https://whova.com/embedded/session/rlstc_202011/1416855/?view=)



â€‹                        **Abstract:**
The aim of the session is to  help you do reinforcement learning experiments. The first part covers  general advice about RL, tips and tricks and details three examples  where RL was applied on real robots. The second part will be a practical session using the Stable-Baselines3 library.

**Pre-requisites:**
Python programming, RL basics, (recommended: Google account for the practical session in order to use Google Colab).

**Additional material:**
Website: [https://github.com/DLR-RM/stable-baselines3](https://github.com/DLR-RM/stable-baselines3)
Doc: [https://stable-baselines3.readthedocs.io/en/master/](https://stable-baselines3.readthedocs.io/en/master/)

**Outline:**
Part I: RL Tips and Tricks / The Challenges of Applying RL to Real Robots

1. Introduction (3 minutes)
2. RL Tips and tricks (45 minutes)
   1. General Nuts and Bolts of RL experimentation (10 minutes)
   2. RL in practice on a custom task (custom environment) (30 minutes)
   3. Questions? (5 minutes)
3. The Challenges of Applying RL to Real Robots (45 minutes)
   1. Learning to control an elastic robot - DLR David Neck Example (15 minutes)
   2. Learning to drive in minutes and learning to race in hours - Virtual and real racing car (15 minutes)
   3. Learning to walk with an elastic quadruped robot - DLR bert example (10 minutes)
   4. Questions? (5 minutes+)

Part II: Practical Session with Stable-Baselines3

1. Stable-Baselines3 Overview (20 minutes)
2. Questions? (5 minutes)
3. Practical Session - Code along (1h+)



*action space*

When using continuous space, you need to normalize! (normalized action space -1, -1)

there is a checker for that in stable baselines 3.

*reward*

start with reward shaping.

*termination condition*

early stopping makes learning faster (and safer for robots)

![](../images/aniti_rl_algo.png)

for hyperparameter tuning, Antonin recommends Optuna.



about the Henderson paper: [Deep Reinforcement Learning that Matters](https://arxiv.org/abs/1709.06560)

![](../images/aniti_rl_slr.png)

and then the controller will use latent representation / current speed + history as observation space.

Learning to drive takes then 10 min, and to race 2 hours.



#### handson

slides: [https://araffin.github.io/slides/rlvs-sb3-handson/](https://araffin.github.io/slides/rlvs-sb3-handson/)

notebook: [https://github.com/araffin/rl-handson-rlvs21](https://github.com/araffin/rl-handson-rlvs21)

RL zoo: [https://github.com/DLR-RM/rl-baselines3-zoo](https://github.com/DLR-RM/rl-baselines3-zoo)

documentation for SB3 usefull for completing exercises: [https://stable-baselines3.readthedocs.io/en/master/](https://stable-baselines3.readthedocs.io/en/master/)



https://excalidraw.com/