{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables traces using show_guts decorator\n",
    "> \"usefull to debug\"\n",
    "- show_tags: true\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: true\n",
    "- categories: [python]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# show_guts decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptaton from https://stackoverflow.com/questions/24165374/printing-a-functions-local-variable-names-and-values\n",
    "\n",
    "Update to python 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T12:40:56.796617Z",
     "start_time": "2020-10-07T12:40:56.792208Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import threading\n",
    "\n",
    "def show_guts(f):\n",
    "    sentinel = object()\n",
    "    gutsdata = threading.local()\n",
    "    gutsdata.captured_locals = None\n",
    "    gutsdata.tracing = False\n",
    "\n",
    "    def trace_locals(frame, event, arg):\n",
    "        if event.startswith('c_'):  # C code traces, no new hook\n",
    "            return \n",
    "        if event == 'call':  # start tracing only the first call\n",
    "            if gutsdata.tracing:\n",
    "                return None\n",
    "            gutsdata.tracing = True\n",
    "            return trace_locals\n",
    "        if event == 'line':  # continue tracing\n",
    "            return trace_locals\n",
    "\n",
    "        # event is either exception or return, capture locals, end tracing\n",
    "        gutsdata.captured_locals = frame.f_locals.copy()\n",
    "        return None\n",
    "\n",
    "    def wrapper(*args, **kw):\n",
    "        # preserve existing tracer, start our trace\n",
    "        old_trace = sys.gettrace()\n",
    "        sys.settrace(trace_locals)\n",
    "\n",
    "        retval = sentinel\n",
    "        try:\n",
    "            retval = f(*args, **kw)\n",
    "        finally:\n",
    "            # reinstate existing tracer, report, clean up\n",
    "            sys.settrace(old_trace)\n",
    "            for key, val in gutsdata.captured_locals.items():\n",
    "                print('{}: {!r}'.format(key, val))\n",
    "            if retval is not sentinel:\n",
    "                print('Returned: {!r}'.format(retval))\n",
    "            gutsdata.captured_locals = None\n",
    "            gutsdata.tracing = False\n",
    "\n",
    "        return retval\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T12:41:26.276822Z",
     "start_time": "2020-10-07T12:41:26.274486Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import CelebA\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T12:41:42.890405Z",
     "start_time": "2020-10-07T12:41:42.876277Z"
    }
   },
   "outputs": [],
   "source": [
    "@show_guts\n",
    "def get_score(current_classifications, original_classifications, target_indices, other_indices, penalty_weight):\n",
    "    '''\n",
    "    Function to return the score of the current classifications, penalizing changes\n",
    "    to other classes with an L2 norm.\n",
    "    Parameters:\n",
    "        current_classifications: the classifications associated with the current noise\n",
    "        original_classifications: the classifications associated with the original noise\n",
    "        target_indices: the index of the target class\n",
    "        other_indices: the indices of the other classes\n",
    "        penalty_weight: the amount that the penalty should be weighted in the overall score\n",
    "    '''\n",
    "    # Steps: 1) Calculate the change between the original and current classifications (as a tensor)\n",
    "    #           by indexing into the other_indices you're trying to preserve, like in x[:, features].\n",
    "    #        2) Calculate the norm (magnitude) of changes per example.\n",
    "    #        3) Multiply the mean of the example norms by the penalty weight. \n",
    "    #           This will be your other_class_penalty.\n",
    "    #           Make sure to negate the value since it's a penalty!\n",
    "    #        4) Take the mean of the current classifications for the target feature over all the examples.\n",
    "    #           This mean will be your target_score.\n",
    "    #### START CODE HERE ####\n",
    "    change_original_classification = (current_classifications[:,other_indices] - original_classifications[:,other_indices])\n",
    "    # Calculate the norm (magnitude) of changes per example and multiply by penalty weight\n",
    "    other_class_penalty = - torch.mean(torch.norm(change_original_classification, dim=1) * penalty_weight)\n",
    "    # Take the mean of the current classifications for the target feature\n",
    "    target_score = torch.mean(current_classifications)\n",
    "    #### END CODE HERE ####\n",
    "    return target_score + other_class_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T12:42:04.044478Z",
     "start_time": "2020-10-07T12:42:03.960280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_classifications: tensor([[1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.]])\n",
      "original_classifications: tensor([[1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.]])\n",
      "target_indices: [1, 3]\n",
      "other_indices: [0, 2]\n",
      "penalty_weight: 0.2\n",
      "change_original_classification: tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "other_class_penalty: tensor(-0.)\n",
      "target_score: tensor(2.5000)\n",
      "Returned: tensor(2.5000)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c7e77f2e4ae1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Must be 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mget_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rows = 10\n",
    "current_class = torch.tensor([[1] * rows, [2] * rows, [3] * rows, [4] * rows]).T.float()\n",
    "original_class = torch.tensor([[1] * rows, [2] * rows, [3] * rows, [4] * rows]).T.float()\n",
    "\n",
    "# Must be 3\n",
    "assert get_score(current_class, original_class, [1, 3] , [0, 2], 0.2).item() == 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai] *",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
