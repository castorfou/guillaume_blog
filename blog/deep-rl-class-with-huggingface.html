<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Deep RL class - huggingface | Guillaumeâ€™s blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Deep RL class - huggingface" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="par Thomas Simonini" />
<meta property="og:description" content="par Thomas Simonini" />
<link rel="canonical" href="https://castorfou.github.io/guillaume_blog/blog/deep-rl-class-with-huggingface.html" />
<meta property="og:url" content="https://castorfou.github.io/guillaume_blog/blog/deep-rl-class-with-huggingface.html" />
<meta property="og:site_name" content="Guillaumeâ€™s blog" />
<meta property="og:image" content="https://castorfou.github.io/guillaume_blog/images/RL.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-06-15T00:00:00-05:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://castorfou.github.io/guillaume_blog/images/RL.png" />
<meta property="twitter:title" content="Deep RL class - huggingface" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-06-15T00:00:00-05:00","datePublished":"2022-06-15T00:00:00-05:00","description":"par Thomas Simonini","headline":"Deep RL class - huggingface","image":"https://castorfou.github.io/guillaume_blog/images/RL.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://castorfou.github.io/guillaume_blog/blog/deep-rl-class-with-huggingface.html"},"url":"https://castorfou.github.io/guillaume_blog/blog/deep-rl-class-with-huggingface.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/guillaume_blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://castorfou.github.io/guillaume_blog/feed.xml" title="Guillaume's blog" /><link rel="shortcut icon" type="image/x-icon" href="/guillaume_blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/guillaume_blog/">Guillaume&#39;s blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/guillaume_blog/about/">About Me</a><a class="page-link" href="/guillaume_blog/search/">Search</a><a class="page-link" href="/guillaume_blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Deep RL class - huggingface</h1><p class="page-description">par Thomas Simonini</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-06-15T00:00:00-05:00" itemprop="datePublished">
        Jun 15, 2022
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      5 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/guillaume_blog/categories/#reinforcement learning">reinforcement learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/guillaume_blog/categories/#huggingface">huggingface</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#unit-1---introduction-to-deep-reinforcement-learning">Unit 1 - Introduction to Deep Reinforcement Learning</a>
<ul>
<li class="toc-entry toc-h6"><a href="#-it-starts-with-some-general-introduction-to-deep-rl-and-then-a-quizz">ğŸ“– It starts with some general introduction to deep RL and then a quizz.</a></li>
<li class="toc-entry toc-h6"><a href="#-1st-practice-uses-this-lunar-lander-environment-and-you-train-a-ppo-agent-to-get-the-highest-score">ğŸ‘©â€ğŸ’» 1st practice uses this lunar lander environment, and you train a PPO agent to get the highest score,</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#unit-2---introduction-to-q-learning">Unit 2 - Introduction to Q-Learning</a>
<ul>
<li class="toc-entry toc-h6"><a href="#-part-1---we-learned-about-the-value-based-methods-and-the-difference-between-monte-carlo-and-temporal-difference-learning-then-a-quizz-easy-one">ğŸ“– part 1 - we learned about the value-based methods and the difference between Monte Carlo and Temporal Difference Learning. Then a quizz (easy one)</a></li>
<li class="toc-entry toc-h6"><a href="#-part-2---and-then-q-learning-which-is-an-off-policy-value-based-method-that-uses-a-td-approach-to-train-its-action-value-function-then-a-quizz-less-easier">ğŸ“– part 2 - and then Q-learning which is an off-policy value-based method that uses a TD approach to train its action-value function. Then a quizz (less easier)</a></li>
<li class="toc-entry toc-h6"><a href="#-hands-on-1st-algo-frozenlake-is-published-in-guillaume63q-frozenlake-v1-4x4-noslippery-2nd-algo-taxi-is-published-in-guillaume63q-taxi-v3-leaderboard-is-here">ğŸ‘©â€ğŸ’» hands-on. 1st algo (FrozenLake) is published in Guillaume63/q-FrozenLake-v1-4x4-noSlippery. 2nd algo (Taxi) is published in Guillaume63/q-Taxi-v3. Leaderboard is here</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#unit-3---deep-q-learning-with-atari-games">Unit 3 - Deep Q-Learning with Atari Games</a>
<ul>
<li class="toc-entry toc-h6"><a href="#-the-deep-q-learning-chapter----httpshuggingfacecoblogdeep-rl-dqn">ğŸ“– The Deep Q-Learning chapter ğŸ‘¾ ğŸ‘‰  https://huggingface.co/blog/deep-rl-dqn</a></li>
<li class="toc-entry toc-h6"><a href="#-start-the-hands-on-here--httpscolabresearchgooglecomgithubhuggingfacedeep-rl-classblobmainunit3unit3ipynb">ğŸ‘©â€ğŸ’» Start the hands-on here ğŸ‘‰ https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/unit3/unit3.ipynb</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#unit-4---an-introduction-to-unity-ml-agents-with-hugging-face-">Unit 4 - An Introduction to Unity ML-Agents with Hugging Face ğŸ¤—</a>
<ul>
<li class="toc-entry toc-h6"><a href="#-tutorial--httpslinkmediumcomkopvpdyz4qb">ğŸ“– tutorial ğŸ‘‰ https://link.medium.com/KOpvPdyz4qb</a></li>
<li class="toc-entry toc-h6"><a href="#-here-are-the-steps-for-the-training">ğŸ‘©â€ğŸ’» Here are the steps for the training:</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#unit-5---policy-gradient-with-pytorch">Unit 5 - Policy Gradient with PyTorch</a>
<ul>
<li class="toc-entry toc-h6"><a href="#1ï¸âƒ£--read-policy-gradient-with-pytorch-chapter">1ï¸âƒ£ ğŸ“– Read Policy Gradient with PyTorch Chapter.</a></li>
<li class="toc-entry toc-h6"><a href="#2ï¸âƒ£--then-dive-on-the-hands-on-where-youll-code-your-first-deep-reinforcement-learning-algorithm-from-scratch-reinforce">2ï¸âƒ£ ğŸ‘©â€ğŸ’» Then dive on the hands-on where youâ€™ll code your first Deep Reinforcement Learning algorithm from scratch: Reinforce.</a></li>
</ul>
</li>
</ul><p>Didnâ€™t mention that but I have started <a href="https://github.com/huggingface/deep-rl-class">The Hugging Face Deep Reinforcement Learning Class</a> by Thomas Simonini.</p>

<p>Thomas is now part of HuggingFace.</p>

<p>1st step is to fork the repo, and for mine it is <a href="https://github.com/castorfou/deep-rl-class">here</a>.</p>

<p>And clone it locally: <code class="language-plaintext highlighter-rouge">git clone git@github.com:castorfou/deep-rl-class.git</code> ou <code class="language-plaintext highlighter-rouge">git clone https://github.com/castorfou/deep-rl-class.git</code></p>

<p>I followed the 1st unit in May/11.</p>

<p>there is a community on discord at <a href="https://discord.gg/aYka4Yhff9">https://discord.gg/aYka4Yhff9</a>, with a lounge about RL.</p>

<h1 id="unit-1---introduction-to-deep-reinforcement-learning">
<a class="anchor" href="#unit-1---introduction-to-deep-reinforcement-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="https://github.com/huggingface/deep-rl-class/tree/main/unit1">Unit 1</a> - Introduction to Deep Reinforcement Learning</h1>

<h6 id="-it-starts-with-some-general-introduction-to-deep-rl-and-then-a-quizz">
<a class="anchor" href="#-it-starts-with-some-general-introduction-to-deep-rl-and-then-a-quizz" aria-hidden="true"><span class="octicon octicon-link"></span></a>ğŸ“– It starts with some <a href="https://huggingface.co/blog/deep-rl-intro">general introduction to deep RL</a> and then a quizz.</h6>

<h6 id="-1st-practice-uses-this-lunar-lander-environment-and-you-train-a-ppo-agent-to-get-the-highest-score">
<a class="anchor" href="#-1st-practice-uses-this-lunar-lander-environment-and-you-train-a-ppo-agent-to-get-the-highest-score" aria-hidden="true"><span class="octicon octicon-link"></span></a>ğŸ‘©â€ğŸ’» 1st practice uses this lunar lander environment, and you train a PPO agent to get the highest score,</h6>

<ul>
  <li>and this runs on colab : <a href="https://github.com/huggingface/deep-rl-class/blob/main/unit1/unit1.ipynb">https://github.com/huggingface/deep-rl-class/blob/main/unit1/unit1.ipynb</a> (just by clicking on <img src="https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667">)</li>
  <li>there is a leaderboard running under huggingface (one can publish models to huggingface) <a href="https://huggingface.co/spaces/chrisjay/Deep-Reinforcement-Learning-Leaderboard">https://huggingface.co/spaces/chrisjay/Deep-Reinforcement-Learning-Leaderboard</a> . Just need an huggingface account for that (used my Michelin account)</li>
</ul>

<p>A guide has been recently added explaining how to tune hyperparameters using optuna. ğŸ‘‰ <a href="https://github.com/huggingface/deep-rl-class/blob/main/unit1/unit1_optuna_guide.ipynb">https://github.com/huggingface/deep-rl-class/blob/main/unit1/unit1_optuna_guide.ipynb</a>. Should do it!</p>

<p>To start unit2. Introduction to Q-Learning</p>

<ul>
  <li>first update from fork just by clicking<img src="https://docs.github.com/assets/cb-33131/images/help/repository/fetch-upstream-drop-down.png" alt='"Fetch upstream" drop-down' style="zoom:15%;">
</li>
  <li>and update your local repo (<code class="language-plaintext highlighter-rouge">git fetch</code> <code class="language-plaintext highlighter-rouge">git pull</code>)</li>
</ul>

<h1 id="unit-2---introduction-to-q-learning">
<a class="anchor" href="#unit-2---introduction-to-q-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="https://github.com/huggingface/deep-rl-class/tree/main/unit2">Unit 2</a> - Introduction to Q-Learning</h1>

<h6 id="-part-1---we-learned-about-the-value-based-methods-and-the-difference-between-monte-carlo-and-temporal-difference-learning-then-a-quizz-easy-one">
<a class="anchor" href="#-part-1---we-learned-about-the-value-based-methods-and-the-difference-between-monte-carlo-and-temporal-difference-learning-then-a-quizz-easy-one" aria-hidden="true"><span class="octicon octicon-link"></span></a>ğŸ“– <a href="https://huggingface.co/blog/deep-rl-q-part1">part 1</a> - we learned about the value-based methods and the difference between Monte Carlo and Temporal Difference Learning. Then a quizz (easy one)</h6>

<h6 id="-part-2---and-then-q-learning-which-is-an-off-policy-value-based-method-that-uses-a-td-approach-to-train-its-action-value-function-then-a-quizz-less-easier">
<a class="anchor" href="#-part-2---and-then-q-learning-which-is-an-off-policy-value-based-method-that-uses-a-td-approach-to-train-its-action-value-function-then-a-quizz-less-easier" aria-hidden="true"><span class="octicon octicon-link"></span></a>ğŸ“– <a href="https://huggingface.co/blog/deep-rl-q-part2">part 2</a> - and then Q-learning which is an <strong>off-policy value-based method that uses a TD approach to train its action-value function</strong>. Then a quizz (less easier)</h6>

<h6 id="-hands-on-1st-algo-frozenlake-is-published-in-guillaume63q-frozenlake-v1-4x4-noslippery-2nd-algo-taxi-is-published-in-guillaume63q-taxi-v3-leaderboard-is-here">
<a class="anchor" href="#-hands-on-1st-algo-frozenlake-is-published-in-guillaume63q-frozenlake-v1-4x4-noslippery-2nd-algo-taxi-is-published-in-guillaume63q-taxi-v3-leaderboard-is-here" aria-hidden="true"><span class="octicon octicon-link"></span></a>ğŸ‘©â€ğŸ’» <a href="https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/unit2/unit2.ipynb">hands-on</a>. 1st algo (FrozenLake) is published in <a href="https://huggingface.co/Guillaume63/q-FrozenLake-v1-4x4-noSlippery">Guillaume63/q-FrozenLake-v1-4x4-noSlippery</a>. 2nd algo (Taxi) is published in <a href="https://huggingface.co/Guillaume63/q-Taxi-v3">Guillaume63/q-Taxi-v3</a>. Leaderboard is <a href="https://huggingface.co/spaces/chrisjay/Deep-Reinforcement-Learning-Leaderboard">here</a>
</h6>

<h1 id="unit-3---deep-q-learning-with-atari-games">
<a class="anchor" href="#unit-3---deep-q-learning-with-atari-games" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="https://github.com/huggingface/deep-rl-class/tree/main/unit3">Unit 3</a> - Deep Q-Learning with Atari Games</h1>

<h6 id="-the-deep-q-learning-chapter----httpshuggingfacecoblogdeep-rl-dqn">
<a class="anchor" href="#-the-deep-q-learning-chapter----httpshuggingfacecoblogdeep-rl-dqn" aria-hidden="true"><span class="octicon octicon-link"></span></a>ğŸ“– The Deep Q-Learning chapter ğŸ‘¾ ğŸ‘‰  <a href="https://huggingface.co/blog/deep-rl-dqn">https://huggingface.co/blog/deep-rl-dqn</a>
</h6>

<h6 id="-start-the-hands-on-here--httpscolabresearchgooglecomgithubhuggingfacedeep-rl-classblobmainunit3unit3ipynb">
<a class="anchor" href="#-start-the-hands-on-here--httpscolabresearchgooglecomgithubhuggingfacedeep-rl-classblobmainunit3unit3ipynb" aria-hidden="true"><span class="octicon octicon-link"></span></a>ğŸ‘©â€ğŸ’» Start the hands-on here ğŸ‘‰ <a href="https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/unit3/unit3.ipynb">https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/unit3/unit3.ipynb</a>
</h6>

<p>from discord, a video (30â€™) by Antonin Raffin about <a href="https://www.youtube.com/watch?v=AidFTOdGNFQ">Automatic Hyperparameter Optimization @ ICRA 22 - Tools for Robotic RL 6/8</a>. Never thought about it that way, it can help to speed training phase.</p>

<p>from discord as well a video to build a <a href="https://www.youtube.com/watch?v=eBCU-tqLGfQ">doom ai model</a> (3 hours!)</p>

<p>and from discord a lecture from Pieter Abbeel explaining Q-value to DQN and why we have this double network at <a href="https://www.youtube.com/watch?v=Psrhxy88zww">L2 Deep Q-Learning (Foundations of Deep RL Series</a>. This is part of a larger lecture available at <a href="https://www.youtube.com/watch?v=2GwBez0D20A&amp;list=PLwRJQ4m4UJjNymuBM9RdmB3Z9N5-0IlY0">Foundations of Deep RL â€“ 6-lecture series by Pieter Abbeel</a></p>

<p>And then a video explaining <a href="https://agarwl.github.io/rliable/">Deep RL at the Edge of the Statistical Precipice</a>. This was from a paper at Neurips.</p>

<h1 id="unit-4---an-introduction-to-unity-ml-agents-with-hugging-face-">
<a class="anchor" href="#unit-4---an-introduction-to-unity-ml-agents-with-hugging-face-" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="https://github.com/huggingface/deep-rl-class/tree/main/unit4">Unit 4</a> - An Introduction to <strong>Unity ML-Agents with Hugging Face ğŸ¤—</strong>
</h1>

<h6 id="-tutorial--httpslinkmediumcomkopvpdyz4qb">
<a class="anchor" href="#-tutorial--httpslinkmediumcomkopvpdyz4qb" aria-hidden="true"><span class="octicon octicon-link"></span></a>ğŸ“– <strong>tutorial</strong> ğŸ‘‰ https://link.medium.com/KOpvPdyz4qb</h6>

<p>Thomas starts with evolutions on RL domain, citing <a href="https://huggingface.co/blog/decision-transformers">Decision Transformers</a> as one of the last hot topic. And then introduces Unity and how it can now be used with RL agents.</p>

<p><img src="https://miro.medium.com/max/1400/0*kYixBHKWwmY65Mg_" alt="unity ML-Agents toolkit"></p>

<p>Interesting idea to introduce <a href="https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-next-state-prediction-f7f4e2f592fa">curiosity</a> and to make it real as an intrinsic reward.</p>

<blockquote>
  <p><em>Note: It guided me to gentle introductions to <a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/">cross-entropy for machine learning</a> and <a href="https://machinelearningmastery.com/what-is-information-entropy/">information entropy</a>.</em></p>

  <ul>
    <li>
      <p><em><strong>Low Probability Event</strong> (surprising): More information. High entropy.</em></p>
    </li>
    <li>
      <p><em><strong>Higher Probability Event</strong> (unsurprising): Less information. Low entropy.</em></p>
    </li>
    <li>
      <p><em><strong>Skewed Probability Distribution</strong> (unsurprising): Low entropy.</em></p>
    </li>
    <li>
      <p><em><strong>Balanced Probability Distribution</strong> (surprising): High entropy.</em></p>
    </li>
  </ul>

  <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>I</mi><mi>n</mi><mi>f</mi><mi>o</mi><mi>r</mi><mi>m</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>:</mo><mspace linebreak="newline"></mspace><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mo>âˆ’</mo><mi>log</mi><mo>â¡</mo><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Information:

\\h(x)=-\log(P(x))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mord mathnormal">n</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal" style="margin-right:0.02778em;">or</span><span class="mord mathnormal">ma</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">:</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">âˆ’</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span></span></span></span></span>

  <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>E</mi><mi>n</mi><mi>t</mi><mi>r</mi><mi>o</mi><mi>p</mi><mi>y</mi><mo>:</mo><mspace linebreak="newline"></mspace><mi>H</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>â€“</mtext><munder><mo>âˆ‘</mo><mrow><mi>x</mi><mo>âˆˆ</mo><mi>X</mi></mrow></munder><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mi>log</mi><mo>â¡</mo><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Entropy:
\\H(X) = â€“ \sum_{x \in X} P(x)  \log(P(x))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ro</span><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">:</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.3717em;vertical-align:-1.3217em;"></span><span class="mord">â€“</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.8557em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mrel mtight">âˆˆ</span><span class="mord mathnormal mtight" style="margin-right:0.07847em;">X</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">âˆ‘</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:1.3217em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span></span></span></span></span>

  <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>C</mi><mi>r</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>âˆ’</mo><mi>E</mi><mi>n</mi><mi>t</mi><mi>r</mi><mi>o</mi><mi>p</mi><mi>y</mi><mo>:</mo><mspace linebreak="newline"></mspace><mi>H</mi><mo stretchy="false">(</mo><mi>P</mi><mo separator="true">,</mo><mi>Q</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>â€“</mtext><munder><mo>âˆ‘</mo><mrow><mi>x</mi><mo>âˆˆ</mo><mi>X</mi></mrow></munder><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mi>log</mi><mo>â¡</mo><mo stretchy="false">(</mo><mi>Q</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Cross-Entropy:\\H(P, Q) = â€“ \sum_{x \in X} P(x)  \log(Q(x))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord mathnormal">ross</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">âˆ’</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ro</span><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">:</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">Q</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.3717em;vertical-align:-1.3217em;"></span><span class="mord">â€“</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.8557em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mrel mtight">âˆˆ</span><span class="mord mathnormal mtight" style="margin-right:0.07847em;">X</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">âˆ‘</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:1.3217em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span></span></span></span></span>

  <p>Cross-Entropy and KL divergence are similar but not exactly the same. Specifically, the KL divergence measures a very similar quantity to  cross-entropy. It measures the average number of extra bits required to  represent a message with Q instead of P, not the total number of bits.</p>

  <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>K</mi><mi>L</mi><mtext>Â </mtext><mi>D</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>n</mi><mi>c</mi><mi>e</mi><mtext>Â </mtext><mo stretchy="false">(</mo><mi>r</mi><mi>e</mi><mi>l</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mtext>Â </mtext><mi>e</mi><mi>n</mi><mi>t</mi><mi>r</mi><mi>o</mi><mi>p</mi><mi>y</mi><mo stretchy="false">)</mo><mo>:</mo><mspace linebreak="newline"></mspace><mi>K</mi><mi>L</mi><mo stretchy="false">(</mo><mi>P</mi><mi mathvariant="normal">âˆ£</mi><mi mathvariant="normal">âˆ£</mi><mi>Q</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>â€“</mtext><munder><mo>âˆ‘</mo><mrow><mi>x</mi><mo>âˆˆ</mo><mi>X</mi></mrow></munder><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mfrac><mrow><mi>log</mi><mo>â¡</mo><mo stretchy="false">(</mo><mi>Q</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><mrow><mi>log</mi><mo>â¡</mo><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mfrac><mspace linebreak="newline"></mspace><mi>H</mi><mo stretchy="false">(</mo><mi>P</mi><mo separator="true">,</mo><mi>Q</mi><mo stretchy="false">)</mo><mo>=</mo><mi>H</mi><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo><mo>+</mo><mi>K</mi><mi>L</mi><mo stretchy="false">(</mo><mi>P</mi><mi mathvariant="normal">âˆ£</mi><mi mathvariant="normal">âˆ£</mi><mi>Q</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">KL\ Divergence\ (relative\ entropy):
\\KL(P||Q)=â€“ \sum_{x \in X} P(x)  \frac{\log(Q(x))}{\log(P(x))}
\\H(P, Q) = H(P) + KL(P || Q)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord mathnormal">L</span><span class="mspace">Â </span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord mathnormal" style="margin-right:0.02778em;">er</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">ce</span><span class="mspace">Â </span><span class="mopen">(</span><span class="mord mathnormal">re</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord mathnormal">e</span><span class="mspace">Â </span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ro</span><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">:</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord">âˆ£âˆ£</span><span class="mord mathnormal">Q</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.7487em;vertical-align:-1.3217em;"></span><span class="mord">â€“</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.8557em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mrel mtight">âˆˆ</span><span class="mord mathnormal mtight" style="margin-right:0.07847em;">X</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">âˆ‘</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:1.3217em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">Q</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord">âˆ£âˆ£</span><span class="mord mathnormal">Q</span><span class="mclose">)</span></span></span></span></span>

</blockquote>

<h6 id="-here-are-the-steps-for-the-training">
<a class="anchor" href="#-here-are-the-steps-for-the-training" aria-hidden="true"><span class="octicon octicon-link"></span></a>ğŸ‘©â€ğŸ’» Here are the steps for the training:</h6>

<ul>
  <li>clone repo and install environment</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># from ~/git/guillaume</span>
git clone https://github.com/huggingface/ml-agents/
<span class="c"># bug with python 3.9 - https://github.com/Unity-Technologies/ml-agents/issues/5689</span>
conda create  <span class="nt">--name</span> ml-agents <span class="nv">python</span><span class="o">=</span>3.8
conda activate ml-agents
<span class="c"># Go inside the repository and install the package </span>
<span class="nb">cd </span>ml-agents 
pip <span class="nb">install</span> <span class="nt">-e</span> ./ml-agents-envs 
pip <span class="nb">install</span> <span class="nt">-e</span> ./ml-agents
</code></pre></div></div>

<ul>
  <li>download the Environment Executable (pyramids from <a href="https://drive.google.com/drive/folders/1cjUOCB6gikJHmOnoQ5R9oM7-_zAFXuA2">google drive</a>)</li>
</ul>

<p>Unzip it and place it inside the MLAgents cloned repo <strong>in a new folder called trained-envs-executables/linux</strong></p>

<ul>
  <li>modify nbr of steps to 1000000 in <code class="language-plaintext highlighter-rouge">config/ppo/PyramidsRND.yaml</code>
</li>
  <li>train</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mlagents-learn config/ppo/PyramidsRND.yaml <span class="nt">--env</span><span class="o">=</span>training-envs-executables/linux/Pyramids/Pyramids <span class="nt">--run-id</span><span class="o">=</span><span class="s2">"First Training"</span> <span class="nt">--no-graphics</span>
</code></pre></div></div>

<ul>
  <li>monitor training</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensorboard <span class="nt">--logdir</span> results <span class="nt">--port</span> 6006
</code></pre></div></div>

<p>(auto reload is off by default this day, click settings and check Reload data) (because I have installed v2.3.0 and not 2.4.0, there is <a href="https://github.com/tensorflow/tensorboard/issues/1946">no autofit domain to data</a> and it is annoying)</p>

<ul>
  <li>push to ğŸ¤— Hub</li>
</ul>

<p>Create a new token (https://huggingface.co/settings/tokens) <strong>with write role</strong></p>

<p>Copy the token, Run this and past the token <code class="language-plaintext highlighter-rouge">huggingface-cli login</code></p>

<p>Push to Hub</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mlagents-push-to-hf <span class="nt">--run-id</span><span class="o">=</span><span class="s1">'First Training'</span> <span class="nt">--local-dir</span><span class="o">=</span><span class="s1">'results/First Training'</span> <span class="nt">--repo-id</span><span class="o">=</span><span class="s1">'Guillaume63/MLAgents-Pyramids'</span> <span class="nt">--commit-message</span><span class="o">=</span><span class="s1">'Trained pyramids agent upload'</span>
</code></pre></div></div>

<p>and now I can play it from <a href="https://huggingface.co/Guillaume63/MLAgents-Pyramids">https://huggingface.co/Guillaume63/MLAgents-Pyramids</a> and watch your Agent playâ€¦</p>

<h1 id="unit-5---policy-gradient-with-pytorch">
<a class="anchor" href="#unit-5---policy-gradient-with-pytorch" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="https://github.com/huggingface/deep-rl-class/tree/main/unit5">Unit 5</a> - Policy Gradient with PyTorch</h1>

<h6 id="1ï¸âƒ£--read-policy-gradient-with-pytorch-chapter">
<a class="anchor" href="#1%EF%B8%8F%E2%83%A3--read-policy-gradient-with-pytorch-chapter" aria-hidden="true"><span class="octicon octicon-link"></span></a>1ï¸âƒ£ ğŸ“– <strong>Read <a href="https://huggingface.co/blog/deep-rl-pg">Policy Gradient with PyTorch Chapter</a></strong>.</h6>

<p>Advantage and disadvantage of policy gradient vs DQN.</p>

<p>Reinforce algorithm (Monte Carlo policy gradient): it uses an estimated return from an entire episode to update the policy parameters.</p>

<p>The output of it is a probability distribution of actions. And we try to maximize J(Î¸) which is this estimated return. (details of Policy Gradient theorem in this <a href="https://www.youtube.com/watch?v=AKbX1Zvo7r8&amp;ab_channel=PieterAbbeel">video</a> from Pieter Abbeel)</p>

<p>We will update weights using this gradient: 
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î¸</mi><mo>â†</mo><mi>Î¸</mi><mo>+</mo><mi>Î±</mi><msub><mi mathvariant="normal">âˆ‡</mi><mi>Î¸</mi></msub><mi>J</mi><mo stretchy="false">(</mo><mi>Î¸</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\theta \gets  \theta + \alpha\nabla_\theta J(\theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">Î¸</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">â†</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">Î¸</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">Î±</span><span class="mord"><span class="mord">âˆ‡</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">Î¸</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">Î¸</span><span class="mclose">)</span></span></span></span>
<img src="https://huggingface.co/blog/assets/85_policy_gradient/pg.jpg" alt=""></p>

<ul>
  <li>
    <p>$\nabla_\theta\log\pi_\theta(a_t | s_t)$ is the direction of <strong>steepest increase of the (log) probability</strong> of selecting action at from state $s_t$. =&gt; This tells use <strong>how we should change the weights of policy</strong> if we want to increase/decrease the log probability of selecting action at state $s_t$.</p>
  </li>
  <li>
    <p>$R(\tau)$ is the scoring function:</p>

    <ul>
      <li>If the return is high, it will push up the probabilities of the (state, action) combinations.</li>
    </ul>
  </li>
  <li>
    <p>Else, if the return is low, it will push down the probabilities of the (state, action) combinations.</p>
  </li>
</ul>

<h6 id="2ï¸âƒ£--then-dive-on-the-hands-on-where-youll-code-your-first-deep-reinforcement-learning-algorithm-from-scratch-reinforce">
<a class="anchor" href="#2%EF%B8%8F%E2%83%A3--then-dive-on-the-hands-on-where-youll-code-your-first-deep-reinforcement-learning-algorithm-from-scratch-reinforce" aria-hidden="true"><span class="octicon octicon-link"></span></a>2ï¸âƒ£ ğŸ‘©â€ğŸ’» Then dive on the hands-on where youâ€™ll <strong>code your first Deep Reinforcement Learning algorithm from scratch: Reinforce</strong>.</h6>

<p>ğŸ‘‰ <a href="https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/unit5/unit5.ipynb">https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/unit5/unit5.ipynb</a></p>


  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="castorfou/guillaume_blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/guillaume_blog/blog/deep-rl-class-with-huggingface.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/guillaume_blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/guillaume_blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/guillaume_blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Journey for a datascientist</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/castorfou" target="_blank" title="castorfou"><svg class="svg-icon grey"><use xlink:href="/guillaume_blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/GuillaumeRamel1" target="_blank" title="GuillaumeRamel1"><svg class="svg-icon grey"><use xlink:href="/guillaume_blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
