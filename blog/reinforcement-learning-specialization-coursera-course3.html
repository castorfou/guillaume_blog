<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Reinforcement Learning Specialization - Coursera - course 3 - Prediction and Control with Function Approximation | Guillaume’s blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Reinforcement Learning Specialization - Coursera - course 3 - Prediction and Control with Function Approximation" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="From University of Alberta. My notes on course 3." />
<meta property="og:description" content="From University of Alberta. My notes on course 3." />
<link rel="canonical" href="https://castorfou.github.io/guillaume_blog/blog/reinforcement-learning-specialization-coursera-course3.html" />
<meta property="og:url" content="https://castorfou.github.io/guillaume_blog/blog/reinforcement-learning-specialization-coursera-course3.html" />
<meta property="og:site_name" content="Guillaume’s blog" />
<meta property="og:image" content="https://castorfou.github.io/guillaume_blog/images/RL.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-06-07T00:00:00-05:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://castorfou.github.io/guillaume_blog/images/RL.png" />
<meta property="twitter:title" content="Reinforcement Learning Specialization - Coursera - course 3 - Prediction and Control with Function Approximation" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2021-06-07T00:00:00-05:00","datePublished":"2021-06-07T00:00:00-05:00","description":"From University of Alberta. My notes on course 3.","headline":"Reinforcement Learning Specialization - Coursera - course 3 - Prediction and Control with Function Approximation","image":"https://castorfou.github.io/guillaume_blog/images/RL.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://castorfou.github.io/guillaume_blog/blog/reinforcement-learning-specialization-coursera-course3.html"},"url":"https://castorfou.github.io/guillaume_blog/blog/reinforcement-learning-specialization-coursera-course3.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/guillaume_blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://castorfou.github.io/guillaume_blog/feed.xml" title="Guillaume's blog" /><link rel="shortcut icon" type="image/x-icon" href="/guillaume_blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/guillaume_blog/">Guillaume&#39;s blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/guillaume_blog/about/">About Me</a><a class="page-link" href="/guillaume_blog/search/">Search</a><a class="page-link" href="/guillaume_blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Reinforcement Learning Specialization - Coursera - course 3 - Prediction and Control with Function Approximation</h1><p class="page-description">From University of Alberta. My notes on course 3.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-06-07T00:00:00-05:00" itemprop="datePublished">
        Jun 7, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      14 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/guillaume_blog/categories/#reinforcement learning">reinforcement learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/guillaume_blog/categories/#deepmind">deepmind</a>
        &nbsp;
      
        <a class="category-tags-link" href="/guillaume_blog/categories/#coursera">coursera</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h6"><a href="#course-introduction">Course introduction</a></li>
<li class="toc-entry toc-h2"><a href="#course-3---week-1---on-policy-prediction-with-approximation">Course 3 - Week 1 - On-policy Prediction with Approximation</a>
<ul>
<li class="toc-entry toc-h6"><a href="#module-1-learning-objectives">Module 1 Learning Objectives</a></li>
<li class="toc-entry toc-h6"><a href="#lesson-1-estimating-value-functions-as-supervised-learning">Lesson 1: Estimating Value Functions as Supervised Learning</a></li>
<li class="toc-entry toc-h6"><a href="#lesson-2-the-objective-for-on-policy-prediction">Lesson 2: The Objective for On-policy Prediction</a></li>
<li class="toc-entry toc-h6"><a href="#lesson-3-the-objective-for-td">Lesson 3: The Objective for TD</a></li>
<li class="toc-entry toc-h6"><a href="#lesson-4-linear-td">Lesson 4: Linear TD</a></li>
<li class="toc-entry toc-h6"><a href="#assignment">Assignment</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#course-3---week-2---constructing-features-for-prediction">Course 3 - Week 2 - Constructing Features for Prediction</a>
<ul>
<li class="toc-entry toc-h6"><a href="#module-2-learning-objectives">Module 2 Learning Objectives</a></li>
<li class="toc-entry toc-h6"><a href="#lesson-1-feature-construction-for-linear-methods">Lesson 1: Feature Construction for Linear Methods</a></li>
<li class="toc-entry toc-h6"><a href="#lesson-2-neural-networks">Lesson 2: Neural Networks</a></li>
<li class="toc-entry toc-h6"><a href="#lesson-3-training-neural-networks">Lesson 3: Training Neural Networks</a></li>
<li class="toc-entry toc-h6"><a href="#assignment-1">Assignment</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#course-3---week-3---control-with-approximation">Course 3 - Week 3 - Control with Approximation</a>
<ul>
<li class="toc-entry toc-h6"><a href="#module-3-learning-objectives">Module 3 Learning Objectives</a></li>
<li class="toc-entry toc-h6"><a href="#lesson-1-episodic-sarsa-with-function-approximation">Lesson 1: Episodic Sarsa with Function Approximation</a></li>
<li class="toc-entry toc-h6"><a href="#lesson-2-exploration-under-function-approximation">Lesson 2: Exploration under Function Approximation</a></li>
<li class="toc-entry toc-h6"><a href="#lesson-3-average-reward">Lesson 3: Average Reward</a></li>
<li class="toc-entry toc-h6"><a href="#assignment-2">Assignment</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#course-3---week-4---policy-gradient">Course 3 - Week 4 - Policy Gradient</a>
<ul>
<li class="toc-entry toc-h6"><a href="#module-4-learning-objectives">Module 4 Learning Objectives</a></li>
<li class="toc-entry toc-h6"><a href="#lesson-1-learning-parameterized-policies">Lesson 1: Learning Parameterized Policies</a></li>
<li class="toc-entry toc-h6"><a href="#lesson-2-policy-gradient-for-continuing-tasks">Lesson 2: Policy Gradient for Continuing Tasks</a></li>
<li class="toc-entry toc-h6"><a href="#lesson-3-actor-critic-for-continuing-tasks">Lesson 3: Actor-Critic for Continuing Tasks</a></li>
<li class="toc-entry toc-h6"><a href="#lesson-4-policy-parameterizations">Lesson 4: Policy Parameterizations</a></li>
<li class="toc-entry toc-h6"><a href="#assignment-3">Assignment</a></li>
</ul>
</li>
</ul><p>Coursera website:  <a href="https://www.coursera.org/learn/prediction-control-function-approximation/home/welcome">course 3 - Prediction and Control with Function Approximation</a> of <a href="https://www.coursera.org/specializations/reinforcement-learning">Reinforcement Learning Specialization</a></p>

<p>my notes on <a href="/guillaume_blog/blog/reinforcement-learning-specialization-coursera.html">course 1 - Fundamentals of Reinforcement Learning</a>, <a href="/guillaume_blog/blog/reinforcement-learning-specialization-coursera-course2.html">course 2 - Sample-based Learning Methods</a>, <a href="/guillaume_blog/blog/reinforcement-learning-specialization-coursera-course4.html">course 4 - A Complete Reinforcement Learning System (Capstone)</a></p>

<p><strong>specialization roadmap</strong> - course 3 - <strong>Prediction and Control with Function Approximation</strong> <a href="https://github.com/castorfou/Reinforcement-Learning-specialization/blob/main/course%203%20-%20function%20approximation/Course-3_Prediction-and-Control-with-Function-Approximation-Learning-Objectives.pdf">(syllabus)</a></p>

<p><strong>course 3</strong> - In Course 3, we leave the relative comfort of small finite MDPs and investigate RL with <strong>function approximation</strong>. Here we will see that the main concepts from Courses 1 and 2 transferred to problems with larger <strong>infinite state spaces</strong>. We will cover <strong>feature construction</strong>, <strong>neural network learning</strong>, <strong>policy gradient methods</strong>, and other particularities of the function approximation setting.</p>

<p>Week 1 - On-policy Prediction with Approximation</p>

<p>Week 2 - Constructing Features for Prediction</p>

<p>Week 3 - Control with Approximation</p>

<p>Week 4 - Policy Gradient</p>

<h6 id="course-introduction">
<a class="anchor" href="#course-introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Course introduction</h6>

<p><strong>Video by Adam and Martha</strong></p>

<p><img src="../images/RL_algo_in_a_tree.png" alt=""></p>

<p>In course 2:</p>

<p><img src="../images/RL_with_a_table.png" alt=""></p>

<p>In course 3:</p>

<p><img src="../images/RL_without_a_table.png" alt=""></p>

<h2 id="course-3---week-1---on-policy-prediction-with-approximation">
<a class="anchor" href="#course-3---week-1---on-policy-prediction-with-approximation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Course 3 - Week 1 - On-policy Prediction with Approximation</h2>

<h6 id="module-1-learning-objectives">
<a class="anchor" href="#module-1-learning-objectives" aria-hidden="true"><span class="octicon octicon-link"></span></a>Module 1 Learning Objectives</h6>

<p><strong>Lesson 1: Estimating Value Functions as Supervised Learning</strong></p>

<ul>
  <li>Understand how we can use parameterized functions to approximate value functions</li>
  <li>Explain the meaning of linear value function approximation</li>
  <li>Recognize that the tabular case is a special case of linear value function approximation.</li>
  <li>Understand that there are many ways to parameterize an approximate value function</li>
  <li>Understand what is meant by generalization and discrimination</li>
  <li>Understand how generalization can be beneficial</li>
  <li>Explain why we want both generalization and discrimination from our function approximation</li>
  <li>Understand how value estimation can be framed as a supervised learning problem</li>
  <li>Recognize not all function approximation methods are well suited for reinforcement learning</li>
</ul>

<p><strong>Lesson 2: The Objective for On-policy Prediction</strong></p>

<ul>
  <li>Understand the mean-squared value error objective for policy evaluation</li>
  <li>Explain the role of the state distribution in the objective</li>
  <li>Understand the idea behind gradient descent and stochastic gradient descent</li>
  <li>Outline the gradient Monte Carlo algorithm for value estimation</li>
  <li>Understand how state aggregation can be used to approximate the value function</li>
  <li>Apply Gradient Monte-Carlo with state aggregation</li>
</ul>

<p><strong>Lesson 3: The Objective for TD</strong></p>

<ul>
  <li>Understand the TD-update for function approximation</li>
  <li>Highlight the advantages of TD compared to Monte-Carlo</li>
  <li>Outline the Semi-gradient TD(0) algorithm for value estimation</li>
  <li>Understand that TD converges to a biased value estimate</li>
  <li>Understand that TD converges much faster than Gradient Monte Carlo</li>
</ul>

<p><strong>Lesson 4: Linear TD</strong></p>

<ul>
  <li>Derive the TD-update with linear function approximation</li>
  <li>Understand that tabular TD(0) is a special case of linear semi-gradient TD(0)</li>
  <li>Highlight the advantages of linear value function approximation over nonlinear</li>
  <li>Understand the fixed point of linear TD learning</li>
  <li>Describe a theoretical guarantee on the mean squared value error at the TD fixed point</li>
</ul>

<h6 id="lesson-1-estimating-value-functions-as-supervised-learning">
<a class="anchor" href="#lesson-1-estimating-value-functions-as-supervised-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Lesson 1: Estimating Value Functions as Supervised Learning</h6>

<p><strong>Reading</strong> Chapter 9.1-9.4 <strong>(pp. 197-209)</strong>  in the Reinforcement Learning textbook</p>

<blockquote>
  <p>In many of the tasks to which we would like to apply reinforcement learning the state space is combinatorial and enormous; the number of possible camera images, for example, is much larger than the number of atoms in the universe.</p>

  <p>In many of our target tasks, almost every state encountered will never have been seen before. To make sensible decisions in such states it is necessary to generalize from previous encounters with different states that are in some sense similar to the current one. In other words, the key issue is that of <strong>generalization</strong>. How can experience with a limited subset of the state space be usefully generalized to produce a good approximation over a much larger subset?</p>

  <p>Fortunately, generalization from examples has already been extensively studied, and
we do not need to invent totally new methods for use in reinforcement learning. To some extent we need only combine reinforcement learning methods with existing generalization methods. The kind of generalization we require is often called function approximation because it takes examples from a desired function (e.g., a value function) and attempts to generalize from them to construct an approximation of the entire function. Function approximation is an instance of supervised learning, the primary topic studied in machine learning, artificial neural networks, pattern recognition, and statistical curve fitting.</p>
</blockquote>

<p><strong>Video Moving to Parameterized Functions</strong> by Adam</p>

<p>By the end of this video, you’ll be able to <em>understand</em> how we can use <strong>parameterized functions</strong> to approximate values, <em>explain</em> <strong>linear value function approximation</strong>, <em>recognize</em> that the tabular case is a special case of linear value function approximation, and <em>understand</em> that there are many ways to parameterize an approximate value function.</p>

<p><strong>Video Generalization and Discrimination</strong> by Martha</p>

<p>By the end of this video, you will be able to <em>understand</em> what is meant by <strong>generalization</strong> and <strong>discrimination</strong>, <em>understand</em> how generalization can be beneficial, and <em>explain</em> why we want both generalization and discrimination from our function approximation.</p>

<p><strong>Video Framing Value Estimation as Supervised Learning</strong> by Martha</p>

<p>By the end of this video, you will be able to <em>understand</em> how value estimation can be framed as a <strong>supervised learning</strong> problem, and <em>recognize</em> that not all function approximation methods are well suited for reinforcement learning.</p>

<h6 id="lesson-2-the-objective-for-on-policy-prediction">
<a class="anchor" href="#lesson-2-the-objective-for-on-policy-prediction" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Lesson 2: The Objective for On-policy Prediction</strong>
</h6>

<p><strong>Video The Value Error Objective</strong> by Adam</p>

<p>By the end of this video you will be able to <em>understand</em> the <strong>Mean Squared Value Error objective</strong> for policy evaluation and <em>explain</em> the role of the <strong>state distribution</strong> in the objective.</p>

<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mrow><mi>V</mi><mi>E</mi></mrow><mo stretchy="true">‾</mo></mover><mo>=</mo><mstyle scriptlevel="0" displaystyle="true"><munder><mo>∑</mo><mi>s</mi></munder><mi>μ</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo stretchy="false">[</mo><msub><mi>v</mi><mi>π</mi></msub><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>−</mo><mover accent="true"><mi>v</mi><mo>^</mo></mover><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>w</mi><mo stretchy="false">)</mo><msup><mo stretchy="false">]</mo><mn>2</mn></msup></mstyle></mrow><annotation encoding="application/x-tex">\overline{VE}=\displaystyle\sum_{s}\mu(s)[v_\pi(s)-\hat{v}(s,w)]^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8833em;"></span><span class="mord overline"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8833em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span></span></span><span style="top:-3.8033em;"><span class="pstrut" style="height:3em;"></span><span class="overline-line" style="border-bottom-width:0.04em;"></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.3em;vertical-align:-1.25em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.9em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.25em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">μ</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">π</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1141em;vertical-align:-0.25em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.2222em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mclose">)</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>
This is the <strong>Mean Squared Value Error Objective</strong> where $\mu$ reflects how much we care about each state (a probability distribution)</p>

<p><strong>Video Introducing Gradient Descent</strong> by Martha</p>

<p>By the end of this video, you will be able to <em>understand</em> the idea of <strong>gradient descent</strong>, and <em>understand</em> that gradient descent converges to stationary points.</p>

<p><strong>Video Gradient Monte for Policy Evaluation</strong> by Martha</p>

<p>By the end of this video, you will be able to <em>understand</em> how to use gradient descent and <strong>stochastic gradient descent</strong> to minimize value error and <em>outline</em> the <strong>Gradient Monte Carlo</strong> algorithm for value estimation.</p>

<p><img src="../images/C3W1_gradient_monte_carlo.png" alt=""></p>

<p><strong>Video State Aggregation with Monte Carlo</strong> by Adam</p>

<p>By the end of this video, you will be able to <em>understand</em> how <strong>state aggregation</strong> can be used to approximate the value function and <em>apply</em> gradient Monte Carlo with state aggregation.</p>

<h6 id="lesson-3-the-objective-for-td">
<a class="anchor" href="#lesson-3-the-objective-for-td" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Lesson 3: The Objective for TD</strong>
</h6>

<p><strong>Video Semi-Gradient TD for Policy Evaluation</strong> by Adam</p>

<p>By the end of this video you will be able to <em>understand</em> the <strong>TD update</strong> for function approximation, and <em>outline</em> the <strong>semi-gradient TD(0)</strong> algorithm for value estimation.</p>

<p><img src="../images/C3W1_semi_gradient_TD0.png" alt=""></p>

<p><strong>Video Comparing TD and Monte Carlo with State Aggregation</strong> by Adam</p>

<p>By the end of this video, you’ll be able to <em>understand</em> that TD converges to a bias value estimate and <em>understand</em> that TD can learn faster than Gradient Monte Carlo.</p>

<p><strong>Video Doina Precup: Building Knowledge for AI Agents with Reinforcement Learning</strong></p>

<h6 id="lesson-4-linear-td">
<a class="anchor" href="#lesson-4-linear-td" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Lesson 4: Linear TD</strong>
</h6>

<p><strong>Video The Linear TD Update</strong> by Martha</p>

<p>By the end of this video, you’ll be able to <em>derive</em> the TD update with linear function approximation, <em>understand</em> that tabular TD(0) as a special case of <strong>linear semi gradient TD(0)</strong>, and <em>understand</em> why we care about linear TD as a special case.</p>

<p><strong>Video The True Objective for TD</strong> by Martha</p>

<p>By the end of this video, you will be able to <em>understand</em> the <strong>fixed point</strong> of linear TD and <em>describe</em> a theoretical guarantee on the mean squared value error at the TD fixed point.</p>

<p><strong>Video Week 1 Summary</strong> by Adam</p>

<h6 id="assignment">
<a class="anchor" href="#assignment" aria-hidden="true"><span class="octicon octicon-link"></span></a>Assignment</h6>

<p>TD with State Aggregation</p>

<p>notebooks in <a href="https://github.com/castorfou/Reinforcement-Learning-specialization/tree/main/assignements/course%203%20week%201">github</a></p>

<h2 id="course-3---week-2---constructing-features-for-prediction">
<a class="anchor" href="#course-3---week-2---constructing-features-for-prediction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Course 3 - Week 2 - Constructing Features for Prediction</h2>

<h6 id="module-2-learning-objectives">
<a class="anchor" href="#module-2-learning-objectives" aria-hidden="true"><span class="octicon octicon-link"></span></a>Module 2 Learning Objectives</h6>

<p><strong>Lesson 1: Feature Construction for Linear Methods</strong></p>

<ul>
  <li>Describe the difference between coarse coding and tabular representations</li>
  <li>Explain the trade-off when designing representations between discrimination and generalization</li>
  <li>Understand how different coarse coding schemes affect the functions that can be represented</li>
  <li>Explain how tile coding is a (computationally?) convenient case of coarse coding</li>
  <li>Describe how designing the tilings affects the resultant representation</li>
  <li>Understand that tile coding is a computationally efficient implementation of coarse coding</li>
</ul>

<p><strong>Lesson 2: Neural Networks</strong></p>

<ul>
  <li>Define a neural network</li>
  <li>Define activation functions</li>
  <li>Define a feedforward architecture</li>
  <li>Understand how neural networks are doing feature construction</li>
  <li>Understand how neural networks are a non-linear function of state</li>
  <li>Understand how deep networks are a composition of layers</li>
  <li>Understand the tradeoff between learning capacity and challenges presented by deeper networks</li>
</ul>

<p><strong>Lesson 3: Training Neural Networks</strong></p>

<ul>
  <li>Compute the gradient of a single hidden layer neural network</li>
  <li>Understand how to compute the gradient for arbitrarily deep networks</li>
  <li>Understand the importance of initialization for neural networks</li>
  <li>Describe strategies for initializing neural networks</li>
  <li>Describe optimization techniques for training neural networks</li>
</ul>

<h6 id="lesson-1-feature-construction-for-linear-methods">
<a class="anchor" href="#lesson-1-feature-construction-for-linear-methods" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Lesson 1: Feature Construction for Linear Methods</strong>
</h6>

<p><strong>Reading</strong> Chapter 9.4-9.5.0 <strong>(pp. 204-210)</strong>, 9.5.3-9.5.4 <strong>(pp. 215-222)</strong> and 9.7 <strong>(pp. 223-228)</strong> in the Reinforcement Learning textbook</p>

<p><strong>Video Coarse Coding</strong> by Adam</p>

<p>By the end of this video, you’ll be able to <em>describe</em> <strong>coarse coding</strong> and <em>describe</em> how it relates to <strong>state aggregation</strong>.</p>

<p><img src="../images/C3W2_coarse_coding.png" alt=""></p>

<p><strong>Video Generalization Properties of Coarse Coding</strong> by Martha</p>

<p>By the end of this video, you’ll be able to <em>describe</em> how <strong>coarse coding parameters</strong> affect <strong>generalization</strong> and <strong>discrimination</strong>, and <em>understand</em> how that affects <strong>learning speed and accuracy</strong>.</p>

<p><strong>Video Tile Coding</strong> by Adam</p>

<p>By the end of this video, you’ll be able to <em>explain</em> how <strong>tile coding</strong> achieves both <strong>generalization</strong> and <strong>discrimination</strong>, and <em>understand</em> the benefits and limitations of tile coding.</p>

<p><strong>Video Using Tile Coding in TD</strong> by Adam</p>

<p>By the end of this video, you’ll be able to <em>explain</em> how to use <strong>tile coding</strong> with <strong>TD learning</strong> and <em>identify</em> important properties of <strong>tile code representations</strong>.</p>

<h6 id="lesson-2-neural-networks">
<a class="anchor" href="#lesson-2-neural-networks" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Lesson 2: Neural Networks</strong>
</h6>

<p><strong>Video What is a Neural Network?</strong> by Martha</p>

<p>By the end of this video, you’ll be able to <em>define</em> a <strong>neural network</strong>, <em>define</em> an <strong>activation function</strong> and <em>understand</em> how a neural network is a <strong>parameterized function</strong>.</p>

<p><strong>Video Non-linear Approximation with Neural Networks</strong> by Martha</p>

<p>By the end of this video, you will <em>understand</em> how neural networks do <strong>feature construction</strong>, and <em>understand</em> how neural networks are a <strong>non-linear function</strong> of state.</p>

<p><strong>Video Deep Neural Networks</strong> by Adam</p>

<p>By the end of this video, you will <em>understand</em> how <strong>deep neural networks</strong> are composed of <strong>many layers</strong> and <em>understand</em> that <strong>depth</strong> can facilitate learning <strong>features</strong> through <strong>composition</strong> and <strong>abstraction</strong>.</p>

<h6 id="lesson-3-training-neural-networks">
<a class="anchor" href="#lesson-3-training-neural-networks" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Lesson 3: Training Neural Networks</strong>
</h6>

<p><strong>Video Gradient Descent for Training Neural Networks</strong> by Martha</p>

<p>By the end of this video, you’ll be able to <em>derive</em> the <strong>gradient</strong> of a neural network and <em>implement</em> <strong>gradient descent</strong> on a neural network.</p>

<p><strong>Video Optimization Strategies for NNs</strong> by Martha</p>

<p>By the end of this video, you will be able to <em>understand</em> the importance of <strong>initialization</strong> for neural networks and <em>describe</em> <strong>optimization techniques</strong> for training neural networks.</p>

<p><strong>Video David Silver on Deep Learning + RL = AI?</strong></p>

<p><strong>Video Week 2 Review</strong> by Adam</p>

<h6 id="assignment-1">
<a class="anchor" href="#assignment-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Assignment</h6>

<p>Semi-gradient TD with a Neural Network</p>

<p>notebooks in <a href="https://github.com/castorfou/Reinforcement-Learning-specialization/tree/main/assignements/course%203%20week%202">github</a></p>

<h2 id="course-3---week-3---control-with-approximation">
<a class="anchor" href="#course-3---week-3---control-with-approximation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Course 3 - Week 3 - Control with Approximation</h2>

<h6 id="module-3-learning-objectives">
<a class="anchor" href="#module-3-learning-objectives" aria-hidden="true"><span class="octicon octicon-link"></span></a>Module 3 Learning Objectives</h6>

<p><strong>Lesson 1: Episodic Sarsa with Function Approximation</strong></p>

<ul>
  <li>Explain the update for Episodic Sarsa with function approximation</li>
  <li>Introduce the feature choices, including passing actions to features or stacking state features</li>
  <li>Visualize value function and learning curves</li>
  <li>Discuss how this extends to Q-learning easily, since it is a subset of Expected Sarsa</li>
</ul>

<p><strong>Lesson 2: Exploration under Function Approximation</strong></p>

<ul>
  <li>Understanding optimistically initializing your value function as a form of exploration</li>
</ul>

<p><strong>Lesson 3: Average Reward</strong></p>

<ul>
  <li>Describe the average reward setting</li>
  <li>Explain when average reward optimal policies are different from discounted solutions</li>
  <li>Understand how differential value functions are different from discounted value functions</li>
</ul>

<h6 id="lesson-1-episodic-sarsa-with-function-approximation">
<a class="anchor" href="#lesson-1-episodic-sarsa-with-function-approximation" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Lesson 1: Episodic Sarsa with Function Approximation</strong>
</h6>

<p><strong>Reading</strong> Chapter 10 <strong>(pp. 243-246)</strong> and 10.3 <strong>(pp. 249-252)</strong> in the Reinforcement Learning textbook</p>

<p><strong>Video Episodic Sarsa with Function Approximation</strong> by Adam</p>

<p>By the end of this video, you’ll be able to <em>understand</em> how to construct <strong>action-dependent features</strong> for approximate action values and <em>explain</em> how to use <strong>Sarsa</strong> in <strong>episodic tasks</strong> with <strong>function approximation</strong>.</p>

<p><img src="../images/C3W3_episodic_sarsa_appro.png" alt=""></p>

<p><strong>Video Episodic Sarsa in Mountain Car</strong> by Adam</p>

<p>By the end of this video, you will <em>gain experience</em> analyzing the performance of an <strong>approximate TD control</strong> method.</p>

<p><strong>Video Expected Sarsa with Function Approximation</strong> by Adam</p>

<p>By the end of this video, you’ll be able to <em>explain</em> the update for <strong>expected Sarsa</strong> with <strong>function approximation</strong>, and <em>explain</em> the update for <strong>Q-learning</strong> with <strong>function approximation</strong>.</p>

<p><img src="../images/C3W3_expected_sarsa_q_learning.png" alt=""></p>

<h6 id="lesson-2-exploration-under-function-approximation">
<a class="anchor" href="#lesson-2-exploration-under-function-approximation" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Lesson 2: Exploration under Function Approximation</strong>
</h6>

<p><strong>Video Exploration under Function Approximation</strong> by Martha</p>

<p>By the end of this video, you’ll be able to <em>describe</em> how <strong>optimistic initial values</strong> and <strong>$\epsilon$-greedy</strong> can be used with <strong>function approximation</strong>.</p>

<h6 id="lesson-3-average-reward">
<a class="anchor" href="#lesson-3-average-reward" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Lesson 3: Average Reward</strong>
</h6>

<p><strong>Video Average Reward: A New Way of Formulating Control Problems</strong> by Martha</p>

<p>By the end of this video, you’ll be able to <em>describe</em> the <strong>average reward</strong> setting, <em>explain</em> when <strong>average reward</strong> optimal policies are different from policies obtained under discounting and <em>understand</em> <strong>differential value functions</strong>.</p>

<p><strong>Satinder Singh on Intrinsic Rewards</strong></p>

<p><strong>Video Week 3 Review</strong> by Martha</p>

<h6 id="assignment-2">
<a class="anchor" href="#assignment-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Assignment</h6>

<p>Function Approximation and Control</p>

<p>notebooks in <a href="https://github.com/castorfou/Reinforcement-Learning-specialization/tree/main/assignements/course%203%20week%203">github</a></p>

<h2 id="course-3---week-4---policy-gradient">
<a class="anchor" href="#course-3---week-4---policy-gradient" aria-hidden="true"><span class="octicon octicon-link"></span></a>Course 3 - Week 4 - Policy Gradient</h2>

<h6 id="module-4-learning-objectives">
<a class="anchor" href="#module-4-learning-objectives" aria-hidden="true"><span class="octicon octicon-link"></span></a>Module 4 Learning Objectives</h6>

<p><strong>Lesson 1: Learning Parameterized Policies</strong></p>

<ul>
  <li>Understand how to define policies as parameterized functions</li>
  <li>Define one class of parameterized policies based on the softmax function</li>
  <li>Understand the advantages of using parameterized policies over action-value based methods</li>
</ul>

<p><strong>Lesson 2: Policy Gradient for Continuing Tasks</strong></p>

<ul>
  <li>Describe the objective for policy gradient algorithms</li>
  <li>Describe the results of the policy gradient theorem</li>
  <li>Understand the importance of the policy gradient theorem</li>
</ul>

<p><strong>Lesson 3: Actor-Critic for Continuing Tasks</strong></p>

<ul>
  <li>Derive a sample-based estimate for the gradient of the average reward objective</li>
  <li>Describe the actor-critic algorithm for control with function approximation, for continuing tasks</li>
</ul>

<p><strong>Lesson 4: Policy Parameterizations</strong></p>

<ul>
  <li>Derive the actor-critic update for a softmax policy with linear action preferences</li>
  <li>Implement this algorithm</li>
  <li>Design concrete function approximators for an average reward actor-critic algorithm</li>
  <li>Analyze the performance of an average reward agent</li>
  <li>Derive the actor-critic update for a gaussian policy</li>
  <li>Apply average reward actor-critic with a gaussian policy to a particular task with continuous actions</li>
</ul>

<h6 id="lesson-1-learning-parameterized-policies">
<a class="anchor" href="#lesson-1-learning-parameterized-policies" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Lesson 1: Learning Parameterized Policies</strong>
</h6>

<p><strong>Reading</strong> Chapter 13 <strong>(pp. 321-336)</strong> in the Reinforcement Learning textbook</p>

<p><strong>Video Learning Policies Directly</strong> by Adam</p>

<p>By the end of this video, you’ll be able to <em>understand</em> how to define policies as <strong>parameterized functions</strong> and <em>define</em> one class of parametrized policies based on the <strong>softmax</strong> function.</p>

<p><strong>Video Advantages of Policy Parameterization</strong> by Adam</p>

<p>By the end of this video, you’ll be able to <em>understand</em> some of the advantages of using parameterized policies.</p>

<h6 id="lesson-2-policy-gradient-for-continuing-tasks">
<a class="anchor" href="#lesson-2-policy-gradient-for-continuing-tasks" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Lesson 2: Policy Gradient for Continuing Tasks</strong>
</h6>

<p><strong>Video The Objective for Learning Policies</strong> by Martha</p>

<p>By the end of this video, you’ll be able to <em>describe</em> the objective for <strong>policy gradient algorithms</strong>.</p>

<p><img src="../images/C3W4_objective_policy_gradient.png" alt=""></p>

<p><strong>Video The Policy Gradient Theorem</strong> by Martha</p>

<p>By the end of this video, you will be able to <em>describe</em> the result of the <strong>policy gradient theorem</strong> and <em>understand</em> the importance of the policy gradient theorem.</p>

<p><img src="../images/C3W4_policy_gradient_theorem.png" alt=""></p>

<h6 id="lesson-3-actor-critic-for-continuing-tasks">
<a class="anchor" href="#lesson-3-actor-critic-for-continuing-tasks" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Lesson 3: Actor-Critic for Continuing Tasks</strong>
</h6>

<p><strong>Video Estimating the Policy Gradient</strong> by Martha</p>

<p>By the end of this video, you will be able to <em>derive</em> a <strong>sample-based estimate</strong> for the gradient of the average reward objective.</p>

<p><img src="../images/C3W4_stochastic_gradient_ascent.png" alt=""></p>

<p><strong>Video Actor-Critic Algorithm</strong> by Adam</p>

<p>By the end of this video, you’ll be able to <em>describe</em> the <strong>actor-critic algorithm</strong> for control with function approximation for continuing tasks.</p>

<p><img src="../images/C3W4_actor_critic_algorithm.png" alt=""></p>

<h6 id="lesson-4-policy-parameterizations">
<a class="anchor" href="#lesson-4-policy-parameterizations" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Lesson 4: Policy Parameterizations</strong>
</h6>

<p><strong>Video Actor-Critic with Softmax Policies</strong> by Adam</p>

<p>By the end of this video you’ll be able to <em>derive</em> the actor critic update for a Softmax policy with linear action preferences and <em>implement</em> this algorithm.</p>

<p><strong>Video Demonstration with Actor-Critic</strong> by Adam</p>

<p>By the end of this video, you’ll be able to <em>design</em> a function approximator for an average reward actor-critic algorithm and <em>analyze</em> the performance of an average reward agent.</p>

<p><strong>Video Gaussian Policies for Continuous Actions</strong> by Martha</p>

<p>By the end of this video, you’ll be able to <em>derive</em> the actor-critic update for a <strong>Gaussian policy</strong> and <em>apply</em> average reward actor-critic with a Gaussian policy to task with continuous actions.</p>

<p><strong>Video Week 4 Summary</strong> by Martha</p>

<p><img src="../images/C3W4_actor_critic_actions.png" alt=""></p>

<h6 id="assignment-3">
<a class="anchor" href="#assignment-3" aria-hidden="true"><span class="octicon octicon-link"></span></a>Assignment</h6>

<p>Average Reward Softmax Actor-Critic using Tile-coding</p>

<p>notebooks in <a href="https://github.com/castorfou/Reinforcement-Learning-specialization/tree/main/assignements/course%203%20week%204">github</a></p>


  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="castorfou/guillaume_blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/guillaume_blog/blog/reinforcement-learning-specialization-coursera-course3.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/guillaume_blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/guillaume_blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/guillaume_blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Journey for a datascientist</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/castorfou" target="_blank" title="castorfou"><svg class="svg-icon grey"><use xlink:href="/guillaume_blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/GuillaumeRamel1" target="_blank" title="GuillaumeRamel1"><svg class="svg-icon grey"><use xlink:href="/guillaume_blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
