<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Reinforcement Learning Specialization - Coursera - course 2 - Sample-based Learning Methods | Guillaume’s blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Reinforcement Learning Specialization - Coursera - course 2 - Sample-based Learning Methods" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="From University of Alberta. My notes on course 2." />
<meta property="og:description" content="From University of Alberta. My notes on course 2." />
<link rel="canonical" href="https://castorfou.github.io/guillaume_blog/blog/reinforcement-learning-specialization-coursera-course2.html" />
<meta property="og:url" content="https://castorfou.github.io/guillaume_blog/blog/reinforcement-learning-specialization-coursera-course2.html" />
<meta property="og:site_name" content="Guillaume’s blog" />
<meta property="og:image" content="https://castorfou.github.io/guillaume_blog/images/RL.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-05-25T00:00:00-05:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://castorfou.github.io/guillaume_blog/images/RL.png" />
<meta property="twitter:title" content="Reinforcement Learning Specialization - Coursera - course 2 - Sample-based Learning Methods" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2021-05-25T00:00:00-05:00","datePublished":"2021-05-25T00:00:00-05:00","description":"From University of Alberta. My notes on course 2.","headline":"Reinforcement Learning Specialization - Coursera - course 2 - Sample-based Learning Methods","image":"https://castorfou.github.io/guillaume_blog/images/RL.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://castorfou.github.io/guillaume_blog/blog/reinforcement-learning-specialization-coursera-course2.html"},"url":"https://castorfou.github.io/guillaume_blog/blog/reinforcement-learning-specialization-coursera-course2.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/guillaume_blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://castorfou.github.io/guillaume_blog/feed.xml" title="Guillaume's blog" /><link rel="shortcut icon" type="image/x-icon" href="/guillaume_blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/guillaume_blog/">Guillaume&#39;s blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/guillaume_blog/about/">About Me</a><a class="page-link" href="/guillaume_blog/search/">Search</a><a class="page-link" href="/guillaume_blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Reinforcement Learning Specialization - Coursera - course 2 - Sample-based Learning Methods</h1><p class="page-description">From University of Alberta. My notes on course 2.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-05-25T00:00:00-05:00" itemprop="datePublished">
        May 25, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      13 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/guillaume_blog/categories/#reinforcement learning">reinforcement learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/guillaume_blog/categories/#deepmind">deepmind</a>
        &nbsp;
      
        <a class="category-tags-link" href="/guillaume_blog/categories/#coursera">coursera</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h2"><a href="#course-2---week-1---monte-carlo-methods-for-prediction--control">Course 2 - Week 1 - Monte-Carlo Methods for Prediction &amp; Control</a>
<ul>
<li class="toc-entry toc-h6"><a href="#module-1-learning-objectives">Module 1 Learning Objectives</a></li>
<li class="toc-entry toc-h6"><a href="#lesson-1-introduction-to-monte-carlo-methods">Lesson 1: Introduction to Monte Carlo Methods</a></li>
<li class="toc-entry toc-h6"><a href="#lesson-2-monte-carlo-for-control">Lesson 2: Monte Carlo for Control</a></li>
<li class="toc-entry toc-h6"><a href="#lesson-3-exploration-methods-for-monte-carlo">Lesson 3: Exploration Methods for Monte Carlo</a></li>
<li class="toc-entry toc-h6"><a href="#lesson-4-off-policy-learning-for-prediction">Lesson 4: Off-policy Learning for Prediction</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#course-2---week-2---temporal-difference-learning-methods-for-prediction">Course 2 - Week 2 - Temporal Difference Learning Methods for Prediction</a>
<ul>
<li class="toc-entry toc-h6"><a href="#module-2-learning-objectives">Module 2 Learning Objectives</a></li>
<li class="toc-entry toc-h6"><a href="#lesson-1-introduction-to-temporal-difference-learning">Lesson 1: Introduction to Temporal Difference Learning</a></li>
<li class="toc-entry toc-h6"><a href="#lesson-2-advantages-of-td">Lesson 2: Advantages of TD</a></li>
<li class="toc-entry toc-h6"><a href="#assignment">Assignment</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#course-2---week-3---temporal-difference-learning-methods-for-control">Course 2 - Week 3 - Temporal Difference Learning Methods for Control</a>
<ul>
<li class="toc-entry toc-h6"><a href="#module-3-learning-objectives">Module 3 Learning Objectives</a></li>
<li class="toc-entry toc-h6"><a href="#lesson-1-td-for-control">Lesson 1: TD for Control</a></li>
<li class="toc-entry toc-h6"><a href="#lesson-2-off-policy-td-control-q-learning">Lesson 2: Off-policy TD Control: Q-learning</a></li>
<li class="toc-entry toc-h6"><a href="#lesson-3-expected-sarsa">Lesson 3: Expected Sarsa</a></li>
<li class="toc-entry toc-h6"><a href="#assignment-1">Assignment</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#course-2---week-4---planning-learning--acting">Course 2 - Week 4 - Planning, Learning &amp; Acting</a>
<ul>
<li class="toc-entry toc-h6"><a href="#module-4-learning-objectives">Module 4 Learning Objectives</a></li>
<li class="toc-entry toc-h6"><a href="#lesson-1-what-is-a-model">Lesson 1: What is a model?</a></li>
<li class="toc-entry toc-h6"><a href="#lesson-2-planning">Lesson 2: Planning</a></li>
<li class="toc-entry toc-h6"><a href="#lesson-3-dyna-as-a-formalism-for-planning">Lesson 3: Dyna as a formalism for planning</a></li>
<li class="toc-entry toc-h6"><a href="#lesson-4-dealing-with-inaccurate-models">Lesson 4: Dealing with inaccurate models</a></li>
<li class="toc-entry toc-h6"><a href="#assignment-2">Assignment</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#text-book-part-1-summary">Text Book Part 1 Summary</a>
<ul>
<li class="toc-entry toc-h6"><a href="#course-wrap-up">Course wrap-up</a></li>
</ul>
</li>
</ul><p>Coursera website:  <a href="https://www.coursera.org/learn/sample-based-learning-methods/home/welcome">course 2 - Sample-based Learning Methods</a> of <a href="https://www.coursera.org/specializations/reinforcement-learning">Reinforcement Learning Specialization</a></p>

<p>my notes on <a href="/guillaume_blog/blog/reinforcement-learning-specialization-coursera.html">course 1 - Fundamentals of Reinforcement Learning</a>, <a href="/guillaume_blog/blog/reinforcement-learning-specialization-coursera-course3.html">course 3 - Prediction and Control with Function Approximation</a>, <a href="/guillaume_blog/blog/reinforcement-learning-specialization-coursera-course4.html">course 4 - A Complete Reinforcement Learning System (Capstone)</a></p>

<p><strong>specialization roadmap</strong> - course 2 - <strong>Sample-based Learning Methods</strong></p>

<p><strong>course 2</strong> - In Course 2, we built on these ideas and design algorithms for learning <strong>without a model</strong> of the world. We study three classes of methods designed for learning from trial and error interaction. We start with <strong>Monte Carlo</strong> methods and then move on to <strong>temporal difference</strong> learning, including Q learning. We conclude Course 2 with an investigation of methods for <strong>planning</strong> with learned models.</p>

<p>Week 1 - Monte-Carlo Methods for Prediction &amp; Control</p>

<p>Week 2 - Temporal Difference Learning Methods for Prediction</p>

<p>Week 3 - Temporal Difference Learning Methods for Control</p>

<p>Week 4 - Planning, Learning &amp; Acting</p>

<h2 id="course-2---week-1---monte-carlo-methods-for-prediction--control">
<a class="anchor" href="#course-2---week-1---monte-carlo-methods-for-prediction--control" aria-hidden="true"><span class="octicon octicon-link"></span></a>Course 2 - Week 1 - Monte-Carlo Methods for Prediction &amp; Control</h2>

<h6 id="module-1-learning-objectives">
<a class="anchor" href="#module-1-learning-objectives" aria-hidden="true"><span class="octicon octicon-link"></span></a>Module 1 Learning Objectives</h6>

<p><strong>Lesson 1: Introduction to Monte-Carlo Methods</strong></p>

<ul>
  <li>Understand how Monte-Carlo methods can be used to estimate value functions from sampled interaction</li>
  <li>Identify problems that can be solved using Monte-Carlo methods</li>
  <li>Use Monte-Carlo prediction to estimate the value function for a given policy.</li>
</ul>

<p><strong>Lesson 2: Monte-Carlo for Control</strong></p>

<ul>
  <li>Estimate action-value functions using Monte-Carlo</li>
  <li>Understand the importance of maintaining exploration in Monte-Carlo algorithms</li>
  <li>Understand how to use Monte-Carlo methods to implement a GPI algorithm</li>
  <li>Apply Monte-Carlo with exploring starts to solve an MDP</li>
</ul>

<p><strong>Lesson 3: Exploration Methods for Monte-Carlo</strong></p>

<ul>
  <li>Understand why exploring starts can be problematic in real problems</li>
  <li>Describe an alternative exploration method for Monte-Carlo control</li>
</ul>

<p><strong>Lesson 4: Off-policy learning for prediction</strong></p>

<ul>
  <li>Understand how off-policy learning can help deal with the exploration problem</li>
  <li>Produce examples of target policies and examples of behavior policies</li>
  <li>Understand importance sampling</li>
  <li>Use importance sampling to estimate the expected value of a target distribution using samples from a different distribution</li>
  <li>Understand how to use importance sampling to correct returns</li>
  <li>Understand how to modify the Monte-Carlo prediction algorithm for off-policy learning.</li>
</ul>

<h6 id="lesson-1-introduction-to-monte-carlo-methods">
<a class="anchor" href="#lesson-1-introduction-to-monte-carlo-methods" aria-hidden="true"><span class="octicon octicon-link"></span></a>Lesson 1: Introduction to Monte Carlo Methods</h6>

<p><strong>Reading</strong> Chapter 5.0-5.5 <strong>(pp. 91-104)</strong>  in the Reinforcement Learning textbook</p>

<blockquote>
  <p>Although a model is required, the model need only generate sample transitions, not the complete probability distributions of all possible transitions that is required for dynamic programming (DP).</p>
</blockquote>

<p><strong>Video What is Monte Carlo</strong> by Martha</p>

<p>By the end of this video you will be able to <em>understand</em> how <strong>Monte Carlo</strong> methods can be used to estimate value functions from sampled interaction and <em>identify</em> problems that can be solved using Monte Carlo methods.</p>

<p><strong>Video Using Monte Carlo for Prediction</strong> by Martha</p>

<p>By the end of this video, you will be able to <em>use</em> <strong>Monte Carlo prediction</strong> to estimate the value function for a given policy.</p>

<p><img src="../images/C2W1_1_mc_algo.png" alt=""></p>

<h6 id="lesson-2-monte-carlo-for-control">
<a class="anchor" href="#lesson-2-monte-carlo-for-control" aria-hidden="true"><span class="octicon octicon-link"></span></a>Lesson 2: Monte Carlo for Control</h6>

<p><strong>Video Using Monte Carlo for Action Values</strong> by Adam</p>

<p>By the end of this video, you’ll be able to <em>estimate</em> <strong>action-value functions</strong> using Monte Carlo and <em>understand</em> the importance of <strong>maintaining exploration</strong> in Monte Carlo algorithms.</p>

<p><strong>Video Using Monte Carlo methods for generalized policy iteration</strong> by Adam</p>

<p>By the end of this video, you will <em>understand</em> how to use Monte Carlo methods to implement a <strong>generalized policy iteration</strong> GPI algorithm.</p>

<p><img src="../images/C2W1_1_gpi.png" alt=""></p>

<p><strong>Video Solving the BlackJack Example</strong> by Adam</p>

<p>By the end of this video, you’ll be able to <em>apply</em> <strong>Monte Carlo with Exploring Starts</strong> to solve an example MDP.</p>

<h6 id="lesson-3-exploration-methods-for-monte-carlo">
<a class="anchor" href="#lesson-3-exploration-methods-for-monte-carlo" aria-hidden="true"><span class="octicon octicon-link"></span></a>Lesson 3: Exploration Methods for Monte Carlo</h6>

<p><strong>Video Epsilon-soft policies</strong> by Adam</p>

<p>By the end of this video you will <em>understand</em> why exploring starts can be problematic in real problems and you will be able to <em>describe</em> an alternative expiration method to <strong>maintain exploration</strong> in Monte Carlo control.</p>

<h6 id="lesson-4-off-policy-learning-for-prediction">
<a class="anchor" href="#lesson-4-off-policy-learning-for-prediction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Lesson 4: Off-policy Learning for Prediction</h6>

<p><strong>Video Why does off-policy learning matter?</strong> by Martha</p>

<p>By the end of this video you will be able to <em>understand</em> how <strong>off policy learning</strong> can help deal with the expiration problem. You will also be able to <em>produce</em> examples of Target policies and examples of <strong>behavior policies</strong>.</p>

<p>The key points to take away from today are that <strong>off policy learning</strong> is another way to obtain <em>continual exploration</em>. The policy that we are <em>learning</em> is called the <strong>target policy</strong> and the policy that we are choosing <em>actions</em> from is the <strong>behavior policy</strong>.</p>

<p><strong>Video Importance Sampling</strong> by Martha</p>

<p>By the end of this video, you will be able to <em>use</em> <strong>importance sampling</strong> to estimate the expected value of a target distribution using samples from a different distribution.</p>

<p><strong>Video Off-Policy Monte Carlo Prediction</strong> by Martha</p>

<p>By the end of this video, you will be able to <em>understand</em> how to use <strong>important sampling</strong> to correct returns, and you will <em>understand</em> how to modify the <strong>Monte Carlo prediction algorithm</strong> for off-policy learning.</p>

<p><strong>Video Emma Brunskill: Batch Reinforcement Learning</strong></p>

<p><strong>Video Week 1 Summary</strong> by Martha</p>

<p><strong>Reading</strong> Chapter 5.10 <strong>(pp. 115-116)</strong>  in the Reinforcement Learning textbook</p>

<h2 id="course-2---week-2---temporal-difference-learning-methods-for-prediction">
<a class="anchor" href="#course-2---week-2---temporal-difference-learning-methods-for-prediction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Course 2 - Week 2 - Temporal Difference Learning Methods for Prediction</h2>

<h6 id="module-2-learning-objectives">
<a class="anchor" href="#module-2-learning-objectives" aria-hidden="true"><span class="octicon octicon-link"></span></a>Module 2 Learning Objectives</h6>

<p><strong>Lesson 1: Introduction to Temporal Difference Learning</strong></p>

<ul>
  <li>Define temporal-difference learning</li>
  <li>Define the temporal-difference error</li>
  <li>Understand the TD(0) algorithm</li>
</ul>

<p><strong>Lesson 2: Advantages of TD</strong></p>

<ul>
  <li>Understand the benefits of learning online with TD</li>
  <li>Identify key advantages of TD methods over Dynamic Programming and Monte Carlo methods</li>
  <li>Identify the empirical benefits of TD learning</li>
</ul>

<h6 id="lesson-1-introduction-to-temporal-difference-learning">
<a class="anchor" href="#lesson-1-introduction-to-temporal-difference-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Lesson 1: Introduction to Temporal Difference Learning</h6>

<p><strong>Reading</strong> Chapter 6-6.3 <strong>(pp. 116-128)</strong>  in the Reinforcement Learning textbook</p>

<p><strong>Video What is Temporal Difference (TD) learning?</strong> by Adam</p>

<p>By the end of this video, you’ll be able to <em>define</em> <strong>temporal difference learning</strong>, <em>define</em> the <strong>temporal difference error</strong>, and <em>understand</em> the <strong>TD(0) algorithm</strong>.</p>

<p><img src="../images/C2W2_1_td0_algo.png" alt=""></p>

<p><strong>Video Rich Sutton: The Importance of TD Learning</strong> by Richard Sutton</p>

<h6 id="lesson-2-advantages-of-td">
<a class="anchor" href="#lesson-2-advantages-of-td" aria-hidden="true"><span class="octicon octicon-link"></span></a>Lesson 2: Advantages of TD</h6>

<p><strong>Video The advantages of temporal difference learning</strong> by Martha</p>

<p>By the end of this video, you will be able to <em>understand</em> the benefits of <strong>learning online with TD</strong> and <em>identify</em> key advantages of TD methods over dynamic programming and Monte Carlo.</p>

<p><strong>Video Comparing TD and Monte Carlo</strong> by Adam</p>

<p>By the end of this video, you’ll be able to <em>identify</em> the <strong>empirical benefits</strong> of <strong>TD Learning</strong>.</p>

<p><strong>Video Andy Barto and Rich Sutton: More on the History of RL</strong></p>

<p><strong>Video Week 2 Summary</strong> by Adam</p>

<h6 id="assignment">
<a class="anchor" href="#assignment" aria-hidden="true"><span class="octicon octicon-link"></span></a>Assignment</h6>

<p>Policy Evaluation in Cliff Walking Environment</p>

<p>notebooks in <a href="https://github.com/castorfou/Reinforcement-Learning-specialization/tree/main/assignements/course%202%20week%202">github</a></p>

<h2 id="course-2---week-3---temporal-difference-learning-methods-for-control">
<a class="anchor" href="#course-2---week-3---temporal-difference-learning-methods-for-control" aria-hidden="true"><span class="octicon octicon-link"></span></a>Course 2 - Week 3 - Temporal Difference Learning Methods for Control</h2>

<h6 id="module-3-learning-objectives">
<a class="anchor" href="#module-3-learning-objectives" aria-hidden="true"><span class="octicon octicon-link"></span></a>Module 3 Learning Objectives</h6>

<p><strong>Lesson 1: TD for Control</strong></p>

<ul>
  <li>Explain how generalized policy iteration can be used with TD to find improved policies</li>
  <li>Describe the Sarsa control algorithm</li>
  <li>Understand how the Sarsa control algorithm operates in an example MDP</li>
  <li>Analyze the performance of a learning algorithm</li>
</ul>

<p><strong>Lesson 2: Off-policy TD Control: Q-learning</strong></p>

<ul>
  <li>Describe the Q-learning algorithm</li>
  <li>Explain the relationship between Q-learning and the Bellman optimality equations.</li>
  <li>Apply Q-learning to an MDP to find the optimal policy</li>
  <li>Understand how Q-learning performs in an example MDP</li>
  <li>Understand the differences between Q-learning and Sarsa</li>
  <li>Understand how Q-learning can be off-policy without using importance sampling</li>
  <li>Describe how the on-policy nature of Sarsa and the off-policy nature of Q-learning affect their relative performance</li>
</ul>

<p><strong>Lesson 3: Expected Sarsa</strong></p>

<ul>
  <li>Describe the Expected Sarsa algorithm</li>
  <li>Describe Expected Sarsa’s behaviour in an example MDP</li>
  <li>Understand how Expected Sarsa compares to Sarsa control</li>
  <li>Understand how Expected Sarsa can do off-policy learning without using importance sampling</li>
  <li>Explain how Expected Sarsa generalizes Q-learning</li>
</ul>

<h6 id="lesson-1-td-for-control">
<a class="anchor" href="#lesson-1-td-for-control" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Lesson 1: TD for Control</strong>
</h6>

<p><strong>Reading</strong> Chapter 6.4-6.6 <strong>(pp. 129-134)</strong>  in the Reinforcement Learning textbook</p>

<p><img src="../images/C2W3_1_sarsa_algo.png" alt=""></p>

<p><img src="../images/C2W3_2_qlearning_algo.png" alt=""></p>

<p><strong>Video</strong> <strong>Sarsa: GPI with TD</strong> by Martha</p>

<p>By the end of this video, you’ll be able to <em>explain</em> how generalized policy iteration can be used with TD to find <strong>improved policies</strong>, as well as <em>describe</em> the <strong>Sarsa control algorithm</strong></p>

<p><strong>Video Sarsa in the Windy Grid World</strong> by Adam</p>

<p>By the end of this video, you will <em>understand</em> how the <strong>Sarsa</strong> control algorithm operates in an example <strong>MDP</strong>. You will also <em>gain experience</em> analyzing the <strong>performance</strong> of a learning algorithm.</p>

<h6 id="lesson-2-off-policy-td-control-q-learning">
<a class="anchor" href="#lesson-2-off-policy-td-control-q-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Lesson 2: Off-policy TD Control: Q-learning</strong>
</h6>

<p><strong>Video What is Q-learning?</strong> by Martha</p>

<p>By the end of this video, you will be able to <em>describe</em> the <strong>Q-learning</strong> algorithm, and <em>explain</em> the relationship between <strong>Q-learning</strong> and the <strong>Bellman optimality equations</strong>.</p>

<p><strong>Video Q-learning in the Windy Grid World</strong> by Adam</p>

<p>By the end of this video, you will <em>gain insight</em> into how <strong>Q-Learning</strong> performs in an example <strong>MDP</strong>. And <em>gain experience</em> comparing the <strong>performance</strong> of multiple learning algorithms on a single MDP.</p>

<p><strong>Video How is Q-learning off-policy?</strong> by Martha</p>

<p>By the end of this video, you will <em>understand</em> how Q-learning can be <strong>off-policy</strong> without using <strong>important sampling</strong> and be able to <em>describe</em> how learning <strong>on-policy or off-policy</strong> might affect performance in <strong>control</strong>.</p>

<h6 id="lesson-3-expected-sarsa">
<a class="anchor" href="#lesson-3-expected-sarsa" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Lesson 3: Expected Sarsa</strong>
</h6>

<p><strong>Video Expected Sarsa</strong> by Martha</p>

<p>By the end of this video, you will be able to <em>explain</em> the <strong>expected Sarsa</strong> algorithm.</p>

<p><strong>Video Expected Sarsa in the Cliff World</strong> by Adam</p>

<p>By the end of this video, you will be able to <em>describe</em> <strong>expected Sarsas</strong>’s behavior in an example <strong>MDP</strong> and <em>empirically</em> compare <strong>expected Sarsa</strong> and <strong>Sarsa</strong>.</p>

<p><strong>Video Generality of Expected Sarsa</strong> by Martha</p>

<p>By the end of this video, you will <em>understand</em> how Expected Sarsa can do <strong>off-policy</strong> learning without using <strong>importance sampling</strong> and <em>explain</em> how <strong>Expected Sarsa</strong> generalizes <strong>Q-learning</strong>.</p>

<p><strong>Video Week 3 summary</strong> by Adam</p>

<p><img src="../images/C2W3_3_summary.png" alt=""></p>

<p>Sarsa uses a sample based version of the Bellman equation. It learns Q-pi.</p>

<p>Q-learning uses the Bellman optimality equation. It learns Q-star.</p>

<p>Expected sarsa uses the same Bellman equation as Sarsa, but samples it differently. It takes an expectation over the next action values.</p>

<p>What’s the story with on-policy and off-policy learning?</p>

<p>Sarsa is a on-policy algorithm that learns the action values for the policy it’s currently following. Q-learning is an off-policy algorithm that learns the optimal action values. And Expected Sarsa is both an on-policy and an off-policy algorithm that can learn the action values for any policy.</p>

<h6 id="assignment-1">
<a class="anchor" href="#assignment-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Assignment</h6>

<p>Q-Learning and Expected Sarsa</p>

<p>notebooks in <a href="https://github.com/castorfou/Reinforcement-Learning-specialization/tree/main/assignements/course%202%20week%203">github</a></p>

<h2 id="course-2---week-4---planning-learning--acting">
<a class="anchor" href="#course-2---week-4---planning-learning--acting" aria-hidden="true"><span class="octicon octicon-link"></span></a>Course 2 - Week 4 - Planning, Learning &amp; Acting</h2>

<h6 id="module-4-learning-objectives">
<a class="anchor" href="#module-4-learning-objectives" aria-hidden="true"><span class="octicon octicon-link"></span></a>Module 4 Learning Objectives</h6>

<p><strong>Lesson 1: What is a model?</strong></p>

<ul>
  <li>Describe what a model is and how they can be used</li>
  <li>Classify models as distribution models or sample models</li>
  <li>Identify when to use a distribution model or sample model</li>
  <li>Describe the advantages and disadvantages of sample models and distribution models</li>
  <li>Explain why sample models can be represented more compactly than distribution models</li>
</ul>

<p><strong>Lesson 2: Planning</strong></p>

<ul>
  <li>Explain how planning is used to improve policies</li>
  <li>Describe random-sample one-step tabular Q-planning</li>
</ul>

<p><strong>Lesson 3: Dyna as a formalism for planning</strong></p>

<ul>
  <li>Recognize that direct RL updates use experience from the environment to improve a policy or value function</li>
  <li>Recognize that planning updates use experience from a model to improve a policy or value function</li>
  <li>Describe how both direct RL and planning updates can be combined through the Dyna architecture</li>
  <li>Describe the Tabular Dyna-Q algorithm</li>
  <li>Identify the direct-RL and planning updates in Tabular Dyna-Q</li>
  <li>Identify the model learning and search control components of Tabular Dyna-Q</li>
  <li>Describe how learning from both direct and simulated experience impacts performance</li>
  <li>Describe how simulated experience can be useful when the model is accurate</li>
</ul>

<p><strong>Lesson 4: Dealing with inaccurate models</strong></p>

<ul>
  <li>Identify ways in which models can be inaccurate</li>
  <li>Explain the effects of planning with an inaccurate model</li>
  <li>Describe how Dyna can plan successfully with a partially inaccurate model</li>
  <li>Explain how model inaccuracies produce another exploration-exploitation trade-off</li>
  <li>Describe how Dyna-Q+ proposes a way to address this trade-off</li>
</ul>

<p><strong>Lesson 5: Course wrap-up</strong></p>

<h6 id="lesson-1-what-is-a-model">
<a class="anchor" href="#lesson-1-what-is-a-model" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Lesson 1: What is a model?</strong>
</h6>

<p><strong>Reading</strong> Chapter 8.1-8.3 <strong>(pp. 159-166)</strong>  in the Reinforcement Learning textbook</p>

<blockquote>
  <p>Model-based methods rely on planning as their primary component, while model-free methods primarily rely on learning.</p>
</blockquote>

<p><strong>Video What is a Model?</strong> by Martha</p>

<p>By the end of the video, you will be able to <em>describe</em> a <strong>model</strong> and how it can be used, <em>classify</em> models as <strong>distribution models</strong> or <strong>sample models</strong>, and <em>identify</em> when to use a <strong>distribution model</strong> or <strong>sample model</strong>.</p>

<p><strong>Video Comparing Sample and Distribution Models</strong> by Martha</p>

<p>By the end of this video, you will be able to <em>describe</em> the <strong>advantages and disadvantages</strong> of <strong>sample models</strong> and <strong>distribution models</strong>, and you will also be able to <em>explain</em> why <strong>sample models</strong> can be represented <strong>more compactly</strong> than <strong>distribution models</strong>.</p>

<h6 id="lesson-2-planning">
<a class="anchor" href="#lesson-2-planning" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Lesson 2: Planning</strong>
</h6>

<p><strong>Video Random Tabular Q-planning</strong> by Martha</p>

<p>By the end of this video, you’ll be able to <em>explain</em> how <strong>planning</strong> is used to <strong>improve policies</strong> and <em>describe</em> <strong>random-sample one-step tabular Q-planning</strong>.</p>

<h6 id="lesson-3-dyna-as-a-formalism-for-planning">
<a class="anchor" href="#lesson-3-dyna-as-a-formalism-for-planning" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Lesson 3: Dyna as a formalism for planning</strong>
</h6>

<p><strong>Video The Dyna Architecture</strong> by Adam</p>

<p>By the end of this video, you will be able to <em>understand</em> how <strong>simulate experience</strong> from the model differs from <strong>interacting with the environment</strong>. You will also <em>understand</em> how the <strong>Dyna architecture</strong> mixes <strong>direct RL</strong> updates and <strong>planning</strong> updates.</p>

<p><img src="../images/C2W4_1_dyna_arch.png" alt=""></p>

<p><strong>Video The Dyna Algorithm</strong> by Adam</p>

<p>By the end of this video, you should be able to <em>describe</em> how <strong>Tabular Dyna-Q</strong> works. You will also be able to <em>identify</em> the <strong>direct-RL</strong>, <strong>planning</strong> updates in <strong>Tabular Dyna-Q</strong>, and identify the <strong>model learning</strong> and <strong>search control</strong> components of <strong>Tabular Dyna-Q</strong>.</p>

<p><img src="../images/C2W4_2_dyna_algo.png" alt=""></p>

<p><strong>Video Dyna &amp; Q-learning in a Simple Maze</strong> by Adam</p>

<p>By the end of this video you will be able to <em>describe</em> how learning from both <strong>environment-real</strong> and <strong>model</strong> experience impacts performance. You will also be able to <em>explain</em> how an <strong>accurate model</strong> allows the agent to learn from <strong>fewer environment interactions</strong>.</p>

<h6 id="lesson-4-dealing-with-inaccurate-models">
<a class="anchor" href="#lesson-4-dealing-with-inaccurate-models" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>Lesson 4: Dealing with inaccurate models</strong>
</h6>

<p><strong>Video What if the model is inaccurate?</strong> by Martha</p>

<p>By the end of this video you will be able to <em>identify</em> ways in which <strong>models</strong> can be <strong>inaccurate</strong>, <em>explain</em> the effects of <strong>planning</strong> with an <strong>inaccurate model</strong>, and <em>describe</em> how <strong>Dyna</strong> can plan  successfully with an <strong>incomplete model</strong>.</p>

<p><strong>Video In-depth with changing environments</strong> by Adam</p>

<p>By the end of this video, you’ll be able to <em>explain</em> how model inaccuracies produce another <strong>exploration-exploitation trade-off</strong>, and <em>describe</em> how <strong>Dyna-Q+</strong> addresses this trade-off.</p>

<p><img src="../images/C2W4_3_dyna_q_plus_algo.png" alt=""></p>

<p>Video Drew Bagnell: <strong>self-driving, robotics, and Model Based RL</strong></p>

<p><strong>Video week 4 summary</strong> by Martha</p>

<h6 id="assignment-2">
<a class="anchor" href="#assignment-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Assignment</h6>

<p>Dyna-Q and Dyna-Q+</p>

<p>notebooks in  <a href="https://github.com/castorfou/Reinforcement-Learning-specialization/tree/main/assignements/course%202%20week%204">github</a></p>

<p>Chapter summary <strong>Chapter 8.12 (pp. 188)</strong></p>

<p><img src="../images/C2W4_4_planning_learning.png" alt=""></p>

<blockquote>
  <p>Planning, acting, and model-learning interact in a circular fashion (as in
the figure above), each producing what the other needs to improve; no other
interaction among them is either required or prohibited.</p>
</blockquote>

<blockquote>
  <h1 id="text-book-part-1-summary">
<a class="anchor" href="#text-book-part-1-summary" aria-hidden="true"><span class="octicon octicon-link"></span></a>Text Book Part 1 Summary</h1>

  <hr>

  <p>For a summary of what we’ve covered in the specialization so far, read: <strong>pp. 189-191</strong> in Reinforcement Learning: an introduction .</p>
</blockquote>

<p>All of the methods we have explored so far in this book have three key ideas in common:
first, they all seek to estimate value functions; second, they all operate by backing up
values along actual or possible state trajectories; and third, they all follow the general
strategy of generalized policy iteration (GPI), meaning that they maintain an approximate
value function and an approximate policy, and they continually try to improve each on the
basis of the other. These three ideas are central to the subjects covered in this book. We
suggest that value functions, backing up value updates, and GPI are powerful organizing
principles potentially relevant to any model of intelligence, whether artificial or natural.</p>

<h6 id="course-wrap-up">
<a class="anchor" href="#course-wrap-up" aria-hidden="true"><span class="octicon octicon-link"></span></a>Course wrap-up</h6>


  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="castorfou/guillaume_blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/guillaume_blog/blog/reinforcement-learning-specialization-coursera-course2.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/guillaume_blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/guillaume_blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/guillaume_blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Journey for a datascientist</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/castorfou" target="_blank" title="castorfou"><svg class="svg-icon grey"><use xlink:href="/guillaume_blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/GuillaumeRamel1" target="_blank" title="GuillaumeRamel1"><svg class="svg-icon grey"><use xlink:href="/guillaume_blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
