{
  
    
        "post0": {
            "title": "save git https credentials under wsl",
            "content": "source of inspiration . Microsoft has released a tool to securely keep https credentials: . git-credential-manager . Usefull when one has to use https instead of git(ssl) to connect to git repos. My case when I am behing my corporate firewall and has to link to github repos (such as this blog) . How to setup it . create token in github . I have to create a token at Settings &gt; Developer Settings &gt; Personal Access Tokens . installation of git-credential-manager inside WSL . Download the latest (v2.0.696 at May/3rd 2022) .deb package, and run the following: . sudo dpkg -i &lt;path-to-package&gt; git-credential-manager-core configure git config --global credential.credentialStore gpg export GPG_TTY=$(tty) gpg --full-generate-key sudo apt install -y pass key_id=`gpg --list-keys | awk -F: &#39;/^ / { print $0 }&#39; | cut -d&quot; &quot; -f7` pass init $key_id . or see the step 06 in install ubuntu 22.04 on WSL .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/save-git-https-credentials-under-wsl.html",
            "relUrl": "/blog/save-git-https-credentials-under-wsl.html",
            "date": " • May 3, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "upgrade to last version of fastpages",
            "content": "source of inspiration . as detailed in https://github.com/fastai/fastpages/issues/634 . Hamel asks to restart from a new repo. But how to keep the same blog url? . Easy way is to rename former repo (from guillaume_blog to guillaume_blog_old) and initiate new repo as former one (guillaume_blog). . Here are the steps. . Installation and setup . Installation . Generate a copy of fastpages repo. Just have to follow instructions by clicking at https://github.com/fastai/fastpages/generate. Name repo as guillaume_blog | Click on the PR Initial Setup in your new repo. There are instructions to create a SSH_DEPLOY_KEY. | Merge this PR | Clone this repo locally | Because I use https, I have to create a token at Settings &gt; Developer Settings &gt; Personal Access Tokens | and to keep this token locally, I enter git config --global credential.helper manager before pushing | Copy content . Directories: _notebooks, _posts, _files, _images | Clean content from directories (examples) in _notebooks, _posts, _words | Pages: _pages/about.md, index.html, README.md | and utils: refresh_blog_content.sh, publish.sh |",
            "url": "https://castorfou.github.io/guillaume_blog/blog/upgrade-fastpages.html",
            "relUrl": "/blog/upgrade-fastpages.html",
            "date": " • May 2, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "install ubuntu 22.04 on WSL",
            "content": "source of inspiration . How to install Ubuntu 21.10 on WSL for Windows 10 and 11 . Installation . uninstall image (if needed) . # wsl --unregister &lt;distroName&gt; wsl --unregister ubuntu-22.04 . download images . From cloud images ubuntu (cloud-images &gt; jammy &gt; current), now there are wsl images: . . I just have to download the last jammy (22.04) image jammy-server-cloudimg-amd64-wsl.rootfs.tar.gz . install and setup from powershell . I have downloaded this ubuntu image to D: wsl ubuntu-22.04 download . (base) guillaume@LL11LPC0PQARQ:/mnt/d/wsl$ tree . ├── Ubuntu-20.04 │   └── ext4.vhdx ├── Ubuntu-22.04 │   ├── download │   │   └── jammy-server-cloudimg-amd64-wsl.rootfs.tar.gz │   └── instance . and my ubuntu-22.04 instance will stand in D: wsl ubuntu-22.04 instance . Install with this command from powershell . # wsl --import &lt;distroname&gt; &lt;location of instance&gt; &lt;location of download&gt; wsl --import ubuntu-22.04 D: wsl ubuntu-22.04 instance D: wsl ubuntu-22.04 download jammy-server-cloudimg-amd64-wsl.rootfs.tar.gz . It takes 3-4 minutes to install. and should be visible in your wsl instances. . wsl --list --all -v NAME STATE VERSION ubuntu-22.04 Stopped 2 . then to run it . # wsl -d &lt;distroname&gt; wsl -d ubuntu-22.04 . or . use Windows Terminal as a launcher . Windows Terminal is a smart way to group all terminals (powershell, and all your wsl instances) . . It can be installed even with limited windows store access by clicking install in Installer le Terminal Windows et commencer à le configurer . Automatically all wsl instances appear in Settings. . Automatic setup . copy these 2 scripts in /root/ (given they are in D: wsl ubuntu-22.04 download) . cp /mnt/d/wsl/Ubuntu-22.04/download/setup_wsl_* . . setup_wsl_root.sh download . #!/bin/bash echo &quot;0. get username: &quot; read user_name . /etc/lsb-release echo Configuration for user [$user_name] echo of distribution $DISTRIB_CODENAME echo echo &quot;1. create user and add in sudo&quot; #adduser --disabled-password --gecos &quot;&quot; $user_name adduser --gecos &quot;&quot; $user_name usermod -aG sudo $user_name echo echo &quot;2. create wsl.conf file&quot; rm -rf /etc/wsl.conf tee /etc/wsl.conf &lt;&lt; EOF # Set the user when launching a distribution with WSL. [user] default=$user_name EOF echo echo &quot;3. prepare setup by user&quot; cp setup_wsl_user.sh /home/$user_name chown $user_name:users /home/$user_name/setup_wsl_user.sh chmod 750 /home/$user_name/setup_wsl_user.sh tee -a /home/$user_name/.bashrc &lt;&lt; EOF if [ ! -e &quot;.wsl_configured&quot; ]; then ./setup_wsl_user.sh touch .wsl_configured fi EOF echo echo &quot;end of configuration for root&quot; echo &quot;stop wsl instance by running &#39;wsl --shutdown &lt;distroname&gt;&#39; from powershell&quot; echo &quot;and start from Windows Terminal&quot; . setup_wsl_user.sh download . #!/bin/bash echo &quot;1. setup wsl-vpnkit&quot; if grep -Fxq &quot;wsl-vpnkit&quot; ~/.profile then # code if found echo &quot; wsl-vpnkit already setup&quot; else # code if not found echo &#39;wsl.exe -d wsl-vpnkit service wsl-vpnkit start&#39; &gt;&gt; ~/.profile fi wsl.exe -d wsl-vpnkit service wsl-vpnkit start source ./.bashrc echo echo &quot;2. create ssh key to copy to gitlab&quot; . /etc/lsb-release if [ ! -e &quot;.ssh/id_rsa.pub&quot; ]; then ssh-keygen -t rsa -b 4096 -C &quot;WSL2 ubuntu $DISTRIB_RELEASE&quot; cat .ssh/id_rsa.pub echo &quot;copy this content to gitlab &gt; preferences &gt; SSH Keys&quot; read -p &quot;Press any key to resume ...&quot; fi echo echo &quot;3. update certificates&quot; git clone git@gitlab.michelin.com:devops-foundation/devops_environment.git /tmp/devops_environment sudo cp /tmp/devops_environment/certs/* /usr/local/share/ca-certificates/ sudo update-ca-certificates rm -rf /tmp/devops_environment if [ $DISTRIB_RELEASE == &quot;22.04&quot; ] then echo &#39;bug SSL with ubuntu 22.04 - https://bugs.launchpad.net/ubuntu/+source/openssl/+bug/1963834/comments/7&#39; sudo tee -a /etc/ssl/openssl.cnf &lt;&lt; EOF [openssl_init] ssl_conf = ssl_sect [ssl_sect] system_default = system_default_sect [system_default_sect] Options = UnsafeLegacyRenegotiation EOF fi echo echo &quot;4. update apt sources with artifactory&quot; echo &#39;Acquire { http::User-Agent &quot;Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:13.37) Gecko/20100101 Firefox/31.33.7&quot;; };&#39; | sudo tee /etc/apt/apt.conf.d/90globalprotectconf sudo sed -i &#39;s,http://archive.ubuntu.com/ubuntu,https://artifactory.michelin.com/artifactory/ubuntu-archive-remote,g&#39; /etc/apt/sources.list sudo sed -i &#39;s,http://security.ubuntu.com/ubuntu,https://artifactory.michelin.com/artifactory/ubuntu-archive-remote,g&#39; /etc/apt/sources.list sudo apt update sudo apt upgrade -y echo . Then . chmod +x setup_wsl_root.sh ./setup_wsl_root.sh . As explained stop wsl instance by running wsl --shutdown ubuntu-22.04 from powershell and start from Windows Terminal . It restarts from your user and it will install: . setup wsl-vpnkit | create ssh key to copy to gitlab | update certificates | update apt sources with artifactory | . And now we can install other parts . all the scripts are in https://github.com/castorfou/guillaume_blog/tree/master/files . 00 - keep config files in git . wget -O - https://raw.githubusercontent.com/castorfou/guillaume_blog/master/files/setup_wsl_00_config_files_in_git.sh | bash source .bashrc . 01 - automount secured vbox . wget -O - https://raw.githubusercontent.com/castorfou/guillaume_blog/master/files/setup_wsl_01_automount_secured_vbox.sh | bash . 02 - python with conda and configure base environment (jupyterlab, mamba) . wget -O - https://raw.githubusercontent.com/castorfou/guillaume_blog/master/files/setup_wsl_02_install_python_conda_part1.sh | bash cd source .bashrc wget -O - https://raw.githubusercontent.com/castorfou/guillaume_blog/master/files/setup_wsl_02_install_python_conda_part2.sh | bash . 03 - bat cat . wget -O - https://raw.githubusercontent.com/castorfou/guillaume_blog/master/files/setup_wsl_03_install_batcat.sh | bash source .bashrc . 04 - git access . ln -s /mnt/d/git/ ~/ . 05 - X access with GWSL . GWSL Homepage . if you have access to Windows Store, it is available. . Or alternate download are possible. . . 06 - git credential manager . wget -O - https://raw.githubusercontent.com/castorfou/guillaume_blog/master/files/setup_wsl_06_git_credential_manager.sh | bash . 07 - install wslu . wslu . wget -O - https://raw.githubusercontent.com/castorfou/guillaume_blog/master/files/setup_wsl_07_wslu.sh | bash . Manual setup (skip if to follow automatic setup) . basic setup . With this way to install, you don’t have any user, you don’t have any launcher within Windows. . Create a user and add it to sudo: . # adduser &lt;yourusername&gt; # usermod -aG sudo &lt;yourusername&gt; adduser guillaume usermod -aG sudo guillaume . and I can switch to this user simply with . # su &lt;yourusername&gt; su guillaume . launch distro with yourusername - update wsl.conf . Manually you can now start your distro with your username from powershell . # wsl -d &lt;distroname&gt; -u &lt;yourusername&gt; wsl -d ubuntu-22.04 -u guillaume . Or from another wsl (huge avantage to run in linux terminal instead of powershell) . wsl.exe -d ubuntu-22.04 -u guillaume . but you can better keep this username setting by updating wsl.conf . # /etc/wsl.conf # Set the user when launching a distribution with WSL. [user] default=YourUserName . It is now setup. You can now shutdown this instance from powershell. . # wsl --shutdown &lt;distroname&gt; wsl --shutdown ubuntu-22.04 . and when starting wsl -d ubuntu-22.04, you reach your username. . wsl-vpnkit . As wsl-vpnkit is already installed, I just have to . echo &#39;wsl.exe -d wsl-vpnkit service wsl-vpnkit start&#39; &gt;&gt; ~/.profile source .bashrc . gitlab . ssh-keygen -t rsa -b 4096 -C &quot;WSL2 ubuntu 22.04&quot; . and copy id_rsa.pub into gitlab &gt; preferences &gt; SSH Keys . corporate CA certificates . git clone git@gitlab.michelin.com:devops-foundation/devops_environment.git /tmp/devops_environment sudo cp /tmp/devops_environment/certs/* /usr/local/share/ca-certificates/ sudo update-ca-certificates rm -rf /tmp/devops_environment . apt sources . had to replace focal (20.04) to jammy (22.04) . echo &#39;Acquire { http::User-Agent &quot;Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:13.37) Gecko/20100101 Firefox/31.33.7&quot;; };&#39; | sudo tee /etc/apt/apt.conf.d/90globalprotectconf sudo sed -i &#39;s@^ (deb )http://archive.ubuntu.com/ubuntu/ ( jammy (-updates ) ?.* )$@ 1https://artifactory.michelin.com/artifactory/ubuntu-archive-remote 2 n# &amp;@&#39; /etc/apt/sources.list sudo sed -i &#39;s@^ (deb )http://security.ubuntu.com/ubuntu/ ( jammy (-updates ) ?.* )$@ 1https://artifactory.michelin.com/artifactory/ubuntu-security-remote 2 n# &amp;@&#39; /etc/apt/sources.list . check everything is ok . This command must return google ip: | . host google.fr . This command must return artifactory ip: | . host artifactory.michelin.com . You are able to update your distribution without error: | . sudo apt update sudo apt upgrade -y . Setup config dotfiles with whole filesystem (/) . as detailed in keep dotfiles in git . but to manage the whole filesystem. . init local repo . sudo mkdir -p /.cfg sudo chown $USER:users /.cfg git init --bare /.cfg alias config=&#39;/usr/bin/git --git-dir=/.cfg/ --work-tree=/&#39; config config --local status.showUntrackedFiles no echo &quot;alias config=&#39;/usr/bin/git --git-dir=/.cfg/ --work-tree=/&#39;&quot; &gt;&gt; $HOME/.bash_aliases cd source .bashrc . git default identity (if needed) . config config --global user.email &quot;guillaume.ramelet@michelin.com&quot; config config --global user.name &quot;guillaume&quot; . setup branch and push to central repo . config remote add origin git@gitlab.michelin.com:janus/dotfiles.git config fetch cd config add .bashrc config commit -m &#39;init with .bashrc&#39; config branch GR_WSL2_ubuntu22.04 config checkout GR_WSL2_ubuntu22.04 config push --set-upstream origin GR_WSL2_ubuntu22.04 .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/install-ubuntu-22.04-on-WSL.html",
            "relUrl": "/blog/install-ubuntu-22.04-on-WSL.html",
            "date": " • Apr 25, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "keep dotfiles in git",
            "content": "source of inspiration . as pointed by Jeremy Howard. . How to setup it . prerequisites . I consider I already have a git repo with my dotfiles from other machines. . Repo: git@&lt;your_gitlab_address&gt;:&lt;your_id&gt;/dotfiles.git . I keep one separate branch per machine. Current branches: master (empty), and WSL2. . I am going to add a machine called iolab. . from iolab . git init --bare $HOME/.cfg alias config=&#39;/usr/bin/git --git-dir=$HOME/.cfg/ --work-tree=$HOME&#39; config config --local status.showUntrackedFiles no echo &quot;alias config=&#39;/usr/bin/git --git-dir=$HOME/.cfg/ --work-tree=$HOME&#39;&quot; &gt;&gt; $HOME/.bash_aliases . And we can now run config status . (base) [ 09:53:56 ][ id: ~ ]$ config status # On branch master # # Initial commit # nothing to commit (create/copy files and use &quot;git add&quot; to track) . but now we would like to create a new branch, and push all this to our central repo. . First we have to set this central repo. . config remote add origin git@&lt;your_gitlab_address&gt;:&lt;your_id&gt;/dotfiles.git config fetch . Before creating our branch, we have to commit something (to really create our local branch master) . config add .bashrc config commit -m &#39;init with .bashrc&#39; . And then only we can create our branch iolab . config branch iolab config checkout iolab config push --set-upstream origin iolab . we are now ready to use it . How to use it . config add .bash_aliases config commit -m&#39;bash aliases&#39; config push . How to setup 2 remote repo . There is a nice explanation abut how to work with multiple repos in https://jigarius.com/blog/multiple-git-remote-repositories. . To follow that, I will configure my dotfile repo from WSL2 to push to 2 remotes, one on gitlab (internal) and one on github. . For the moment it is only connected to gitlab. . $ config remote -v origin git@gitlab.michelin.com:janus/dotfiles.git (fetch) origin git@gitlab.michelin.com:janus/dotfiles.git (push) . My github repo is at: https://github.com/castorfou/dotfiles.git (I use https, because of my local firewall) . $ config remote set-url --add --push origin git@gitlab.michelin.com:janus/dotfiles.git $ config remote set-url --add --push origin https://github.com/castorfou/dotfiles.git $ config push origin GR_WSL2 . I have to get a token from github to access in https . To generate a token: . Log into GitHub | Click on your name / Avatar in the upper right corner and select Settings | On the left, click Developer settings | Select Personal access tokens and click Generate new token | Give the token a description/name and select the scope of the token I selected repo only to facilitate pull, push, clone, and commit actions | Click the link Read more about OAuth scopes for details about the permission sets | . | Click Generate token | Copy the token – this is your new password! | Lastly, to ensure the local computer remembers the token, we can enable caching of the credentials. This configures the computer to remember the complex token so that we dont have too. . git config --global credential.helper cache .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/keep-dotfiles-in-git.html",
            "relUrl": "/blog/keep-dotfiles-in-git.html",
            "date": " • Apr 7, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Logbook for February 22",
            "content": "Week 5 - February 22 . Thursday 2/3 . Stephane Mallat - collège de France - Information et complexité video n°2: Estimation par maximum de vraisemblance . Week 8 - February 22 . Monday 2/21 . Stephane Mallat - collège de France - Information et complexité video n°3: Optimisation et modèles exponentiels . Tuesday 2/22 . Antonin Raffin (Stable Baselines 3 author) explains how to better evaluate RL agents using Rliable. Would like to test that. .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/logbook-February.html",
            "relUrl": "/blog/logbook-February.html",
            "date": " • Feb 1, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "setup wsl2 conda mamba and cuda",
            "content": "wsl2 - installation and configuration . most of it is explained in an internal blog entry. . To display windows version: winver.exe . I use version 20H2 build 19042.1415 . installation . It is now as easy as to run wsl --install in powershell as admin. . Full detail at MS WSL doc . Other commands: . # list all wsl distributions installed and their WSL version wsl --list --verbose # list all distributions available wsl --list --online Voici la liste des distributions valides qui peuvent être installées. Installer à l’aide de « wsl --install -d &lt;Distribution&gt; ». NAME FRIENDLY NAME Ubuntu Ubuntu Debian Debian GNU/Linux kali-linux Kali Linux Rolling openSUSE-42 openSUSE Leap 42 SLES-12 SUSE Linux Enterprise Server v12 Ubuntu-16.04 Ubuntu 16.04 LTS Ubuntu-18.04 Ubuntu 18.04 LTS Ubuntu-20.04 Ubuntu 20.04 LTS # install Linux distributions wsl --install -d &lt;Distribution Name&gt; # shutdown wsl: shutdown all wsl --shutdown # define default wsl distribution to use with wsl wsl -s &lt;DistributionName&gt; . installation in a non-system drive . and here is a more advanced config to install in a non system drive (D: instead of C:) . #from powershell New-Item D: WSL Ubuntu-20.04 -ItemType Directory Set-Location D: WSL Ubuntu-20.04 #list+link of distributions in https://docs.microsoft.com/en-us/windows/wsl/install-manual#downloading-distributions Invoke-WebRequest -Uri https://aka.ms/wslubuntu2004 -OutFile Ubuntu-20.04.appx -UseBasicParsing Rename-Item . Ubuntu-20.04.appx Ubuntu-20.04.zip Expand-Archive . Ubuntu-20.04.zip -Verbose # and then run Ubuntu_2004.2021.825.0_x64.appx . I don’t know yet where the WSL disk is located (is it a .vhdx file?) . Disks are located at %USERPROFILE% AppData Local Packages [distro name] . 2 ditros used: . wsl1 ubuntu 18.04: CanonicalGroupLimited.Ubuntu18.04onWindows_79rhkp1fndgsc . | wsl2 ubuntu 20.04: CanonicalGroupLimited.UbuntuonWindows_79rhkp1fndgsc . | . So disks are still in C: . Guess I have to use move-wsl. . Set-Location &#39;D: Program Files (x86) move-wsl &#39; . move-wsl.ps1 PS D: Program Files (x86) move-wsl&gt; . move-wsl.ps1 Getting distros... Select distro to move: 1: Ubuntu-18.04 2: Ubuntu 2 Enter WSL target directory: D: wsl Ubuntu-20.04 Move Ubuntu to &quot;D: wsl Ubuntu-20.04&quot;? (Y|n): Y Exporting VHDX to &quot;D: wsl Ubuntu-20.04 Ubuntu.tar&quot; ... . And after that, have to create file/etc/wsl.conf . guillaume@LL11LPC0PQARQ:~$ cat /etc/wsl.conf [user] default=guillaume . configuration . wsl-vpnkit . It is nicely explained in the Michelin blog entry. DNS resolution is kind of broken (I think due to internal protections we use on our corporate PC) . vpnkit provides a secured solution to make it work. And sakai135 has packaged it for wsl: wsl-vpnkit . The steps to install wsl-vpn kit are: . Create a working directory on your windows workspace and download this packaging of wsl-vpnkit inside. | Now, open a powershell and go to the location of wsl-vpnkit.tar.gz, downloaded during the previous step | On your powershell terminal, launch: | . #/! in powserhsell wsl --import wsl-vpnkit $env:USERPROFILE wsl-vpnkit wsl-vpnkit.tar.gz wsl -d wsl-vpnkit . You can now exit your powershell | For the last step, to ensure all wsl reboot good communication, we will write in .profile file of your ubuntu user wsl-vpnkit initialization command: | . echo &#39;wsl.exe -d wsl-vpnkit service wsl-vpnkit start&#39; &gt;&gt; ~/.profile . Relaunch your WSL terminal | . git configuration . create a SSH key pair under your distribution . ssh-keygen -t rsa -b 4096 -C &quot;WSL2&quot; . Integrate into gitlab using gitlab doc. (copy id_rsa.pub into gitlab &gt; preferences &gt; SSH Keys)cat .s . Add corporate CA certificates . Michelin SI is behind an ssl proxy with his proper PKI for certificates delivering. That is why, your subsystem must add this pki in her recognized authorities. . To do this, we will clone a repository with the certificates in the subsystem and copy them to ca-certificates: . git clone git@gitlab.michelin.com:devops-foundation/devops_environment.git /tmp/devops_environment sudo cp /tmp/devops_environment/certs/* /usr/local/share/ca-certificates/ sudo update-ca-certificates . If everything is ok, terminal notify you that certificates has been added. . You can now clean the temp working folder: . rm -rf /tmp/devops_environment . we can have similar approach to update CA certifcates for Python. 1st step is to locate cacert.pem of your active python environment. . import certifi certifi.where() &gt;&gt; &#39;/home/guillaume/miniconda3/envs/fastai/lib/python3.9/site-packages/certifi/cacert.pem&#39; . TO BE FIXED . After having run update-ca-certifcates, there is an updated ca file at /etc/ssl/certs/ca-certificates.crt. Let’s concatenate it to our cacert.pem. . cp /etc/ssl/certs/ca-certificates.crt /tmp/ca-certificates.crt openssl x509 -in /tmp/ca-certificates.crt -out /tmp/ca-certificates.pem -outform PEM cat /tmp/ca-certificates.pem | tee -a /home/guillaume/miniconda3/envs/fastai/lib/python3.9/site-packages/certifi/cacert.pem . Configure APT . The last step, to have a subsystem ready to use, is to have an apt with Michelin trusted sources configured. Ubuntu based package repositories can’t be used behind Michelin proxy. . Michelin offers its own apt server with artifactory. To configure apt to use artifactory, launch these commands: . echo &#39;Acquire { http::User-Agent &quot;Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:13.37) Gecko/20100101 Firefox/31.33.7&quot;; };&#39; | sudo tee /etc/apt/apt.conf.d/90globalprotectconf sudo sed -i &#39;s@^ (deb )http://archive.ubuntu.com/ubuntu/ ( focal (-updates ) ?.* )$@ 1https://artifactory.michelin.com/artifactory/ubuntu-archive-remote 2 n# &amp;@&#39; /etc/apt/sources.list sudo sed -i &#39;s@^ (deb )http://security.ubuntu.com/ubuntu/ ( focal (-updates ) ?.* )$@ 1https://artifactory.michelin.com/artifactory/ubuntu-security-remote 2 n# &amp;@&#39; /etc/apt/sources.list . Check if your WSL distribution is working correctly . To verify if everything is OK on your distribution: . This command must return google ip: | . host google.fr . This command must return artifactory ip: | . host artifactory.michelin.com . You are able to update your distribution without error: | . sudo apt update sudo apt upgrade -y . Conda Mamba - installation and configuration . conda installation . tmpdir=$(mktemp -d) cd $tmpdir wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh chmod +x Miniconda3-latest-Linux-x86_64.sh # answer yes to question Do you wish the installer to initialize Miniconda3 by running conda init? bash Miniconda3-latest-Linux-x86_64.sh -p $HOME/miniconda3 &gt;&gt; ==&gt; For changes to take effect, close and re-open your current shell. &lt;== . With this configuration, conda will be activate at startup. If you’d prefer that conda’s base environment not be activated on startup, set the auto_activate_base parameter to false: conda config --set auto_activate_base false . conda configuration . As we have in-house CA certificates, and conda uses its own CA certificates (in ~/miniconda3/ssl) . We have to change this behaviour and ask conda to use system CA certifcates. . At the end of .bash_rc, add export REQUESTS_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt . mamba installation . conda install mamba -n base -c conda-forge mamba init . installation jupyter notebook, nb_conda_kernels, jupyter lab . mamba install nb_conda_kernels mamba install -c conda-forge jupyterlab jupyterlab-git . create conda envt - fastai (v2.5.3) (optional) . mamba create --name fastai mamba activate fastai mamba install -c fastai -c pytorch fastai mamba install ipykernel . Jupyter . Start jupyter (lab or notebook) from base environment, and switch to desired python environment. . modify jupyter config . #create jupyter config file in ~.jupyter jupyter notebook --generate-config . And activate jupyter config file to change #c.NotebookApp.use_redirect_file = True to c.NotebookApp.use_redirect_file = False .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/wsl2-conda-mamba-cuda.html",
            "relUrl": "/blog/wsl2-conda-mamba-cuda.html",
            "date": " • Jan 18, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Logbook for January 22",
            "content": "Week 1 - January 22 . Monday 1/3 . Will try to use Zotero for managing research papers. Can sync between PC. Seems helpful. My lib . Tuesday 1/4 . Git revert a file to a previous commit . git log 00 - my_lib.ipynb git checkout f97406b026bfdf529d2dc4de96224bdfbaa576a8 00 - my_lib.ipynb . Week 2 - January 22 . Monday 1/17 . To update fastai from an existing envt under windows . conda update -n base -c defaults conda (from base) conda update fastai -c fastai -c pytorch -c conda-forge -c nvidia (from fastai) . To install mamba under WSL2 . conda install mamba -n base -c conda-forge (from base) then mamba init . To use system CA certificate in WSL2 . export REQUESTS_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt . To install fastai in WSL2 using mamba . mamba install -c fastchan fastai . Thursday 1/20 . Stephane Mallat - collège de France - Information et complexité but unfortunately video is not yet available. . Re-read of arXiv:2110.01889 Deep Neural Networks and Tabular Data: A Survey (here on zotero) . Week 3 - January 22 . Tuesday 1/26 . Video of 1st lecture of Stephane Mallat 2022 is now available. .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/logbook-January.html",
            "relUrl": "/blog/logbook-January.html",
            "date": " • Jan 1, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Logbook for December 21",
            "content": "Week 49 - December 21 . Thursday 12/9 . A deep learning course using PyTorch including Transformers, Generative models and self-supervised learning: https://deeplearning.neuromatch.io/tutorials/intro.html - W1D1 - Gradient Descent and AutoGrad done . NeurIPS workshop on RL next Monday. https://sites.google.com/view/deep-rl-workshop-neurips2021. List of posters is just impressive. . And best fastai sources compiled by Tanishq Mathew Abraham. . From @paperswithcode, Deep learning models for tabular data continue to improve. What are the latest methods and recent progress? https://twitter.com/paperswithcode/status/1458433653269205002 . Friday 12/10 . Registration for neurips 2021 taken. Specially interested by RL workshop on Monday. . Week 50 - December 21 . Monday 12/13 . NeurIPS tutorial: Real-Time Optimization for Fast and Complex Control Systems .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/logbook-December.html",
            "relUrl": "/blog/logbook-December.html",
            "date": " • Dec 1, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Logbook for November 21",
            "content": "Week 47 - November 21 . Monday 11/22 . back after a nice vacation break + some catchup to do for work. . to use iphone as a webcam on linux or windows: https://www.iriun.com/ (but not detected as a webcam in linux) . Wednesday 11/24 . getting this message when pushing to gitlab: remote: GitLab: File is larger than the allowed size of 100 MB . if from the most recent commit: . git rm --cached &lt;my large file&gt; git commit --amend -C HEAD . otherwise follow Tutorial: Removing Large Files from Git on medium (or git clean repo with BFG on this blog) . Thursday 11/25 . when exporting notebooks with plotly graphs, it can help to use . import plotly plotly.offline.init_notebook_mode() . exporting to html will integrate these plotly graphs (but not exporting to pdf) .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/logbook-November.html",
            "relUrl": "/blog/logbook-November.html",
            "date": " • Nov 1, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Just some usefull keyboard shortcuts",
            "content": "I cannot believe I have been using linux for more than 20 years as my main system and have never configured keyboard shortcuts to launch explorer files. . .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/Just-some-usefull-keyboard-shortcuts.html",
            "relUrl": "/blog/Just-some-usefull-keyboard-shortcuts.html",
            "date": " • Oct 21, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Logbook for October 21",
            "content": "Week 40 - October 21 . Monday 10/4 . Jupyter Lab is now packaged as a desktop app. Gave a try 2 minutes but issue with my running environment . To do: read and handson Understanding Variational Autoencoders (VAEs) . To do: reand and handson Patsy: Build Powerful Features with Arbitrary Python Code . Thursday 10/7 . Deep learning seems unstoppable! I’m particularly impressed by the recent progress of deep learning on tabular data. This new survey paper provides an overview of the SOTA deep learning methods on tabular data. A great read for students and practitioners. . Deep Neural Networks and Tabular Data: A Survey - arxiv 2110.01889 . Week 42 - October 21 . Wednesday 10/20 . Using fastai forums to get inspirational content for VAE with tabular data. This one sounds good: Adversarial Autoencoders (with Pytorch). And this talk demystifying bayesian stuff. . “Most of human and animal learning is unsupervised learning. If intelligence was a cake, unsupervised learning would be the cake [base], supervised learning would be the icing on the cake, and reinforcement learning would be the cherry on the cake. We know how to make the icing and the cherry, but we don’t know how to make the cake.” . Yann LeCunn . And Intuitively Understanding Variational Autoencoders mentioned by Jeremy Howard .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/logbook-October.html",
            "relUrl": "/blog/logbook-October.html",
            "date": " • Oct 1, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Generate python modules from jupyter notebooks",
            "content": "I have been using this for more than a year and I have just realized I don&#39;t have any blog entry about it? . Notebook2script . As you may know, the full fastai v2 has been written in notebooks. . This is quite impressive these notebooks are documentation + code, and fastai libraries are built from these notebooks. . I have been using one of the first version of Jeremy Howard&#39;s process, I am quite sure there have been many improvment since then. . Here is the source code for notebook2script.py download . import json,fire,re from pathlib import Path import io def is_export(cell): if cell[&#39;cell_type&#39;] != &#39;code&#39;: return False src = cell[&#39;source&#39;] if len(src) == 0 or len(src[0]) &lt; 7: return False #import pdb; pdb.set_trace() return re.match(r&#39;^ s*# s*export s*$&#39;, src[0], re.IGNORECASE) is not None def getSortedFiles(allFiles, upTo=None): &#39;&#39;&#39;Returns all the notebok files sorted by name. allFiles = True : returns all files = &#39;*_*.ipynb&#39; : returns this pattern upTo = None : no upper limit = filter : returns all files up to &#39;filter&#39; included The sorting optioj is important to ensure that the notebok are executed in correct order. &#39;&#39;&#39; import glob ret = [] if (allFiles==True): ret = glob.glob(&#39;*.ipynb&#39;) # Checks both that is bool type and that is True if (isinstance(allFiles,str)): ret = glob.glob(allFiles) if 0==len(ret): print(&#39;WARNING: No files found&#39;) return ret if upTo is not None: ret = [f for f in ret if str(f)&lt;=str(upTo)] return sorted(ret) def notebook2script(fname=None, allFiles=None, upTo=None, fnameout=None): &#39;&#39;&#39;Finds cells starting with `#export` and puts them into a new module + allFiles: convert all files in the folder + upTo: convert files up to specified one included ES: notebook2script --allFiles=True # Parse all files notebook2script --allFiles=nb* # Parse all files starting with nb* notebook2script --upTo=10 # Parse all files with (name&lt;=&#39;10&#39;) notebook2script --allFiles=*_*.ipynb --upTo=10 # Parse all files with an &#39;_&#39; and (name&lt;=&#39;10&#39;) notebook2script --fnameout=&#39;test_25.py&#39; &#39;&#39;&#39; # initial checks if (allFiles is None) and (upTo is not None): allFiles=True # Enable allFiles if upTo is present if (fname is None) and (not allFiles): print(&#39;Should provide a file name&#39;) if not allFiles: notebook2scriptSingle(fname, fnameout) else: print(&#39;Begin...&#39;) [notebook2scriptSingle(f, fnameout) for f in getSortedFiles(allFiles,upTo)] print(&#39;...End&#39;) def notebook2scriptSingle(fname, *fname_out): &quot;Finds cells starting with `#export` and puts them into a new module&quot; fname = Path(fname) if (fname_out[0]==None): fname_out = f&#39;nb_{fname.stem.split(&quot;_&quot;)[0]}.py&#39; else: fname_out = fname_out[0] main_dic = json.load(open(fname,&#39;r&#39;,encoding=&quot;utf-8&quot;)) code_cells = [c for c in main_dic[&#39;cells&#39;] if is_export(c)] module = f&#39;&#39;&#39; ################################################# ### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ### ################################################# # file to edit: {fname.name} &#39;&#39;&#39; for cell in code_cells: module += &#39;&#39;.join(cell[&#39;source&#39;][1:]) + &#39; n n&#39; # remove trailing spaces module = re.sub(r&#39; +$&#39;, &#39;&#39;, module, flags=re.MULTILINE) if not (fname.parent/&#39;exp&#39;).exists(): (fname.parent/&#39;exp&#39;).mkdir() output_path = fname.parent/&#39;exp&#39;/fname_out with io.open(output_path, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f: f.write(module[:-2]) print(f&quot;Converted {fname} to {output_path}&quot;) if __name__ == &#39;__main__&#39;: fire.Fire(notebook2script) . How to extract modules from notebooks . Mark cells to be exported in your notebook . notebook2scriptexpects a keyword at the top of each cell to be exported. This keyword is #export. . variable3 = &#39;Not this one&#39; . You got the idea . Export your module my_great_module . #code from Jeremy Howard (fastai v2) #!python notebook2script.py &quot;00D059_init_and_import.ipynb&quot; !python notebook2script.py --fnameout=&quot;my_great_module.py&quot; &quot;2021-09-29-nbdev-notebook2script.ipynb&quot; . Converted 2021-09-29-nbdev-notebook2script.ipynb to exp/my_great_module.py . Exported module . If subfolder exp doesn&#39;t exist, it will be automatically created. . And my_great_module.py is being created as well. . . Here is the content generated. . !cat exp/my_great_module.py . ################################################# ### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ### ################################################# # file to edit: 2021-09-29-nbdev-notebook2script.ipynb variable = &#39;This will be exported in a module&#39; variable2 = &#39;This one as well&#39; . Python library needed: fire . import sys !conda install --yes --prefix {sys.prefix} -c conda-forge fire . Collecting package metadata (current_repodata.json): done Solving environment: done ==&gt; WARNING: A newer version of conda exists. &lt;== current version: 4.10.1 latest version: 4.10.3 Please update conda by running $ conda update -n base -c defaults conda ## Package Plan ## environment location: /home/guillaume/anaconda3/envs/xgboost added / updated specs: - fire The following packages will be UPDATED: fire 0.2.1-py_0 --&gt; 0.4.0-pyh44b312d_0 Preparing transaction: done Verifying transaction: done Executing transaction: done . Combine with pdoc to generate documentation . #!python notebook2script.py &quot;00D059_init_and_import.ipynb&quot; library_name = &quot;dataprophet&quot; notebook_name = &quot;00 - dataprophet library - dataprophet&quot; !python notebook2script.py --fnameout=&quot;{library_name}.py&quot; &quot;{notebook_name}.ipynb&quot; !pdoc --html --output-dir exp/html --force &quot;exp/{library_name}.py&quot; . Note the {} notation which allows to use ipython variables as arguments to bash commands . Proper docstring . See in Autogenerate documentation from custom python classes .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/nbdev-notebook2script.html",
            "relUrl": "/blog/nbdev-notebook2script.html",
            "date": " • Sep 29, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Fix pythoncom37.dll popup when launching Jupyter Notebook",
            "content": "when having this popup . . in my case it refers to stablebaselines3 conda environment. . I just rename pythoncom37.dllto pythoncom37.dll.old in AppData Local Continuum anaconda3 envs stablebaselines3 Library bin .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/Fix-pythoncom37.dll-Jupyter-Notebook.html",
            "relUrl": "/blog/Fix-pythoncom37.dll-Jupyter-Notebook.html",
            "date": " • Sep 28, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Fix non-unique cell issue in Jupyter Notebook",
            "content": "subject is explained in https://github.com/jupyter/notebook/issues/6001 . import nbformat as nbf from glob import glob import uuid def get_cell_id(id_length=8): return uuid.uuid4().hex[:id_length] # your notebook name/keyword nb_name = &#39;04 - data analysis from dataprophet - W34.ipynb&#39; notebooks = list(filter(lambda x: nb_name in x, glob(&quot;./*.ipynb&quot;, recursive=True))) # iterate over notebooks for ipath in sorted(notebooks): # load notebook ntbk = nbf.read(ipath, nbf.NO_CONVERT) cell_ids = [] for cell in ntbk.cells: cell_ids.append(cell[&#39;id&#39;]) # reset cell ids if there are duplicates if not len(cell_ids) == len(set(cell_ids)): for cell in ntbk.cells: cell[&#39;id&#39;] = get_cell_id() nbf.write(ntbk, ipath) .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/Fix-non-unique-cell-issue-in-Jupyter-Notebook.html",
            "relUrl": "/blog/Fix-non-unique-cell-issue-in-Jupyter-Notebook.html",
            "date": " • Sep 16, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Logbook for September 21",
            "content": "Week 35 - September 21 . Thursday 9/2 . Paper reviewed on arxiv about Continuous Control With Deep Reinforcement Learning. (Lillicrap et. al - 2015) arXiv:1509.02971. This is about DDPG. Initial paper comes from David Silver: Deterministic policy gradient algorithms in ICML 2014, but is not easy to read. Here is a review from towardsdatascience, in which the Deep Deterministic Policy Gradients (DDPG) is presented, and is written for people who wish to understand the DDPG algorithm. . Week 36 - September 21 . Monday 9/6 . Install of barrier to share keyboard/mouse between linux and windows. Nice combinaison with KVM usb switch. . Move wsl to another drive with move-wsl . Wednesday 9/8 . Creation of custom gym environment and optimization using DQN, then DDPG with stable baselines 3. Takes around 50,000 steps to optimize a ultra simple grid problem… No success with DDPG, something missing? . Thursday 9/9 . Still playing with gym and stable baselines 3. A2C, PPO and SAC are working but DDPG and TD3 are not (and I don’t know why) . Week 38 - September 21 . Monday 9/20 . Back to Aniti RL virtual school. Looking for material to be used to explain RL to my colleagues, and how to properly describe the experience I am running with gym. . Certainly will start lectures from deepming: 2021 DeepMind x UCL RL Lecture Series . Thursday 9/23 . Start plotly course from datacamp using my datacamp learning process. I need basic interactivity and 3d plots to illustrate reward functions. .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/logbook-September.html",
            "relUrl": "/blog/logbook-September.html",
            "date": " • Sep 1, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "git clean repo with bfg",
            "content": "bfg website . bfg installation from scratch . install java8 . sudo apt install openjdk-8-jre-headless java -version &gt;&gt; openjdk version &quot;1.8.0_312&quot; &gt;&gt; OpenJDK Runtime Environment (build 1.8.0_312-8u312-b07-0ubuntu1~20.04-b07) &gt;&gt; OpenJDK 64-Bit Server VM (build 25.312-b07, mixed mode) . download bfg.jar . cd ~ mkdir -p Applications/bfg cd Applications/bfg # link from https://rtyley.github.io/bfg-repo-cleaner/ wget https://repo1.maven.org/maven2/com/madgag/bfg/1.14.0/bfg-1.14.0.jar . and add an alias to .bashrc . $ grep -n -s bfg .* .bashrc:95:alias bfg=&#39;java -jar ~/Applications/bfg/bfg-1.14.0.jar&#39; $ source .bashrc . General usage . We will fix ~/git/d059-vld-ic . Remove big files . no need to create a clone, we can directly work on our repo . bfg --strip-blobs-bigger-than 100M ~/git/d059-vld-ic cd ~/git/d059-vld-ic git reflog expire --expire=now --all &amp;&amp; git gc --prune=now --aggressive . Note: if you get a message Warning : no large blobs matching criteria found in packfiles - does the repo need to be packed?, you have to launch git gc . Remove big files from protected commits . Protected commits -- These are your protected commits, and so their contents will NOT be altered: * commit d914f24e (protected by &#39;HEAD&#39;) . In that case it is even easier, no need of bfg: . git rm --cached &lt;my large file&gt; git commit --amend -C HEAD . Remove forbidden files such as .mp3, .tar.gz . need to create a clone, we can directly work on our repo . $ cd ~/Applications/bfg java -jar bfg-1.13.0.jar --delete-files &#39;*.mp3&#39; --no-blob-protection ~/git/data-scientist-skills java -jar bfg-1.13.0.jar --delete-files &#39;*.tar.gz&#39; --no-blob-protection ~/git/data-scientist-skills git reflog expire --expire=now --all &amp;&amp; git gc --prune=now --aggressive . Improve .gitignore . see git ignore large files .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/git-clean-large-files.html",
            "relUrl": "/blog/git-clean-large-files.html",
            "date": " • Jul 29, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "git ignore large files",
            "content": "files used . $ ll .gitignore* update_git_ignore.sh .gitignore .gitignore_bigfiles .gitignore_static update_git_ignore.sh . .gitignore_static . Here is my standard entries for .gitignore . $ cat .gitignore_static *.history */.ipynb_checkpoints/* .ipynb_checkpoints/* mlflow/* mlruns/* . .gitignore_bigfiles, .gitignore . Those are filed created by update_git_ignore.sh . update_git_ignore.sh . add all files &gt; 100MB in .gitignore_bigfiles . merge .gitignore_static and .gitignore_bigfiles as .gitignore . display .gitignore . $ cat update_git_ignore.sh #!/bin/bash #update gitignore_bigfiles find . -size +100M -not -path &quot;./.git*&quot;| sed &#39;s|^ ./||g&#39; | cat &gt; .gitignore_bigfiles # create gitignore as concat of gitingore_static and gitignore_bigfiles cat .gitignore_static .gitignore_bigfiles &gt; .gitignore # print content of .gitignore_bigfiles cat .gitignore_bigfiles . Usage . Launch ./update_git_ignore.shbefore adding files to git . $ ./update_git_ignore.sh mlflow/1/5699a81e1a6a44ef8afecd98fff987fc/artifacts/Data Profile.html $ git add . $ git commit -m &#39;example without large files&#39; .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/git-ignore-large-files.html",
            "relUrl": "/blog/git-ignore-large-files.html",
            "date": " • Jul 9, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "Deep Neural Network with PyTorch - Coursera",
            "content": "Coursera website: Deep Neural Networks with PyTorch . Course certificate . Week 1 - Tensor and Datasets . Learning Objectives . Tensors 1D | Two-Dimensional Tensors | Data Set | Differentiation in PyTorch | . notebook . notebook . Tensors 1D . The basics . #initialize import torch a=torch.tensor([7,4,3,2,6]) #dtype, type() a.dtype a.type() #convert with type a=a.type(torch.FloatTensor) #size, ndimension a.size() a.ndimension() #convert to 2D a_2D=a.view(-1, 1) #from_numpy, to numpy import numpy as np numpy_array = np.array([0.0, 1.0, 2.0, 3.0, 4.0]) torch_tensor = torch.from_numpy(numpy_array) back_to_numpy = torch_tensor.numpy() #from pandas import pandas as pd pandas_series = pd.Series([0.1, 2, 0.3, 10.1]) pandas_to_torch = torch.from_numpy(pandas_series.values) #to list this_tensor = torch.tensor([0, 1, 2, 3]) torch_to_list = this_tensor.tolist() #item new_tensor = torch.tensor([5, 2, 6, 1]) new_tensor[0].item() #indexing and slicing c[3:5]=torch.tensor([300.0, 4.0]) . basic operations . #hadamard product z = u*v #dot product, (produit scalaire) result = torch.dot(u, v) . universal functions, mean, max, mathematical functions, plot with linspace . #mean a.mean() #max b.max() #plot y=sin(x) import matplotlib.pyplot as plt %matplotlib inline x = torch.linspace(0, 2 * np.pi, 100) y = torch.sin(x) plt.plot(x.numpy(), y.numpy()) . Ungraded lab . 1.1_1Dtensors_v2.ipynb . Tensors 2D . notebook . Tensor creation in 2D . a = [ [11, 12, 13], [21, 22, 23], [31, 32, 33] ] A = torch.tensor(a) A.ndimension() &gt;&gt; 2 A.shape &gt;&gt; torch.Size([3, 3]) A.size() &gt;&gt; torch.Size([3, 3]) #number of elements A.numel() &gt;&gt; 9 . Indexing and slicing in 2D . A[0, 0:2] &gt;&gt; tensor([11, 12]) A[1:3, 2] &gt;&gt; tensor([23, 33]) . Basic operations in 2D: hadamard product, matrix multiplication . X = torch.tensor([[1,0], [0,1]]) Y = torch.tensor([[2,1], [1,2]]) #hadamard product Z = X*Y Z &gt;&gt; tensor([[2, 0], [0, 2]]) A = torch.tensor([ [0, 1, 1], [1, 0, 1]]) B = torch.tensor([ [1, 1], [1, 1], [-1, 1]]) #matrix multiplication C = torch.mm(A, B) C &gt;&gt; tensor([[0, 2], [0, 2]]) . Ungraded lab . 1.1_2 Two-Dimensional Tensors_v2.ipynb . Derivatives in Pytorch . Derivatives . using $y(x)=x^2$ . x = torch.tensor(2., requires_grad=True) y = x ** 2 #calculate derivative df/dx y.backward() #evaluate at x : df/dx(x) x.grad &gt;&gt; tensor(4.) . using $z(x)=x^2+2x+1$ . x = torch.tensor(2., requires_grad=True) z = x**2 + 2*x + 1 z.backward() x.grad &gt;&gt; tensor(6.) . Note: in my version of pytorch (1.7.1), I cannot use torch.int dtypes. . RuntimeError Traceback (most recent call last) &lt;ipython-input-92-979d0f10c1e7&gt; in &lt;module&gt; -&gt; 3 x = torch.tensor(2, requires_grad=True) 4 z = x**2 + 2*x + 1 5 z.backward() RuntimeError: Only Tensors of floating point and complex dtype can require gradients . Partial derivatives . using $f(u, v)=uv+u^2$, $ frac{ partial f(u,v)}{ partial u} = v+2u$, $ frac{ partial f(u,v)}{ partial v} = u$ . u = torch.tensor(1., requires_grad=True) v = torch.tensor(2., requires_grad=True) f = u*v + u**2 #calculate all partial derivatives df/du and df/dv f.backward() #evaluate partial derivative with respect to u df/du at u, v : df/du(u, v) u.grad &gt;&gt; tensor(4.) #evaluate partial derivative with respect to v df/dv at u, v : df/dv(u, v) v.grad &gt;&gt; tensor(1.) . Ungraded lab . 1.2derivativesandGraphsinPytorch_v2.ipynb . With some explanation about .detach() pointing to torch.autograd documentation. In this page, there is a link to walkthrough of backprop video. . Will have to go back to .detach() . Simple Dataset . Build a Dataset Class and Object . from torch.utils.data import Dataset class toy_set(Dataset): def __init__(self, length=100, transform=None): self.x = 2*torch.ones(length, 2) self.y = torch.ones(length, 1) self.len = length self.transform = transform def __getitem__(self, index): sample=self.x[index], self.y[index] if self.transform: sample = self.transform(sample) return sample def __len__(self): return self.len dataset = toy_set() len(dataset) &gt;&gt; 100 dataset[0] (tensor([2., 2.]), tensor([1.])) . Build a Dataset Transform (e.g. normalize or standardize) . class add_mult(object): def __init__(self, addx=1, muly=1): self.addx = addx self.muly = muly def __call__(self, sample): x=sample[0] y=sample[1] x=x+self.addx y=y*self.muly sample=x, y return sample # automatically apply the transform a_m = add_mult() dataset_ = toy_set(transform=a_m) dataset_[0] &gt;&gt; (tensor([3., 3.]), tensor([1.])) . Compose Transforms . class mult(object): def __init__(self, mul=100): self.mul = mul def __call__(self, sample): x = sample[0] y = sample[1] x = x * self.mul y = y * self.mul sample = x, y return sample from torchvision import transforms data_transform = transforms.Compose([add_mult(), mult()]) # automatically apply the composed transform dataset_tr = toy_set(transform=data_transform) dataset_tr[0] &gt;&gt; (tensor([300., 300.]), tensor([100.])) . Ungraded lab . 1.3.1_simple_data_set_v2.ipynb . Dataset . Dataset Class for Images . from PIL import Image import pandas as pd import os from matplotlib.pyplot import imshow from torch.utils.data import Dataset, DataLoader . class Dataset(Dataset): def __init__(self, csv_file, data_dir, transform=None): self.transform = transform self.data_dir = data_dir data_dir_csv_file = os.path.join(self.data_dir, csv_file) self.data_name = pd.read_csv(data_dir_csv_file) self.len = self.data_name.shape[0] def __len__(self): return self.len def __getitem__(self, idx): img_name=os.path.join(self.data_dir, self.data_name.iloc[idx, 1]) image = Image.open(img_name) y = self.data_name.iloc[idx, 0] if self.transform: image = self.transform(image) return image, y def show_data(data_sample, shape = (28, 28)): plt.imshow(data_sample[0].numpy().reshape(shape), cmap=&#39;gray&#39;) plt.title(&#39;y = &#39; + data_sample[1]) . dataset = Dataset(csv_file=csv_file, data_dir=directory) show_data(dataset[0]) . Torch Vision Transforms . import torchvision.transforms as transforms transforms.CenterCrop(20) transforms.ToTensor() croptensor_data_transform = transforms.Compose( [ transforms.CenterCrop(20), transforms.ToTensor() ] ) dataset = Dataset(csv_file=csv_file, data_dir=directory, transform=croptensor_data_transform) dataset[0][0].shape &gt;&gt; torch.Size([1, 20, 20]) . Torch Vision Datasets . MNIST example . import torchvision.datasets as dsets dataset = dsets.MNIST(root=&#39;./data&#39;, train = False, download = True, transform = transforms.ToTensor()) . Ungraded lab . 1.3.2_Datasets_and_transforms.ipynb . 1.3.3_pre-Built Datasets_and_transforms_v2.ipynb . Week 2 - Linear Regression . Learning Objectives . Linear Regression Prediction | Linear Regression Training | Loss | Gradient Descent | Cost | Linear Regression Training PyTorch | . notebook . notebook . Linear Regression in 1D - Prediction . Simple linear regression - prediction . import torch w = torch.tensor(2.0, requires_grad=True) b = torch.tensor(-1.0, requires_grad=True) def forward(x): y=w*x+b return y . x=torch.tensor([1.0]) yhat=forward(x) yhat &gt;&gt; tensor([1.], grad_fn=&lt;AddBackward0&gt;) x=torch.tensor([[1.0],[2.0]]) forward(x) &gt;&gt; tensor([[1.], [3.]], grad_fn=&lt;AddBackward0&gt;) . PyTorch - Class Linear . from torch.nn import Linear torch.manual_seed(1) model = Linear(in_features=1, out_features=1) list(model.parameters()) &gt;&gt; [Parameter containing: tensor([[0.5153]], requires_grad=True), Parameter containing: tensor([-0.4414], requires_grad=True)] . x=torch.tensor([[1.0],[2.0]]) model(x) &gt;&gt; tensor([[0.0739], [0.5891]], grad_fn=&lt;AddmmBackward&gt;) . PyTorch - Custom Modules . import torch.nn as nn class LR(nn.Module): def __init__(self, in_size, output_size): super(LR, self).__init__() self.linear = nn.Linear(in_size, output_size) def forward(self, x): out = self.linear(x) return out . model = LR(1, 1) list(model.parameters()) &gt;&gt; [Parameter containing: tensor([[-0.9414]], requires_grad=True), Parameter containing: tensor([0.5997], requires_grad=True)] . x=torch.tensor([[1.0],[2.0]]) model(x) &gt;&gt; tensor([[-0.3417], [-1.2832]], grad_fn=&lt;AddmmBackward&gt;) . Model state_dict() . this returns a python dictionary. We will use it as our models get more complex. One Function is to map the relationship of the linear layers to its parameters. we can print out the keys and values. . model.state_dict() &gt;&gt; OrderedDict([(&#39;linear.weight&#39;, tensor([[-0.9414]])), (&#39;linear.bias&#39;, tensor([0.5997]))]) . Ungraded lab . 2.1Prediction1Dregression_v3.ipynb . Linear Regression Training . loss function presented is mean squared error . $l(w,b)= frac{1}{N} displaystyle sum_{n=1}^{N}(y_n-(wx_n+b))^2$ . Gradient Descent and cost . PyTorch Slope . import torch w=torch.tensor(-10.0, requires_grad=True) X=torch.arange(-3,3,0.1).view(-1, 1) f = -3*X . import matplotlib.pyplot as plt plt.plot(X.numpy(), f.numpy()) plt.show() Y = f+0.1*torch.randn(X.size()) plt.plot(X.numpy(), Y.numpy(), &#39;ro&#39;) plt.show() . def forward(x): return w*x def criterion(yhat, y): return torch.mean((yhat-y)**2) . lr = 0.1 for epoch in range(4): Yhat = forward(X) loss= criterion(Yhat, Y) loss.backward() w.data = w.data - lr*w.grad.data w.grad.data.zero_() . Ungraded lab . 2.2_linear_regression_one_parameter_v3.ipynb . Linear Regression Training in PyTorch . Cost surface . # The class for plot the diagram class plot_error_surfaces(object): # Constructor def __init__(self, w_range, b_range, X, Y, n_samples = 30, go = True): W = np.linspace(-w_range, w_range, n_samples) B = np.linspace(-b_range, b_range, n_samples) w, b = np.meshgrid(W, B) Z = np.zeros((30,30)) count1 = 0 self.y = Y.numpy() self.x = X.numpy() for w1, b1 in zip(w, b): count2 = 0 for w2, b2 in zip(w1, b1): Z[count1, count2] = np.mean((self.y - w2 * self.x + b2) ** 2) count2 += 1 count1 += 1 self.Z = Z self.w = w self.b = b self.W = [] self.B = [] self.LOSS = [] self.n = 0 if go == True: plt.figure() plt.figure(figsize = (7.5, 5)) plt.axes(projection=&#39;3d&#39;).plot_surface(self.w, self.b, self.Z, rstride = 1, cstride = 1,cmap = &#39;viridis&#39;, edgecolor = &#39;none&#39;) plt.title(&#39;Cost/Total Loss Surface&#39;) plt.xlabel(&#39;w&#39;) plt.ylabel(&#39;b&#39;) plt.show() plt.figure() plt.title(&#39;Cost/Total Loss Surface Contour&#39;) plt.xlabel(&#39;w&#39;) plt.ylabel(&#39;b&#39;) plt.contour(self.w, self.b, self.Z) plt.show() # Setter def set_para_loss(self, W, B, loss): self.n = self.n + 1 self.W.append(W) self.B.append(B) self.LOSS.append(loss) # Plot diagram def final_plot(self): ax = plt.axes(projection = &#39;3d&#39;) ax.plot_wireframe(self.w, self.b, self.Z) ax.scatter(self.W,self.B, self.LOSS, c = &#39;r&#39;, marker = &#39;x&#39;, s = 200, alpha = 1) plt.figure() plt.contour(self.w,self.b, self.Z) plt.scatter(self.W, self.B, c = &#39;r&#39;, marker = &#39;x&#39;) plt.xlabel(&#39;w&#39;) plt.ylabel(&#39;b&#39;) plt.show() # Plot diagram def plot_ps(self): plt.subplot(121) plt.ylim plt.plot(self.x, self.y, &#39;ro&#39;, label=&quot;training points&quot;) plt.plot(self.x, self.W[-1] * self.x + self.B[-1], label = &quot;estimated line&quot;) plt.xlabel(&#39;x&#39;) plt.ylabel(&#39;y&#39;) plt.ylim((-10, 15)) plt.title(&#39;Data Space Iteration: &#39; + str(self.n)) plt.subplot(122) plt.contour(self.w, self.b, self.Z) plt.scatter(self.W, self.B, c = &#39;r&#39;, marker = &#39;x&#39;) plt.title(&#39;Total Loss Surface Contour Iteration&#39; + str(self.n)) plt.xlabel(&#39;w&#39;) plt.ylabel(&#39;b&#39;) plt.show() get_surface = plot_error_surfaces(15, 15, X, Y, 30) . . . PyTorch (hard way) . def forward(x): y=w*x+b return y def criterion(yhat, y): return torch.mean((yhat-y)**2) w = torch.tensor(-15.0, requires_grad=True) b = torch.tensor(-10.0, requires_grad=True) X = torch.arange(-3, 3, 0.1).view(-1, 1) f = 1*X-1 Y = f+0.1*torch.rand(X.size()) . lr = 0.1 for epoch in range(15): Yhat=forward(X) loss=criterion(Yhat, Y) loss.backward() w.data=w.data-lr*w.grad.data w.grad.data.zero_() b.data=b.data-lr*b.grad.data b.grad.data.zero_() . Ungraded lab . 2.3_training_slope_and_bias_v3.ipynb . Stochastic Gradient Descent and the Data Loader . Stochastic Gradient Descent in PyTorch . w = torch.tensor(-15.0, requires_grad=True) b = torch.tensor(-10.0, requires_grad=True) X = torch.arange(-3, 3, 0.1).view(-1, 1) f = -3*X Y=f+0.1*torch.randn(X.size()) def forward(x): y=w*x+b return y def criterion(yhat, y): return torch.mean((yhat-y)**2) . lr = 0.1 for epoch in range(4): for x, y in zip(X, Y): yhat=forward(x) loss=criterion(yhat, y) loss.backward() w.data=w.data-lr*w.grad.data w.grad.data.zero_() b.data=b.data-lr*b.grad.data b.grad.data.zero_() . Stochastic Gradient Descent DataLoader . dataset . from torch.utils.data import Dataset class Data(Dataset): def __init__(self): self.x = torch.arange(-3, 3, 0.1).view(-1, 1) self.y = -3*X+1 self.len = self.x.shape[0] def __getitem__(self, index): return self.x[index], self.y[index] def __len__(self): return self.len dataset = Data() . dataloader . from torch.utils.data import DataLoader dataset=Data() trainloader = DataLoader(dataset=dataset, batch_size=1) . stochastic gradient descent . for x, y in trainloader: yhat = forward(x) loss = criterion(yhat, y) loss.backward() w.data=w.data-lr*w.grad.data b.data=b.data-lr*b.grad.data w.grad.data.zero_() b.grad.data.zero_() . Ungraded lab . 3.1_stochastic_gradient_descent_v3.ipynb . Mini-Batch Gradient Descent . Iterations = $ frac{ text{training size}}{ text{batch size}}$ . Mini-Batch Gradient Descent in Pytorch . dataset = Data() trainloader = DataLoader(dataset=dataset, batch_size=5) lr=0.1 LOSS = [] for epoch in range(4): for x, y in trainloader: yhat=forward(x) loss = criterion(yhat, y) loss.backward() w.data=w.data-lr*w.grad.data b.data=b.data-lr*b.grad.data w.grad.data.zero_() b.grad.data.zero_() LOSS.append(loss.item()) . Optimization in PyTorch . criterion = nn.MSELoss() trainloader = DataLoader(dataset=dataset, batch_size=1) model = LR(1,1) from torch import nn, optim optimizer = optim.SGD(model.parameters(), lr = 0.01) optimizer.state_dict() &gt;&gt; {&#39;state&#39;: {}, &#39;param_groups&#39;: [{&#39;lr&#39;: 0.01, &#39;momentum&#39;: 0, &#39;dampening&#39;: 0, &#39;weight_decay&#39;: 0, &#39;nesterov&#39;: False, &#39;params&#39;: [0, 1]}]} . for epoch in range(100): for x, y in trainloader: yhat = model(x) loss = criterion(yhat, y) optimizer.zero_grad() loss.backward() optimizer.step() . . Ungraded lab . 3.3_PyTorchway_v3.ipynb . Training, Validation and Test Split . standard explanation about Train, Validation, Test . Training, Validation and Test Split in PyTorch . Dataset to generate train_data and val_data . from torch.utils.data import Dataset, DataLoader class Data(Dataset): def __init__(self, train = True): self.x = torch.arange(-3, 3, 0.1).view(-1, 1) self.f = -3*self.x+1 self.y = self.f+0.1*torch.randn(self.x.size()) self.len = self.x.shape[0] if train == True: self.y[0] = 0 self.y[50:55] = 20 else: pass def __getitem__(self, index): return self.x[index], self.y[index] def __len__(self): return self.len train_data = Data() val_data = Data(train=False) . LR model . import torch.nn as nn class LR(nn.Module): def __init__(self, input_size, output_size): super(LR, self).__init__() self.linear = nn.Linear(input_size, output_size) def forward(self, x): out=self.linear(x) return out . criterion = nn.MSELoss() trainloader = DataLoader(dataset=train_data, batch_size=1) . epochs = 10 learning_rates = [0.0001, 0.001, 0.01, 0.1, 1] validation_error = torch.zeros(len(learning_rates)) test_error=torch.zeros(len(learning_rates)) MODELS=[] . from torch import optim from tqdm import tqdm for i, learning_rate in tqdm(enumerate(learning_rates)): model = LR(1,1) optimizer = optim.SGD(model.parameters(), lr = learning_rate) for epoch in range(epochs): for x, y in trainloader: yhat = model(x) loss = criterion(yhat, y) optimizer.zero_grad() loss.backward() optimizer.step() yhat=model(train_data.x) loss=criterion(yhat, train_data.y) test_error[i]=loss.item() yhat=model(val_data.x) loss=criterion(yhat, val_data.y) validation_error[i]=loss.item() MODELS.append(model) . import numpy as np plt.semilogx(np.array(learning_rates), validation_error.numpy(), label=&#39;training cost/total loss&#39;) plt.semilogx(np.array(learning_rates), test_error.numpy(), label=&#39;validation cost/total loss&#39;) plt.ylabel(&#39;Cost Total loss&#39;) plt.xlabel(&#39;learning rate&#39;) plt.legend() plt.show() . . Week 3 - Multiple Input Output Linear Regression - Logistic Regression for Classification . Learning Objectives . Multiple Linear Regression | Multiple Linear Regression Training | Linear Regression Multiple Outputs | Linear Regression Multiple Outputs Training | . notebook . notebook . Multiple Input Linear Regression Prediction . Class Linear . import torch from torch.nn import Linear torch.manual_seed(1) model = Linear(in_features=2, out_features=1) list(model.parameters()) &gt;&gt; [Parameter containing: tensor([[ 0.3643, -0.3121]], requires_grad=True), Parameter containing: tensor([-0.1371], requires_grad=True)] model.state_dict() &gt;&gt; OrderedDict([(&#39;weight&#39;, tensor([[ 0.3643, -0.3121]])), (&#39;bias&#39;, tensor([-0.1371]))]) #predictions for multiple samples X = torch.tensor([[1.0, 1.0], [1.0, 2.0], [1.0, 3.0]]) yhat = model(X) yhat &gt;&gt; tensor([[-0.0848], [-0.3969], [-0.7090]], grad_fn=&lt;AddmmBackward&gt;) . Custom Modules . import torch.nn as nn class LR(nn.Module): def __init__(self, input_size, output_size): super(LR, self).__init__() self.linear = nn.Linear(input_size, output_size) def forward(self, x): out = self.linear(x) return out . Ungraded lab . 4.1.multiple_linear_regression_prediction_v2.ipynb . Multiple Input Linear Regression Training . Cost function and Gradient Descent for Multiple Linear Regression . Cost function . l(w,b)=1N∑n=1N(yn−(xnw+b))2l(w,b)= frac{1}{N} displaystyle sum_{n=1}^{N}(y_n-(x_nw+b))^2l(w,b)=N1​n=1∑N​(yn​−(xn​w+b))2 . Gradient of loss function with respect to the weights . ∇l(w,b)=[∂l(w,b)∂w1⋮∂l(w,b)∂wd] nabla l(w,b) = begin{bmatrix} frac{ partial l(w,b)}{ partial w_1} vdots frac{ partial l(w,b)}{ partial w_d} end{bmatrix}∇l(w,b)=⎣ . ⎡​∂w1​∂l(w,b)​⋮∂wd​∂l(w,b)​​⎦ . ⎤​ . Gradient of loss function with respect to the bias . ∂l(w,b)∂b frac{ partial l(w,b)}{ partial b}∂b∂l(w,b)​ . Update of weights . wk+1=wk−η∇l(wk,bk)w^{k+1} = w^k- eta nabla l(w^k,b^k)wk+1=wk−η∇l(wk,bk) . [w1k+1⋮wdk+1]=[w1k⋮wdk]−η[∂l(wk,bk)∂w1⋮∂l(wk,bk)∂wd] begin{bmatrix} w_1^{k+1} vdots w_d^{k+1} end{bmatrix}= begin{bmatrix} w_1^{k} vdots w_d^{k} end{bmatrix}- eta begin{bmatrix} frac{ partial l(w^k,b^k)}{ partial w_1} vdots frac{ partial l(w^k,b^k)}{ partial w_d} end{bmatrix}⎣ . ⎡​w1k+1​⋮wdk+1​​⎦ . ⎤​=⎣ . ⎡​w1k​⋮wdk​​⎦ . ⎤​−η⎣ . ⎡​∂w1​∂l(wk,bk)​⋮∂wd​∂l(wk,bk)​​⎦ . ⎤​ . and update of bias . bk+1=bk−η∂l(wk,bk)∂bb^{k+1}=b^k- eta frac{ partial l(w^k,b^k)}{ partial b}bk+1=bk−η∂b∂l(wk,bk)​ . Train the model in PyTorch . from torch import nn, optim import torch class LR(nn.Module): def __init__(self, input_size, output_size): super(LR, self).__init__() self.linear = nn.Linear(input_size, output_size) def forward(self, x): out = self.linear(x) return out . from torch.utils.data import Dataset, DataLoader class Data2D(Dataset): def __init__(self): self.x = torch.zeros(20,2) self.x[:, 0] = torch.arange(-1,1,0.1) self.x[:, 1] = torch.arange(-1,1,0.1) self.w = torch.tensor([ [1.0], [1.0]]) self.b = 1 self.f = torch.mm(self.x, self.w)+self.b self.y = self.f + 0.1*torch.randn((self.x.shape[0], 1)) self.len = self.x.shape[0] def __getitem__(self, index): return self.x[index], self.y[index] def __len__(self): return self.len . data_set = Data2D() criterion = nn.MSELoss() trainloader = DataLoader(dataset=data_set, batch_size=2) model = LR(input_size=2, output_size=1) optimizer = optim.SGD(model.parameters(), lr=0.1) . for epoch in range(100): for x, y in trainloader: yhat = model(x) loss = criterion(yhat, y) optimizer.zero_grad() loss.backward() optimizer.step() . Ungraded lab . 4.2.multiple_linear_regression_training_v2.ipynb . Multiple Output Linear Regression . Linear regression with multiple outputs . . Custom Modules . import torch.nn as nn import torch class LR(nn.Module): def __init__(self, input_size, output_size): super(LR, self).__init__() self.linear = nn.Linear(input_size, output_size) def forward(self, x): out = self.linear(x) return out torch.manual_seed(1) model = LR(input_size=2, output_size=2) list(model.parameters()) &gt;&gt; [Parameter containing: tensor([[ 0.3643, -0.3121], [-0.1371, 0.3319]], requires_grad=True), Parameter containing: tensor([-0.6657, 0.4241], requires_grad=True)] #with 2 columns and 3 rows X=torch.tensor([[1.0, 1.0], [1.0,2.0], [1.0, 3.0]]) Yhat = model(X) Yhat &gt;&gt; tensor([[-0.6135, 0.6189], [-0.9256, 0.9508], [-1.2377, 1.2827]], grad_fn=&lt;AddmmBackward&gt;) . Ungraded lab . 4.3.multi-target_linear_regression.ipynb . Multiple Output Linear Regression Training . Training in PyTorch . Training is the same, what changes is Dataset: . from torch.utils.data import Dataset, DataLoader class Data2D(Dataset): def __init__(self): self.x = torch.zeros(20,2) self.x[:, 0] = torch.arange(-1,1,0.1) self.x[:, 1] = torch.arange(-1,1,0.1) self.w = torch.tensor([ [1.0, -1.0], [1.0, -1.0]]) self.b = torch.tensor([[1.0, -1.0]]) self.f = torch.mm(self.x, self.w)+self.b self.y = self.f + 0.1*torch.randn((self.x.shape[0], 1)) self.len = self.x.shape[0] def __getitem__(self, index): return self.x[index], self.y[index] def __len__(self): return self.len . and model instantiation . from torch import nn, optim data_set = Data2D() criterion = nn.MSELoss() trainloader = DataLoader(dataset=data_set, batch_size=1) model = LR(input_size=2, output_size=2) optimizer = optim.SGD(model.parameters(), lr=0.001) . Training: . for epoch in range(100): for x, y in trainloader: yhat = model(x) loss = criterion(yhat, y) optimizer.zero_grad() loss.backward() optimizer.step() . Ungraded lab . 4.4.training_multiple_output_linear_regression.ipynb . Linear Classifier and Logistic Regression . σ(z)=11+e−z sigma(z)= frac{1}{1+e^{-z}}σ(z)=1+e−z1​ . sigmoid is used as the threshold function in logistic regression . Logistic Regression: Prediction . logistic function in PyTorch . as a function: torch.sigmoid . import torch import matplotlib.pyplot as plt z = torch.arange(-100, 100, 0.1).view(-1, 1) yhat = torch.sigmoid(z) plt.plot(z.numpy(), yhat.numpy()) . . as a class: nn.Signmoid() . import torch import torch.nn as nn import matplotlib.pyplot as plt z = torch.arange(-100, 100, 0.1).view(-1, 1) sig = nn.Sigmoid() yhat = sig(z) plt.plot(z.numpy(), yhat.numpy()) . torch.nn.Sigmoid vs torch.sigmoid - PyTorch Forums . torch.nn.Sigmoid (note the capital “S”) is a class. When you instantiate it, you get a function object, that is, an object that you can call like a function. In contrast, torch.sigmoid is a function. . nn.Sequential . . sequential_model = nn.Sequential(nn.Linear(1,1), nn.Sigmoid()) . nn.Module . import torch.nn as nn class logistic_regression(nn.Module): def __init__(self, in_size): super(logistic_regression, self).__init__() self.linear = nn.Linear(in_size, 1) def forward(self, x): z = torch.sigmoid(self.linear(x)) return z custom_model = logistic_regression(1) . Making a prediction . x=torch.tensor([[1.0], [2.0]]) custom_model(x) &gt;&gt; tensor([[0.4129], [0.3936]], grad_fn=&lt;SigmoidBackward&gt;) sequential_model(x) &gt;&gt; tensor([[0.2848], [0.2115]], grad_fn=&lt;SigmoidBackward&gt;) . Multidimensional Logistic Regression . custom_2D_model = logistic_regression(2) sequential_2D_model = nn.Sequential(nn.Linear(2, 1), nn.Sigmoid()) x=torch.tensor([[1.0, 2.0]]) yhat = sequential_2D_model(x) yhat &gt;&gt; tensor([[0.7587]], grad_fn=&lt;SigmoidBackward&gt;) . Ungraded lab . 5.1logistic_regression_prediction_v2.ipynb . Bernoulli Distribution and Maximum Likelihood Estimation . To fine the parameter values of the Bernoulli distribution, we do not maximize the likelihood function but the log of the likelihood function: Loss likelihood which is given by . l(θ)=ln⁡(p(Y∣θ))=∑n=1Nynln⁡(θ)+(1−yn)ln⁡(1−θ)l( theta) = ln(p(Y| theta))= displaystyle sum_{n=1}^{N}y_n ln( theta)+(1-y_n) ln(1- theta)l(θ)=ln(p(Y∣θ))=n=1∑N​yn​ln(θ)+(1−yn​)ln(1−θ) . Note: We want to get . θ^=argmaxθ(P(Y∣θ)) hat theta = argmax_ theta(P(Y| theta))θ^=argmaxθ​(P(Y∣θ)) . where . P(Y∣θ)=∏n=1Nθyn(1−θ)1−ynP(Y| theta) = displaystyle prod_{n=1}^{N} theta^{y_n}(1- theta)^{1-y_n}P(Y∣θ)=n=1∏N​θyn​(1−θ)1−yn​ . Logistic Regression Cross Entropy Loss . Loss function $l(w,b)= frac{1}{N} displaystyle sum_{n=1}^{N}(y_n- sigma(wx_n+b))^2$ . Cross entropy loss . l(θ)=−1N∑n=1Nynln⁡(σ(wxn+b))+(1−yn)ln⁡(1−σ(wxn+b))l( theta)=- frac{1}{N} displaystyle sum_{n=1}^{N}y_n ln( sigma(wx_n+b))+(1-y_n) ln(1- sigma(wx_n+b))l(θ)=−N1​n=1∑N​yn​ln(σ(wxn​+b))+(1−yn​)ln(1−σ(wxn​+b)) . def criterion(yhat, y): out = -1 * torch.mean(y * torch.log(yhat) + (1-y) * torch.log(1-yhat)) return out . Logistic Regression in PyTorch . Create a model (using Sequential) . model = nn.Sequential(nn.Linear(1, 1), nn.Sigmoid()) . or create a custom one . import torch.nn as nn class logistic_regression(nn.Module): def __init__(self, in_size): super(logistic_regression, self).__init__() self.linear = nn.Linear(in_size, 1) def forward(self, x): z = torch.sigmoid(self.linear(x)) return z . Then define our loss function . def criterion(yhat, y): out = -1 * torch.mean(y * torch.log(yhat) + (1-y) * torch.log(1-yhat)) return out . or simply BCE (binary cross entropy) . criterion = nn.BCELoss() . Putting all pieces together: . #dataset import torch from torch.utils.data import Dataset class Data(Dataset): def __init__(self): self.x = torch.arange(-1, 1, 0.1).view(-1, 1) self.y = torch.zeros(self.x.shape[0], 1) self.y[self.x[:, 0] &gt; 0.2] = 1 self.len = self.x.shape[0] def __getitem__(self, index): return self.x[index], self.y[index] def __len__(self): return self.len dataset = Data() # dataloader from torch.utils.data import DataLoader trainloader = DataLoader(dataset=dataset, batch_size=1) # model import torch.nn as nn model = nn.Sequential(nn.Linear(1, 1), nn.Sigmoid()) # optimizer from torch import optim optimizer = optim.SGD(model.parameters(), lr = 0.01) # loss criterion = nn.BCELoss() # training for epoch in range(100): for x, y in trainloader: yhat = model(x) loss = criterion(yhat, y) optimizer.zero_grad() loss.backward() optimizer.step() . Ungraded lab . 5.2.2bad_inshilization_logistic_regression_with_mean_square_error_v2.ipynb . Week 4 - Softmax regression . Learning Objectives . Using Lines to Classify Data | Softmax Prediction in PyTorch | Softmax Pytorch MNIST | . notebook . notebook . Softmax Prediction . Softmax is a combination of logistic regression and argmax . . Softmax function . Custom module using nn.module . import torch.nn as nn class Softmax(nn.Module): def __init__(self, in_size, out_size): super(Softmax, self).__init__() self.linear = nn.Linear(in_size, out_size) def forward(self, x): out = self.linear(x) return out . import torch torch.manual_seed(1) # 2 dimensions input samples and 3 output classes model = Softmax(2,3) x = torch.tensor([[1.0, 2.0]]) z = model(x) z &gt;&gt; tensor([[-0.4053, 0.8864, 0.2807]], grad_fn=&lt;AddmmBackward&gt;) _, yhat = z.max(1) yhat &gt;&gt; tensor([1]) . and with multiple samples . X=torch.tensor([[1.0, 1.0],[1.0, 2.0],[1.0, -3.0]]) z = model(X) z &gt;&gt; tensor([[-0.0932, 0.5545, -0.1433], [-0.4053, 0.8864, 0.2807], [ 1.1552, -0.7730, -1.8396]], grad_fn=&lt;AddmmBackward&gt;) _, yhat = z.max(1) yhat &gt;&gt; tensor([1, 1, 0]) . Softmax PyTorch . Load Data . import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets train_dataset = dsets.MNIST(root=&#39;./data&#39;, train = True, download = True, transform=transforms.ToTensor()) validation_dataset = dsets.MNIST(root=&#39;./data&#39;, train = False, download = True, transform=transforms.ToTensor()) . train_dataset[0] is a tuple with the image and the class: . Create Model . import torch.nn as nn class Softmax(nn.Module): def __init__(self, in_size, out_size): super(Softmax, self).__init__() self.linear = nn.Linear(in_size, out_size) def forward(self, x): out = self.linear(x) return out input_dim = 28 * 28 output_dim = 10 model = Softmax(input_dim, output_dim) . criterion = nn.CrossEntropyLoss() import torch.optim as optim optimizer = optim.SGD(model.parameters(), lr=0.01) n_epochs = 100 accuracy_list = [] train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = 100) validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000) . Train Model . from tqdm import tqdm for epoch in tqdm(range(n_epochs)): for x, y in train_loader: optimizer.zero_grad() z = model(x.view(-1, 28 * 28)) loss = criterion(z, y) loss.backward() optimizer.step() correct = 0 for x_test, y_test in validation_loader: z = model(x_test.view(-1, 28 * 28)) _, yhat = torch.max(z.data, 1) correct = correct+(yhat == y_test).sum().item() accuracy = correct / y.shape[0] accuracy_list.append(accuracy) . Ungraded lab . 5.4softmax_in_one_dimension_v2.ipynb . Ungraded lab . 6.2lab_predicting _MNIST_using_Softmax_v2.ipynb . # The function to plot parameters def PlotParameters(model): W = model.state_dict()[&#39;linear.weight&#39;].data w_min = W.min().item() w_max = W.max().item() fig, axes = plt.subplots(2, 5) fig.subplots_adjust(hspace=0.01, wspace=0.1) for i, ax in enumerate(axes.flat): if i &lt; 10: # Set the label for the sub-plot. ax.set_xlabel(&quot;class: {0}&quot;.format(i)) # Plot the image. ax.imshow(W[i, :].view(28, 28), vmin=w_min, vmax=w_max, cmap=&#39;seismic&#39;) ax.set_xticks([]) ax.set_yticks([]) # Ensure the plot is shown correctly with multiple plots # in a single Notebook cell. plt.show() # Plot the parameters PlotParameters(model) . . Week 4 - Shallow neural networks . Learning Objectives . Simple Neural Networks | More Hidden Neurons | Neural Networks with Multiple Dimensional | Multi-Class Neural Networks | Backpropagation | Activation Functions | . notebook . notebook . Neural networks in One Dimension . using nn.Module . import torch import torch.nn as nn from torch import sigmoid class Net(nn.Module): def __init__(self, D_in, H, D_out): super(Net, self).__init__() self.linear1 = nn.Linear(D_in, H) self.linear2 = nn.Linear(H, D_out) def forward(self, x): x=sigmoid(self.linear1(x)) x=sigmoid(self.linear2(x)) return x . model = Net(1, 2, 1) x = torch.tensor([0.0]) yhat = model(x) yhat &gt;&gt; tensor([0.5972], grad_fn=&lt;SigmoidBackward&gt;) # multiple samples x = torch.tensor([[0.0], [2.0], [3.0]]) yhat = model(x) yhat &gt;&gt; tensor([[0.5972], [0.5925], [0.5894]], grad_fn=&lt;SigmoidBackward&gt;) # to get a discrete value we apply a threshold yhat = yhat &lt; 0.59 yhat &gt;&gt; tensor([[False], [False], [ True]]) . model.state_dict() &gt;&gt; OrderedDict([(&#39;linear1.weight&#39;, tensor([[0.3820], [0.4019]])), (&#39;linear1.bias&#39;, tensor([-0.7746, -0.3389])), (&#39;linear2.weight&#39;, tensor([[-0.3466, 0.2201]])), (&#39;linear2.bias&#39;, tensor([0.4115]))]) . using nn.Sequential . model = nn.Sequential(nn.Linear(1, 2), nn.Sigmoid(), nn.Linear(2, 1), nn.Sigmoid()) . train the model . we create the data . X = torch.arange(-20, 20, 1).view(-1, 1).type(torch.FloatTensor) Y = torch.zeros(X.shape[0]) Y[(X[:, 0]&gt;-4) &amp; (X[:, 0] &lt;4)] = 1.0 . we create a training function . from tqdm import tqdm def train(Y, X, model, optimizer, criterion, epochs=1000): cost = [] total = 0 for epoch in tqdm(range(epochs)): total = 0 for x, y in zip(X, Y): yhat = model(x) loss = criterion(yhat, y.view(-1)) optimizer.zero_grad() loss.backward() optimizer.step() total+=loss.item() cost.append(total) return cost . and the training process is now . #loss criterion = nn.BCELoss() #data X = torch.arange(-20, 20, 1).view(-1, 1).type(torch.FloatTensor) Y = torch.zeros(X.shape[0]) Y[(X[:, 0]&gt;-4) &amp; (X[:, 0] &lt;4)] = 1.0 #model model = Net(1, 2, 1) #optimizer optimizer = torch.optim.SGD(model.parameters(), lr = 0.01) #train the model cost = train(Y, X, model, optimizer, criterion, epochs=1000) &gt;&gt; 100%|██████████| 1000/1000 [00:12&lt;00:00, 76.96it/s] . Ungraded lab . 7.1_simple1hiddenlayer.ipynb . I like how to display intermediate representations of learning performance: . . # The function for plotting the model def PlotStuff(X, Y, model, epoch, leg=True): plt.plot(X.numpy(), model(X).detach().numpy(), label=(&#39;epoch &#39; + str(epoch))) plt.plot(X.numpy(), Y.numpy(), &#39;r&#39;) plt.xlabel(&#39;x&#39;) if leg == True: plt.legend() else: pass . activation values (called in the training loop). Using model variables (model.a1) which seems a bad practice. . . plt.scatter(model.a1.detach().numpy()[:, 0], model.a1.detach().numpy()[:, 1], c=Y.numpy().reshape(-1)) plt.title(&#39;activations&#39;) plt.show() . and final loss curve . . Neural Networks More Hidden Neurons . using nn.Module . import torch import torch.nn as nn from torch import sigmoid from torch.utils.data import Dataset, DataLoader import matplotlib.pyplot as plt from tqdm import tqdm . class to get our dataset . class Data(Dataset): def __init__(self): self.x = torch.linspace(-20, 20, 100).view(-1, 1) self.y = torch.zeros(self.x.shape[0]) self.y[(self.x[:, 0]&gt;-10) &amp; (self.x[:, 0]&lt;-5)] = 1 self.y[(self.x[:, 0]&gt;5) &amp; (self.x[:, 0]&lt;10)] = 1 self.y = self.y.view(-1, 1) self.len = self.x.shape[0] def __getitem__(self, index): return self.x[index], self.y[index] def __len__(self): return self.len . class for creating our model . class Net(nn.Module): def __init__(self, D_in, H, D_out): super(Net, self).__init__() self.linear1 = nn.Linear(D_in, H) self.linear2 = nn.Linear(H, D_out) def forward(self, x): x=sigmoid(self.linear1(x)) x=sigmoid(self.linear2(x)) return x . and the function to train our model . # The function for plotting the model def PlotStuff(X, Y, model): plt.plot(X.numpy(), model(X).detach().numpy()) plt.plot(X.numpy(), Y.numpy(), &#39;r&#39;) plt.xlabel(&#39;x&#39;) def train(data_set, model, criterion, train_loader, optimizer, epochs=5): cost = [] total=0 for epoch in tqdm(range(epochs)): total=0 for x, y in train_loader: optimizer.zero_grad() yhat = model(x) loss = criterion(yhat, y) loss.backward() optimizer.step() total+=loss.item() PlotStuff(data_set.x, data_set.y, model) cost.append(total) return cost . process for training is identical to logistic regression . criterion = nn.BCELoss() data_set = Data() train_loader = DataLoader(dataset=data_set, batch_size=100) model = Net(1, 6, 1) optimizer = torch.optim.Adam(model.parameters(), lr = 0.001) train(data_set, model, criterion, train_loader, optimizer, epochs=1000) . using nn.Sequential . model = nn.Sequential( nn.Linear(1, 7), nn.Sigmoid(), nn.Linear(7, 1), nn.Sigmoid() ) . Ungraded lab . 7.2multiple_neurons.ipynb . Neural Networks with Multiple Dimensional Input . implementation . import torch import torch.nn as nn from torch import sigmoid from torch.utils.data import Dataset, DataLoader from tqdm import tqdm import numpy as np . we create a dataset class . class XOR_Data(Dataset): def __init__(self, N_s=100): self.x = torch.zeros((N_s, 2)) self.y = torch.zeros((N_s, 1)) for i in range(N_s // 4): self.x[i, :] = torch.Tensor([0.0, 0.0]) self.y[i, 0] = torch.Tensor([0.0]) self.x[i + N_s // 4, :] = torch.Tensor([0.0, 1.0]) self.y[i + N_s // 4, 0] = torch.Tensor([1.0]) self.x[i + N_s // 2, :] = torch.Tensor([1.0, 0.0]) self.y[i + N_s // 2, 0] = torch.Tensor([1.0]) self.x[i + 3 * N_s // 4, :] = torch.Tensor([1.0, 1.0]) self.y[i + 3 * N_s // 4, 0] = torch.Tensor([0.0]) self.x = self.x + 0.01 * torch.randn((N_s, 2)) self.len = N_s def __getitem__(self, index): return self.x[index], self.y[index] def __len__(self): return self.len # Plot the data def plot_stuff(self): plt.plot(self.x[self.y[:, 0] == 0, 0].numpy(), self.x[self.y[:, 0] == 0, 1].numpy(), &#39;o&#39;, label=&quot;y=0&quot;) plt.plot(self.x[self.y[:, 0] == 1, 0].numpy(), self.x[self.y[:, 0] == 1, 1].numpy(), &#39;ro&#39;, label=&quot;y=1&quot;) plt.legend() . . We create a class for creating our model . class Net(nn.Module): def __init__(self, D_in, H, D_out): super(Net, self).__init__() self.linear1 = nn.Linear(D_in, H) self.linear2 = nn.Linear(H, D_out) def forward(self, x): x=sigmoid(self.linear1(x)) x=sigmoid(self.linear2(x)) return x . We create a function to train our model . # Calculate the accuracy def accuracy(model, data_set): return np.mean(data_set.y.view(-1).numpy() == (model(data_set.x)[:, 0] &gt; 0.5).numpy()) def train(data_set, model, criterion, train_loader, optimizer, epochs=5): COST = [] ACC = [] for epoch in tqdm(range(epochs)): total=0 for x, y in train_loader: optimizer.zero_grad() yhat = model(x) loss = criterion(yhat, y) optimizer.zero_grad() loss.backward() optimizer.step() #cumulative loss total+=loss.item() ACC.append(accuracy(model, data_set)) COST.append(total) return COST . process for training is identical to logistic regression . criterion = nn.BCELoss() data_set = XOR_Data() train_loader = DataLoader(dataset=data_set, batch_size=1) model = Net(2, 4, 1) optimizer = torch.optim.SGD(model.parameters(), lr = 0.01) train(data_set, model, criterion, train_loader, optimizer, epochs=500) . overfitting and underfitting . Solution: . use validation data to determine optimum number of neurons | get more data | regularization: for example dropout | . Ungraded lab . 7.3xor_v2.ipynb . Multi-Class Neural Networks . using nn.Module . we don’t have sigmoid for the output, and D_out is our number of classes . class Net(nn.Module): def __init__(self, D_in, H, D_out): super(Net, self).__init__() self.linear1 = nn.Linear(D_in, H) self.linear2 = nn.Linear(H, D_out) def forward(self, x): x=sigmoid(self.linear1(x)) x=(self.linear2(x)) return x . using nn.Sequential . input_dim = 2 hidden_dim = 6 output_dim = 3 model = nn.Sequential( nn.Linear(input_dim, hidden_dim), nn.Sigmoid(), nn.Linear(hidden_dim, output_dim) ) . training . we create a validation and training dataset . import torchvision.datasets as dsets import torchvision.transforms as transforms train_dataset = dsets.MNIST(root=&#39;./data&#39;, train = True, download = True, transform=transforms.ToTensor()) validation_dataset = dsets.MNIST(root=&#39;./data&#39;, train = False, download = True, transform=transforms.ToTensor()) . we create a validation and training loader . train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000) validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=2000) . criterion = nn.CrossEntropyLoss() . we create the training function . from tqdm import tqdm def train(model, criterion, train_loader, validation_loader, optimizer, epochs=100): i = 0 useful_stuff = {&#39;training_loss&#39;: [],&#39;validation_accuracy&#39;: []} for epoch in tqdm(range(epochs)): for i, (x, y) in enumerate(train_loader): optimizer.zero_grad() z = model(x.view(-1, 28 * 28)) loss = criterion(z, y) loss.backward() optimizer.step() #loss for every iteration useful_stuff[&#39;training_loss&#39;].append(loss.data.item()) correct = 0 for x, y in validation_loader: #validation z = model(x.view(-1, 28 * 28)) _, label = torch.max(z, 1) correct += (label == y).sum().item() accuracy = 100 * (correct / len(validation_dataset)) useful_stuff[&#39;validation_accuracy&#39;].append(accuracy) return useful_stuff . We instantiate and train the model . input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 model = Net(input_dim, hidden_dim, output_dim) training_results = train(model, criterion, train_loader, validation_loader, optimizer, epochs=30) . To plot accuracy and lost . # Define a function to plot accuracy and loss def plot_accuracy_loss(training_results): plt.subplot(2, 1, 1) plt.plot(training_results[&#39;training_loss&#39;], &#39;r&#39;) plt.ylabel(&#39;loss&#39;) plt.title(&#39;training loss iterations&#39;) plt.subplot(2, 1, 2) plt.plot(training_results[&#39;validation_accuracy&#39;]) plt.ylabel(&#39;accuracy&#39;) plt.xlabel(&#39;epochs&#39;) plt.show() plot_accuracy_loss(training_results) . . To plot improper classified items . count = 0 for x, y in validation_dataset: z = model(x.reshape(-1, 28 * 28)) _,yhat = torch.max(z, 1) if yhat != y: show_data(x) count += 1 if count &gt;= 5: break . . Ungraded lab . 7.4one_layer_neural_network_MNIST.ipynb . Backpropagation . Following the chain rule in gradient calculation, it happens that gradient results are getting closer and closer to 0. (i.e. vanishing gradient) therefore we cannot improve model parameters. . One way to deal with that is to change activation function. . Activation functions . sigmoid, tanh, relu . . sigmoid, tanh, relu in PyTorch . class Net_sigmoid(nn.Module): def __init__(self, D_in, H, D_out): super(Net, self).__init__() self.linear1 = nn.Linear(D_in, H) self.linear2 = nn.Linear(H, D_out) def forward(self, x): x=sigmoid(self.linear1(x)) x=(self.linear2(x)) return x . class Net_tanh(nn.Module): def __init__(self, D_in, H, D_out): super(Net, self).__init__() self.linear1 = nn.Linear(D_in, H) self.linear2 = nn.Linear(H, D_out) def forward(self, x): x=torch.tanh(self.linear1(x)) x=(self.linear2(x)) return x . class Net_relu(nn.Module): def __init__(self, D_in, H, D_out): super(Net, self).__init__() self.linear1 = nn.Linear(D_in, H) self.linear2 = nn.Linear(H, D_out) def forward(self, x): x=torch.relu(self.linear1(x)) x=(self.linear2(x)) return x . using nn.Sequential . model_tanh = nn.Sequential( nn.Linear(input_dim, hidden_dim), nn.Tanh(), nn.Linear(hidden_dim, output_dim) ) model_relu = nn.Sequential( nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, output_dim) ) . Ungraded lab . 7.5.1activationfuction_v2.ipynb . Ungraded lab . 7.5.2mist1layer_v2.ipynb . to monitor gpu usage: nvidia-smi -l 1 . . Week 5 - Deep neural networks . Learning Objectives . building deep networks | Dropout | Neural Network initialization weights | Gradient Descent with Momentum | . notebook . notebook . Deep Neural Networks . Deep, following this course definition, is when number of hidden layers &gt; 1. . using nn.Module . import torch import torch.nn as nn from torch import sigmoid . class Net(nn.Module): def __init__(self, D_in, H1, H2, D_out): super(Net, self).__init__() self.linear1 = nn.Linear(D_in, H1) self.linear2 = nn.Linear(H1, H2) self.linear3 = nn.Linear(H2, D_out) def forward(self, x): x=sigmoid(self.linear1(x)) x=sigmoid(self.linear2(x)) x=self.linear3(x) return x . using nn.Sequential . input_dim = 2 hidden_dim1 = 6 hidden_dim2 = 4 output_dim = 3 model = nn.Sequential( nn.Linear(input_dim, hidden_dim1), nn.Sigmoid(), nn.Linear(hidden_dim1, hidden_dim2), nn.Sigmoid(), nn.Linear(hidden_dim2, output_dim) ) . training . there is no change compare to other networks . we create a validation and training dataset . import torchvision.datasets as dsets import torchvision.transforms as transforms train_dataset = dsets.MNIST(root=&#39;./data&#39;, train = True, download = True, transform=transforms.ToTensor()) validation_dataset = dsets.MNIST(root=&#39;./data&#39;, train = False, download = True, transform=transforms.ToTensor()) . we create a validation and training loader . train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000) validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=2000) . criterion = nn.CrossEntropyLoss() . we create the training function . from tqdm import tqdm def train(model, criterion, train_loader, validation_loader, optimizer, epochs=100): i = 0 useful_stuff = {&#39;training_loss&#39;: [],&#39;validation_accuracy&#39;: []} for epoch in tqdm(range(epochs)): for i, (x, y) in enumerate(train_loader): optimizer.zero_grad() z = model(x.view(-1, 28 * 28)) loss = criterion(z, y) loss.backward() optimizer.step() #loss for every iteration useful_stuff[&#39;training_loss&#39;].append(loss.data.item()) correct = 0 for x, y in validation_loader: #validation z = model(x.view(-1, 28 * 28)) _, label = torch.max(z, 1) correct += (label == y).sum().item() accuracy = 100 * (correct / len(validation_dataset)) useful_stuff[&#39;validation_accuracy&#39;].append(accuracy) return useful_stuff . We instantiate and train the model . input_dim = 28 * 28 hidden_dim1 = 50 hidden_dim2 = 50 output_dim = 10 model = Net(input_dim, hidden_dim1, hidden_dim2, output_dim) optimizer = torch.optim.SGD(model.parameters(), lr=0.01) training_results = train(model, criterion, train_loader, validation_loader, optimizer, epochs=30) . Ungraded lab - deep neural networks . 8.1.1mist2layer_v2.ipynb . . Deep Neural Networks : nn.ModuleList() . jdc . this is a nice library to allow breaking down definition of classes in separate notebook cells . Installation is as simple as pip install jdc . and usage is . import jdc . and start a cell with %%add_to &lt;your class name&gt; . python implementation . import torch import torch.nn as nn from torch import sigmoid import jdc . class Net(nn.Module): def __init__(self, Layers): super(Net, self).__init__() self.hidden = nn.ModuleList() for input_size, output_size in zip(Layers, Layers[1:]): self.hidden.append(nn.Linear(input_size, output_size)) . Layers = [2, 3, 4, 3] model = Net(Layers) . %%add_to Net def forward(self, x): L = len(self.hidden) for (l, linear_transform) in zip(range(L), self.hidden): if (l &lt; L-1): x = torch.relu(linear_transform(x)) else: x = linear_transform(x) return x . Ungraded lab - nn.ModuleList() . 8.1.2mulitclassspiralrulu_v2.ipynb . . Dropout . using nn.Module . class Net(nn.Module): def __init__(self, in_size, n_hidden, out_size, p=0): super(Net, self).__init__() self.drop = nn.Dropout(p=p) self.linear1 = nn.Linear(in_size, n_hidden) self.linear2 = nn.Linear(n_hidden, n_hidden) self.linear3 = nn.Linear(n_hidden, out_size) def forward(self, x): x=torch.relu(self.linear1(x)) x=self.drop(x) x=torch.relu(self.linear2(x)) x=self.drop(x) x=self.linear3(x) return x . using nn.Sequential . model = nn.Sequential( nn.Linear(1, 10), nn.Dropout(0.5), nn.ReLU(), nn.Linear(10, 12), nn.Dropout(0.5), nn.ReLU(), nn.Linear(12, 1), ) . training . create data . from torch.utils.data import Dataset, DataLoader import numpy as np # Create data class for creating dataset object class Data(Dataset): # Constructor def __init__(self, N_SAMPLES=1000, noise_std=0.15, train=True): a = np.matrix([-1, 1, 2, 1, 1, -3, 1]).T self.x = np.matrix(np.random.rand(N_SAMPLES, 2)) self.f = np.array(a[0] + (self.x) * a[1:3] + np.multiply(self.x[:, 0], self.x[:, 1]) * a[4] + np.multiply(self.x, self.x) * a[5:7]).flatten() self.a = a self.y = np.zeros(N_SAMPLES) self.y[self.f &gt; 0] = 1 self.y = torch.from_numpy(self.y).type(torch.LongTensor) self.x = torch.from_numpy(self.x).type(torch.FloatTensor) self.x = self.x + noise_std * torch.randn(self.x.size()) self.f = torch.from_numpy(self.f) self.a = a if train == True: torch.manual_seed(1) self.x = self.x + noise_std * torch.randn(self.x.size()) torch.manual_seed(0) # Getter def __getitem__(self, index): return self.x[index], self.y[index] # Get Length def __len__(self): return self.len # Plot the diagram def plot(self): X = data_set.x.numpy() y = data_set.y.numpy() h = .02 x_min, x_max = X[:, 0].min(), X[:, 0].max() y_min, y_max = X[:, 1].min(), X[:, 1].max() xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) Z = data_set.multi_dim_poly(np.c_[xx.ravel(), yy.ravel()]).flatten() f = np.zeros(Z.shape) f[Z &gt; 0] = 1 f = f.reshape(xx.shape) plt.title(&#39;True decision boundary and sample points with noise &#39;) plt.plot(self.x[self.y == 0, 0].numpy(), self.x[self.y == 0,1].numpy(), &#39;bo&#39;, label=&#39;y=0&#39;) plt.plot(self.x[self.y == 1, 0].numpy(), self.x[self.y == 1,1].numpy(), &#39;ro&#39;, label=&#39;y=1&#39;) plt.contour(xx, yy, f,cmap=plt.cm.Paired) plt.xlim(0,1) plt.ylim(0,1) plt.legend() # Make a multidimension ploynomial function def multi_dim_poly(self, x): x = np.matrix(x) out = np.array(self.a[0] + (x) * self.a[1:3] + np.multiply(x[:, 0], x[:, 1]) * self.a[4] + np.multiply(x, x) * self.a[5:7]) out = np.array(out) return out . . instantiate the model . model_drop = Net(2, 300, 2, p=0.5) . train method tells the model we are in the training phase which will implement the dropout method, later we use the eval method to tell the model it is in the evaluation phase and that will turn off the dropout method . model_drop.train() optimizer = torch.optim.Adam(model_drop.parameters(), lr = 0.01) criterion = nn.CrossEntropyLoss() data_set = Data() validation_set = Data(train=False) . # Initialize the LOSS dictionary to store the loss LOSS = {} LOSS[&#39;training data dropout&#39;] = [] LOSS[&#39;validation data dropout&#39;] = [] . train the model . # Train the model from tqdm import tqdm epochs = 500 def train_model(epochs): for epoch in tqdm(range(epochs)): #all the samples are used for training yhat_drop = model_drop(data_set.x) loss_drop = criterion(yhat_drop, data_set.y) #store the loss for both the training and validation data for both models LOSS[&#39;training data dropout&#39;].append(loss_drop.item()) model_drop.eval() LOSS[&#39;validation data dropout&#39;].append(criterion(model_drop(validation_set.x), validation_set.y).item()) model_drop.train() optimizer.zero_grad() loss_drop.backward() optimizer.step() train_model(epochs) . # The function for calculating accuracy def accuracy(model, data_set): _, yhat = torch.max(model(data_set.x), 1) return (yhat == data_set.y).numpy().mean() # Print out the accuracy of the model with dropout print(&quot;The accuracy of the model with dropout: &quot;, accuracy(model_drop, validation_set)) &gt;&gt; The accuracy of the model with dropout: 0.866 . Ungraded lab - dropout classification . 8.2.1dropoutPredictin_v2.ipynb . Ungraded lab - dropout regression . 8.2.2dropoutRegression_v2.ipynb . Neural Network initialization weights . Different methods exist: . uniform distribution for parameters: we simply make the lower bound of the range of the distribution the negative of the inverse of square root of L in. the upper bound of the range of the distribution is the positive of the inverse of square root of L in. See this paper for more details. LeCun, Yann A., et al. “Efficient backprop.” Neural networks: Tricks of the trade. Springer, Berlin, Heidelberg, 2012. 9-48 | . linear=nn.Linear(input_size,output_size) linear.weight.data.uniform_(0, 1) . xavier method: Xavier Initialization is another popular method and is used in conjunction with the tanh activation. It takes into consideration the number of input neurons “Lin” as well as the number of neuron in the next layer “L out”. This paper for more details: Glorot, Xavier and Yoshua Bengio. “Understanding the difficulty of training deep feedforward neural networks” 2010. | . linear=nn.Linear(input_size,output_size) torch.nn.init.xavier_uniform_(linear.weight) . He method: For relu we use the He initialize method, after we create a linear object, We use the following method to initialize the weights, for more info check out the following paper. He, Kaiming, et al. “Delving deep into rectifiers: surpassing human-level performance in imagenet classification” | . linear = nn.Linear(input_size, output_size) torch.nn.init.kaiming_uniform_(linear.weight, nonlinearity=&#39;relu&#39;) . Ungraded lab - initialization . 8.3.1.initializationsame.ipynb . Ungraded lab - Xavier initialization . 8.3.2Xaviermist1layer_v2.ipynb . Ungraded lab - He initialization . 8.3.3.He_Initialization_v2.ipynb . Gradient Descent with Momentum . PyTorch implementation . In PyTorch, this is just defined at optim level . optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum = 0.4) . Ungraded lab - momentum with different polynomial . 8.4.1_MomentumwithPolynomialFunctions_v2.ipynb . Ungraded lab - Neural Network momentum . 8.4.2_NeuralNetworkswithMomentum_v2.ipynb . Batch Normalization . . 𝛾, 𝛽 parameters are are actually scale and shift parameters, which we’re going to learn via training. . using nn.Module . class Net_BatchNorm(nn.Module): def __init__(self, in_size, n_hidden1, n_hidden2, out_size): super(Net_BatchNorm, self).__init__() self.linear1 = nn.Linear(in_size, n_hidden1) self.linear2 = nn.Linear(n_hidden1, n_hidden2) self.linear3 = nn.Linear(n_hidden2, out_size) self.bn1 = nn.BatchNorm1d(n_hidden1) self.bn2 = nn.BatchNorm1d(n_hidden2) def forward(self, x): x=torch.sigmoid(self.bn1(self.linear1(x))) x=torch.sigmoid(self.bn2(self.linear2(x))) x=self.linear3(x) return x . Ungraded lab - Batch normalization . 8.5.1BachNorm_v2.ipynb . comparing training loss for each iteration and accuracy on validation data for both Batch / No Batch normalization. . . . Week 6 - Convolutional neural networks . Learning Objectives . Convolution | Activation Functions | Max Pooling | Convolution: Multiple Channels | Convolutional Neural Network | TORCH-VISION MODELS | . notebook . notebook . Convolution . convolution explanation from stanford course CS231 . convolution . conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3) . image = torch.zeros(1,1,5,5) image[0,0,:,2] = 1 image &gt;&gt; tensor([[[[0., 0., 1., 0., 0.], [0., 0., 1., 0., 0.], [0., 0., 1., 0., 0.], [0., 0., 1., 0., 0.], [0., 0., 1., 0., 0.]]]]) z=conv(image) &gt;&gt; tensor([[[[ 0.6065, 0.0728, -0.7915], [ 0.6065, 0.0728, -0.7915], [ 0.6065, 0.0728, -0.7915]]]], grad_fn=&lt;ThnnConv2DBackward&gt;) conv.state_dict() &gt;&gt; OrderedDict([(&#39;weight&#39;, tensor([[[[ 0.1132, -0.0418, 0.3140], [-0.2261, -0.1528, -0.3270], [-0.2140, -0.1900, 0.2127]]]])), (&#39;bias&#39;, tensor([0.0423]))]) . stride . conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride = 2) . zeros padding . conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride = 2, padding = 1) . size of activation map . Feature size = ((Image size + 2 * Padding size − Kernel size) / Stride)+1 . Ungraded lab - What’s convolution . 9.1What_is_Convolution.ipynb . Activation Functions and Max Polling . Activation function using nn.Module . import torch image = torch.zeros(1,1,5,5) image[0,0,:,2] = 1 conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3) z=conv(image) A = torch.relu(z) . Activation function using nn.Sequential . relu = nn.ReLU() A = relu(z) . Max pooling . max = nn.MaxPool2d(2, stride=1) max(image) . torch.max_pool2d(image, stride=1, kernel_size=2) . Ungraded lab - Activation Functions and Max Polling . 9.2Activation_max_pooling.ipynb . Multiple Input and Output Channels . Ungraded lab - Activation Functions and Max Polling . 9.3Multiple Channel Convolution.ipynb . Convolutional Neural Network . using nn.Module . class CNN(nn.Module): def __init__(self,out_1=2,out_2=1): super(CNN,self).__init__() #first Convolutional layers self.cnn1=nn.Conv2d(in_channels=1,out_channels=out_1,kernel_size=2,padding=0) self.maxpool1=nn.MaxPool2d(kernel_size=2 ,stride=1) #second Convolutional layers self.cnn2=nn.Conv2d(in_channels=out_1,out_channels=out_2,kernel_size=2,stride=1,padding=0) self.maxpool2=nn.MaxPool2d(kernel_size=2 ,stride=1) #max pooling #fully connected layer self.fc1=nn.Linear(out_2*7*7,2) def forward(self,x): #first Convolutional layers x=self.cnn1(x) #activation function x=torch.relu(x) #max pooling x=self.maxpool1(x) #first Convolutional layers x=self.cnn2(x) #activation function x=torch.relu(x) #max pooling x=self.maxpool2(x) #flatten output x=x.view(x.size(0),-1) #fully connected layer x=self.fc1(x) return x . training . n_epochs=10 cost_list=[] accuracy_list=[] N_test=len(validation_dataset) cost=0 #n_epochs for epoch in range(n_epochs): cost=0 for x, y in train_loader: #clear gradient optimizer.zero_grad() #make a prediction z=model(x) # calculate loss loss=criterion(z,y) # calculate gradients of parameters loss.backward() # update parameters optimizer.step() cost+=loss.item() cost_list.append(cost) correct=0 #perform a prediction on the validation data for x_test, y_test in validation_loader: z=model(x_test) _,yhat=torch.max(z.data,1) correct+=(yhat==y_test).sum().item() accuracy=correct/N_test accuracy_list.append(accuracy) . Ungraded lab - Convolutional Neural Network Simple example . 9.4.1ConvolutionalNeralNetworkSimple example.ipynb . def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1): #by Duane Nielsen from math import floor if type(kernel_size) is not tuple: kernel_size = (kernel_size, kernel_size) h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1) w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1) return h, w . out=conv_output_shape((11,11), kernel_size=2, stride=1, pad=0, dilation=1) print(out) out1=conv_output_shape(out, kernel_size=2, stride=1, pad=0, dilation=1) print(out1) out2=conv_output_shape(out1, kernel_size=2, stride=1, pad=0, dilation=1) print(out2) out3=conv_output_shape(out2, kernel_size=2, stride=1, pad=0, dilation=1) print(out3) &gt;&gt; (10, 10) (9, 9) (8, 8) (7, 7) . Ungraded lab - Convolutional Neural Network MNIST . 9.4.2CNN_Small_Image.ipynbb . Ungraded lab - Convolutional Neural Networks with Batch Norm . 9.4.3CNN_Small_Image_batch.ipynb . GPU in PyTorch . torch.cuda.is_available() &gt;&gt; True device = torch.device(&#39;cuda:0&#39;) torch.tensor([1,2,32,4]).to(device) &gt;&gt; tensor([ 1, 2, 32, 4], device=&#39;cuda:0&#39;) model = CNN() model.to(device) . Training on GPU . for epoch in range(num_epochs): for features, labels in train_loader: features, labels = features.to(device), labels.to(device) optimizer.zero_grad() predictions = model(features) loss = criterion(predictions, labels) loss.backward() optimizer.step() . TORCH-VISION MODELS . load resnet18 with pretrained parameters . import torch import torchvision.models as models from torch.utils.data import Dataset, DataLoader from torchvision import transforms import torch.nn as nn torch.manual_seed(0) model = models.resnet18(pretrained=True) mean = [0.485, 0.456, 0.406] std = [0.229, 0.224, 0.225] composed = transforms.Compose([transforms.Resize(224), transforms.ToTensor(), transforms.Normalize(mean, std)]) train_dataset = Dataset(transform=composed, train = True) validation_dataset = Dataset(transform=composed) . freeze parameters and add a final layer to be trained . for param in model.parameters(): param.requires_grad=False model.fc = nn.Linear(512, 7) . train_loader = DataLoader(dataset=train_loader, batch_size=15) validation_loader = DataLoader(dataset=validation_loader, batch_size=10) . provides only parameters to be trained to optim . criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam([parameters for parameters in model.parameters() if parameters.requires_grad], lr = 0.003) N_EPOCHS = 20 loss_list = [] accuracy_list = [] correct = 0 n_test = len(validation_dataset) . train the model, switching to model.train and model.eval . for epoch in range(N_EPOCHS): loss_sublist = [] for x, y in train_loader: model.train() optimizer.zero_grad() z = model(x) loss = criterion(z, y) loss_sublist.append(loss.data.item()) loss.backward() optimizer.step() loss_list.append(np.mean(loss_sublist)) correct = 0 for x_test, y_test in validation_loader: model.eval() z = model(x_test) _, yhat = torch.max(z.data, 1) correct += (yhat == y_test).sum().item() accuracy = correct / n_test accuracy_list.append(accuracy) . Week 7 - Fashion MNIST . Learning Objectives . Apply all you have learned to train a Convolutional Neural Network | . notebook . notebook .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/deep-neural-network-coursera.html",
            "relUrl": "/blog/deep-neural-network-coursera.html",
            "date": " • Jul 7, 2021"
        }
        
    
  
    
        ,"post18": {
            "title": "Logbook for July 21",
            "content": "Week 26 - July 21 . Thursday 7/1 . Paper reviewed on arxiv about revisiting Deep Learning Models for Tabular Data. arXiv:2106.11959v1 (FT-Transformer, ResNet, MLP) architectures can compete with GBDT models. Examples are not detailed in paper but code seems available at https://github.com/yandex-research/rtdl (rtdl (Revisiting Tabular Deep Learning) based on PyTorch) . RL Course by David Silver Exploration and Exploitation (lecture 9) . Week 27 - July 21 . Wednesday 7/7 . Start of Deep Neural Networks with PyTorch on Coursera by IBM. . Friday 7/9 . Deep Neural Networks with PyTorch - week 2 - linear regression . Week 28 - July 21 . Monday 7/12 . Deep Neural Networks with PyTorch - week 3 - multiple input output linear regression, logistic regression for classification . Deep Neural Networks with PyTorch - week 4 - softmax regression . Thursday 7/15 . Deep Neural Networks with PyTorch - week 4 - shallow neural networks . Deep Neural Networks with PyTorch - week 5 - deep neural networks . Friday 7/16 . Deep Learning with PyTorch book . . PyTorch Ecosystem day 2021: . lightning to remove boilerplate code - to be tested! | torchstudio to visualize dataset and models | pytorch-ignite from my colleague Sylvain - high-level library to help training evaluating NN | pytorchts to play with timeseries | multitask RL environments and baselines for RL | rlstructures python library for RL research | mbrl-lib to write model-based RL algorithms | pystiche framework for style transfer | . Deep Neural Networks with PyTorch - week 6 - convolutional neural networks - even if I don’t need it . Week 30 - July 21 . Monday 7/26 . Deep Neural Networks with PyTorch - week 7 - fashion mnist . Reading of some papers: . Deep Reinforcement Learning Approaches for Process Control by Steven Spielberg Pon Kumar - thesis, UBC 2017 | A Deep Learning Architecture for Predictive Control by Steven Spielberg Pon Kumar - ScienceDirect 2018 | Deep Reinforcement Learning for Process Control: A Primer for Beginners by Steven Spielberg et al. 2004 | Reinforcement Learning for Statistical Process Control in Manufacturing - by Zsolt János Viharos 2020 | .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/logbook-July.html",
            "relUrl": "/blog/logbook-July.html",
            "date": " • Jul 1, 2021"
        }
        
    
  
    
        ,"post19": {
            "title": "Autogenerate documentation from custom python classes",
            "content": "Installation . !conda env list . # conda environments: # base /home/explore/miniconda3 aniti /home/explore/miniconda3/envs/aniti audioclass /home/explore/miniconda3/envs/audioclass coursera_rl /home/explore/miniconda3/envs/coursera_rl d059 * /home/explore/miniconda3/envs/d059 datacamp /home/explore/miniconda3/envs/datacamp deeplearning_specialization /home/explore/miniconda3/envs/deeplearning_specialization deeplearning_specialization_keras /home/explore/miniconda3/envs/deeplearning_specialization_keras deeplearning_specialization_tf1 /home/explore/miniconda3/envs/deeplearning_specialization_tf1 drl_handson /home/explore/miniconda3/envs/drl_handson drl_simonini /home/explore/miniconda3/envs/drl_simonini fastai /home/explore/miniconda3/envs/fastai fastaudio /home/explore/miniconda3/envs/fastaudio gan /home/explore/miniconda3/envs/gan gan_tensorflow /home/explore/miniconda3/envs/gan_tensorflow macos /home/explore/miniconda3/envs/macos minecraft /home/explore/miniconda3/envs/minecraft mit_6002x /home/explore/miniconda3/envs/mit_6002x mit_6S191 /home/explore/miniconda3/envs/mit_6S191 pycaret /home/explore/miniconda3/envs/pycaret pytorch /home/explore/miniconda3/envs/pytorch scikit-learn-course /home/explore/miniconda3/envs/scikit-learn-course shap /home/explore/miniconda3/envs/shap squeezebox /home/explore/miniconda3/envs/squeezebox stablebaselines3 /home/explore/miniconda3/envs/stablebaselines3 zoe /home/explore/miniconda3/envs/zoe . import sys !conda install --yes --prefix {sys.prefix} pdoc3 . Collecting package metadata (current_repodata.json): done Solving environment: / The environment is inconsistent, please check the package plan carefully The following packages are causing the inconsistency: - defaults/linux-64::matplotlib==3.3.2=0 - defaults/linux-64::matplotlib-base==3.3.2=py38h817c723_0 - defaults/linux-64::spacy==2.3.2=py38hfd86e86_0 - pytorch/linux-64::torchvision==0.8.1=py38_cu110 - defaults/linux-64::numpy==1.19.2=py38h54aff64_0 - defaults/linux-64::thinc==7.4.1=py38hfd86e86_0 - defaults/linux-64::cython-blis==0.4.1=py38h7b6447c_1 - fastai/noarch::fastprogress==1.0.0=pyh39e3cac_0 - fastai/noarch::fastai==2.1.4=py_0 - defaults/linux-64::mkl_random==1.1.1=py38h0573a6f_0 - pytorch/linux-64::pytorch==1.7.0=py3.8_cuda11.0.221_cudnn8.0.3_0 - defaults/linux-64::pandas==1.1.3=py38he6710b0_0 - fastai/noarch::fastscript==1.0.0=0 - defaults/linux-64::scipy==1.5.2=py38h0b6359f_0 - defaults/linux-64::mkl_fft==1.2.0=py38h23d657b_0 - fastai/noarch::fastbook==0.0.14=py_0 - defaults/linux-64::scikit-learn==0.23.2=py38h0573a6f_0 done ## Package Plan ## environment location: /home/explore/miniconda3/envs/d059 added / updated specs: - pdoc3 The following NEW packages will be INSTALLED: mako pkgs/main/noarch::mako-1.1.4-pyhd3eb1b0_0 markdown pkgs/main/linux-64::markdown-3.3.4-py38h06a4308_0 pdoc3 pkgs/main/noarch::pdoc3-0.9.2-pyhd3eb1b0_0 The following packages will be UPDATED: ca-certificates 2020.10.14-0 --&gt; 2021.5.25-h06a4308_1 certifi pkgs/main/noarch::certifi-2020.6.20-p~ --&gt; pkgs/main/linux-64::certifi-2021.5.30-py38h06a4308_0 mkl_fft 1.2.0-py38h23d657b_0 --&gt; 1.3.0-py38h54f3939_0 openssl 1.1.1h-h7b6447c_0 --&gt; 1.1.1k-h27cfd23_0 scipy 1.5.2-py38h0b6359f_0 --&gt; 1.6.2-py38h91f5cce_0 Preparing transaction: done Verifying transaction: done Executing transaction: done . Generate documentation from .py files . !pdoc --html --output-dir /home/explore/git/guillaume/d059/exp/html /home/explore/git/guillaume/d059/exp/init_D059.py . /home/explore/git/guillaume/d059/exp/html/init_D059.html . This is the way to generate doc: . pdoc --html --output-dir exp/html exp/my_classe.py . force refresh of doc . in case of existing html file . pdoc --html --output-dir exp/html --force exp/my_classe.py . Example . Proper docstring to get nice output . Here is an example that gives good result: . &quot;&quot;&quot; Un wrapper de lecture de contenu. En fonction de la nature du fichier (extension), appelle le bon lecteur. Les extensions supportées sont : - .csv : pour appel à pd.read_csv. (Les colonnes Unnamed seront supprimées) - .xls, .xlsx : pour appel à pd.read_excel - .accdb : pour appel à pd.read_sql (disponible uniquement sous Windows) Prend en parametre le nom complet du fichier csv, avec son extension. Et les options à passer. Si un nom de colonne contient Date, le type datetime64 est appliqué. Renvoie le dataframe correspondant. Examples -- &gt; &gt;&gt; getRawContent(root_data+&#39;Stam-CC/MCCSC 25625.csv&#39;, sep=&#39;;&#39;, decimal=&#39;,&#39;, dayfirst=True) &gt;&gt;&gt; getRawContent(root_data+&#39;Stam-CC/ExportData 25625.xlsx&#39;, sheet_name=&#39;ExportData 25625 MCCS&#39;) &gt;&gt;&gt; getRawContent(root_data + &#39;/accessDB/Datos 19_12.accdb&#39;, tablename=&#39;Datos_GR02_25625&#39;) Parameters - filename :string Emplacement du fichier. Format complet avec l&#39;extension Ex: root_data+&#39;Stam-CC/MCCSC 25625.csv&#39; options : **keyword args, optional Arguments valides dans l&#39;appel à pandas.read_csv, ou pandas.read_excel, ou pandas_read_sql : https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html Ex: sep=&#39;;&#39;, decimal=&#39;,&#39;, dayfirst=True Ex: **{&#39;sep&#39;:&#39;;&#39;, &#39;decimal&#39;:&#39;,&#39;, &#39;dayfirst&#39;:True} Returns - dataframe Dataframe correspondant au filename avec les options de lecture associées. &quot;&quot;&quot; . that results as . .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/documentation-python.html",
            "relUrl": "/blog/documentation-python.html",
            "date": " • Jun 30, 2021"
        }
        
    
  
    
        ,"post20": {
            "title": "Slideshows from Jupyter notebook",
            "content": "Installation, configuration . nice explanation from Mark Roekpe&#39;s blog . Slide is a native function from Jupyter (through nbconvert). However one can install RISE to get some additional features. . conda install -c conda-forge rise pip install RISE . . And then for each cell I can decide if I want: . slide - indicates that the selected cell should be the start of a new slide | sub-slide - indicates that the selected cell should be the start of a new sub-slide, which appears in a new frame beneath the previous slide | fragment - indicates that the selected cell should appear as a build to the previous slide | skip - indicates that the selected cell should be skipped and not be a part of the slideshow | notes - indicates that the selected cell should just be presenter notes | - - indicates that the selected cell should follow the behavior of the previous cell, which is useful when a markdown cell and a code cell should appear simultaneously | . Slides are structured this way. To progress we can go right (for new slides) or bottom (for sub-slides). . And we can decide to have new pages (slides / sub-slides) or new fragment in the same page (fragment). . Run slides . We can launch slideshow with RISE button (or Alt-R) . Or we can serve pages through nbconvert: . jupyter nbconvert my_notebook.ipynb --to slides --post serve . !jupyter nbconvert 2021-06-25-slide-show-jupyter.ipynb --to slides --post serve . [NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`. [NbConvertApp] Converting notebook 2021-06-25-slide-show-jupyter.ipynb to slides Serving your slides at http://127.0.0.1:8000/2021-06-25-slide-show-jupyter.slides.html Use Control-C to stop this server WARNING:tornado.access:404 GET /favicon.ico (127.0.0.1) 0.41ms ^C Interrupted . RISE vs nbconvert . RISE nbconvert . run | Alt-R or click Rise icon | cmd line: jupyter nbconvert &lt;name&gt;.ipynb --to slides --post serve | . split cells | supported | not supported | . keyboard shortcuts | &#8592; &#8594; &#8670; &#8671; | &#8592; &#8593; &#8594; &#8595; | . some extensions . tips from Mark Roekpe&#39;s blog. Valid mainly for RISE. . splicell | hide code from slideshow | custom css | open slides automatically | .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/slide-show-jupyter.html",
            "relUrl": "/blog/slide-show-jupyter.html",
            "date": " • Jun 25, 2021"
        }
        
    
  
    
        ,"post21": {
            "title": "Reinforcement Learning Specialization - Coursera - course 4 - A Complete Reinforcement Learning System (Capstone)",
            "content": "Coursera website: course 4 - A Complete Reinforcement Learning System (Capstone) of Reinforcement Learning Specialization . my notes on course 1 - Fundamentals of Reinforcement Learning, course 2 - Sample-based Learning Methods, course 3 - Prediction and Control with Function Approximation . specialization roadmap - course 4 - A Complete Reinforcement Learning System (Capstone) (syllabus) . Week 1 - Welcome to the Course Week 2 - Formalize Word Problem as MDP Week 3 - Choosing The Right Algorithm Week 4 - Identify Key Performance Parameters Week 5 - Implement Your Agent Week 6 - Submit Your Parameter Study! . Course 4 - Week 2 - Formalize Word Problem as MDP . Final Project: Milestone 1 . Video Initial Project Meeting with Martha: Formalizing the Problem . . Video Andy Barto on What are Eligibility Traces and Why are they so named? . By the end of this video, you’ll understand the origin of the idea of eligibility traces and you’ll actually see that you’ve been using a variant of eligibility traces all along. . . Project Resources . Video Let’s Review: Markov Decision Processes . By the end of this video, you’ll be able to understand Markov decision processes or MDPs and describe how the dynamics of MDP are defined. . Video Let’s Review: Examples of Episodic and Continuing Tasks . By the end of this video, you will be able to understand when to formalize a task as episodic or continuing. . Assignment . MoonShot Technologies . notebooks in github . Course 4 - Week 3 - Choosing The Right Algorithm . Weekly Learning Goals . Video Meeting with Niko: Choosing the Learning Algorithm . . Project Resources . Video Let’s Review: Expected Sarsa . . Video Let’s Review: What is Q-learning? . Video Let’s Review: Average Reward- A New Way of Formulating Control Problems . Video Let’s Review: Actor-Critic Algorithm . Video Csaba Szepesvari on Problem Landscape . Video Andy and Rich: Advice for Students . Course 4 - Week 4 - Identify Key Performance Parameters . Weekly Learning Goals . Video Agent Architecture Meeting with Martha: Overview of Design Choices . Now, let’s discuss the meta parameter choices that you will have to make to fully implement the agent. This means we need to decide on the function approximator, choices in the optimizer for updating the action values, and how to do exploration. . Project Resources . Video Let’s Review: Non-linear Approximation with Neural Networks . By the end of this video, you will understand how neural networks do feature construction, and you will understand how neural networks are a non-linear function of state. . Video Drew Bagnell on System ID + Optimal Control . Video Susan Murphy on RL in Mobile Health . Course 4 - Week 5 - Implement your agent . Weekly Learning Goals . Video Meeting with Adam: Getting the Agent Details Right . Project Resources . Video Let’s Review: Optimization Strategies for NNs . By the end of this video, you will be able to understand the importance of initialization for neural networks and describe optimization techniques for training neural networks. . One simple yet effective initialization strategy, is to randomly sample the initial weights from a normal distribution with small variance. This way, each neuron has a different output from other neurons within its layer. This provides a more diverse set of potential features. By keeping the variants small, we ensure that the output of each neuron is within the same range as its neighbors. One downside to this strategy is that, as we add more inputs to a neuron, the variance of the output grows. We can get around this issue by scaling the variance of the weights, by one over the square root of the number of inputs. . . Here’s the stochastic gradient descent update rule and here’s the update modified to include momentum. Notice, it is similar to the regular stochastic gradient descent update plus an extra term called the momentum M. The momentum term summarizes the history of the gradients using a decaying sum of gradients with decay rate Lambda. If recent gradients have all been in similar directions, then we gained momentum in that direction. This means, we make a large step in that direction. If recent updates have conflicting directions, then it kills the momentum. The momentum term will have little impact on the update and we will make a regular gradient descent step. Momentum provably accelerates learning, meaning it gets to a stationary point more quickly. . . So far, we have only talked about a global scalar step size. This is well-known to be problematic because this can result in updates that are too big for some weights and too small for other weights. Adapting the step sizes for each weight, based on statistics about the learning process in practice results in much better performance. Now, how does the update change? The change is very simple. Instead of updating with a scalar Alpha, there’s a vector of step sizes indexed by t to indicate that it can change on each time-step. Each dimension of the gradient, is scaled by its corresponding step size instead of the global step size. There are a variety of methods to adapt a vector of step sizes. You’ll get to implement one in your assignment. . . Video Let’s Review: Expected Sarsa with Function Approximation . By the end of this video, you’ll be able to explain the update for expected Sarsa with function approximation, and explain the update for Q-learning with function approximation. . . . Video Let’s Review: Dyna &amp; Q-learning in a Simple Maze . By the end of this video you will be able to describe how learning from both real and model experience impacts performance. You will also be able to explain how a model allows the agent to learn from fewer interactions with the environment. . Video Meeting with Martha: In-depth on Experience Replay . In Course 3, the agents you implemented update the value function or policy only once with each sample. But this is likely not the most sample efficient way to use our data. You have actually seen a smarter approach in Course 2 where we talked about Dyna as a way to be more sample efficient. But we only talked about Dyna for the tabular setting. . In this video, we will talk about how to make your agent more sample efficient when using function approximation. We will discuss a simple method called experience replay and how it relates to Dyna. To get some intuition for experience replay, let’s first remember a method that we know well, Dyna-Q. The idea is to learn a model using sample experience. Then simulated experience can be obtained from this model to update the values. This procedure of using simulated experience to improve the value estimates is called planning. . Experience replay is a simple method for trying to get the advantages of Dyna. The basic idea is to save a buffer of experience and let the data be the model. We sample experience from this buffer and update the value function with those samples similarly to how we sample from the model and update the values in Dyna. . . Video Martin Riedmiller on The ‘Collect and Infer’ framework for data-efficient RL . Martin Riedmiller, head of the control team at Deepmind has been working for more than 20 years on New Reinforcement Learning Agents for the control of dynamical systems. . The control of dynamical systems is an attractive application area for reinforcement learning controllers. They all share the same principle feedback control structure, a controller gets the observation, computes an action and applies it to the environment. Classical control theory would first model the process as a set of differential equations for which then a control law must be analytically derived. A tedious job in particular if the systems are complex or highly nonlinear. Reinforcement learning in contrast promises to be able to learn the controller autonomously. If only the overall control goal is specified. This is typically done by defining the immediate reward. The RL controller optimizes the expected cumulated sum of rewards over time. . . These two steps together build the so-called collecting and infer framework of reinforcement learning. This perspective keeps us focused on the two main question of data efficient RL. Infer, which means squeezing out the most of a given set of transition data. And collect, which means sampling the most formative data from the environment. . . . Assignment . Implement your agent . notebooks in github . Course 4 - Week 6 - Submit your Parameter Study! . Weekly Learning Goals . Video Meeting with Adam: Parameter Studies in RL . Project Resources . Video Let’s Review: Comparing TD and Monte Carlo . Video Joelle Pineau about RL that Matters . Assignment . Completing the parameter study . notebooks in github . Congratulations! . Video Meeting with Martha: Discussing Your Results . Video Course Wrap-up . Video Specialization Wrap-up .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/reinforcement-learning-specialization-coursera-course4.html",
            "relUrl": "/blog/reinforcement-learning-specialization-coursera-course4.html",
            "date": " • Jun 14, 2021"
        }
        
    
  
    
        ,"post22": {
            "title": "Reinforcement Learning Specialization - Coursera - course 3 - Prediction and Control with Function Approximation",
            "content": "Coursera website: course 3 - Prediction and Control with Function Approximation of Reinforcement Learning Specialization . my notes on course 1 - Fundamentals of Reinforcement Learning, course 2 - Sample-based Learning Methods, course 4 - A Complete Reinforcement Learning System (Capstone) . specialization roadmap - course 3 - Prediction and Control with Function Approximation (syllabus) . course 3 - In Course 3, we leave the relative comfort of small finite MDPs and investigate RL with function approximation. Here we will see that the main concepts from Courses 1 and 2 transferred to problems with larger infinite state spaces. We will cover feature construction, neural network learning, policy gradient methods, and other particularities of the function approximation setting. . Week 1 - On-policy Prediction with Approximation . Week 2 - Constructing Features for Prediction . Week 3 - Control with Approximation . Week 4 - Policy Gradient . Course introduction . Video by Adam and Martha . . In course 2: . . In course 3: . . Course 3 - Week 1 - On-policy Prediction with Approximation . Module 1 Learning Objectives . Lesson 1: Estimating Value Functions as Supervised Learning . Understand how we can use parameterized functions to approximate value functions | Explain the meaning of linear value function approximation | Recognize that the tabular case is a special case of linear value function approximation. | Understand that there are many ways to parameterize an approximate value function | Understand what is meant by generalization and discrimination | Understand how generalization can be beneficial | Explain why we want both generalization and discrimination from our function approximation | Understand how value estimation can be framed as a supervised learning problem | Recognize not all function approximation methods are well suited for reinforcement learning | . Lesson 2: The Objective for On-policy Prediction . Understand the mean-squared value error objective for policy evaluation | Explain the role of the state distribution in the objective | Understand the idea behind gradient descent and stochastic gradient descent | Outline the gradient Monte Carlo algorithm for value estimation | Understand how state aggregation can be used to approximate the value function | Apply Gradient Monte-Carlo with state aggregation | . Lesson 3: The Objective for TD . Understand the TD-update for function approximation | Highlight the advantages of TD compared to Monte-Carlo | Outline the Semi-gradient TD(0) algorithm for value estimation | Understand that TD converges to a biased value estimate | Understand that TD converges much faster than Gradient Monte Carlo | . Lesson 4: Linear TD . Derive the TD-update with linear function approximation | Understand that tabular TD(0) is a special case of linear semi-gradient TD(0) | Highlight the advantages of linear value function approximation over nonlinear | Understand the fixed point of linear TD learning | Describe a theoretical guarantee on the mean squared value error at the TD fixed point | . Lesson 1: Estimating Value Functions as Supervised Learning . Reading Chapter 9.1-9.4 (pp. 197-209) in the Reinforcement Learning textbook . In many of the tasks to which we would like to apply reinforcement learning the state space is combinatorial and enormous; the number of possible camera images, for example, is much larger than the number of atoms in the universe. . In many of our target tasks, almost every state encountered will never have been seen before. To make sensible decisions in such states it is necessary to generalize from previous encounters with different states that are in some sense similar to the current one. In other words, the key issue is that of generalization. How can experience with a limited subset of the state space be usefully generalized to produce a good approximation over a much larger subset? . Fortunately, generalization from examples has already been extensively studied, and we do not need to invent totally new methods for use in reinforcement learning. To some extent we need only combine reinforcement learning methods with existing generalization methods. The kind of generalization we require is often called function approximation because it takes examples from a desired function (e.g., a value function) and attempts to generalize from them to construct an approximation of the entire function. Function approximation is an instance of supervised learning, the primary topic studied in machine learning, artificial neural networks, pattern recognition, and statistical curve fitting. . Video Moving to Parameterized Functions by Adam . By the end of this video, you’ll be able to understand how we can use parameterized functions to approximate values, explain linear value function approximation, recognize that the tabular case is a special case of linear value function approximation, and understand that there are many ways to parameterize an approximate value function. . Video Generalization and Discrimination by Martha . By the end of this video, you will be able to understand what is meant by generalization and discrimination, understand how generalization can be beneficial, and explain why we want both generalization and discrimination from our function approximation. . Video Framing Value Estimation as Supervised Learning by Martha . By the end of this video, you will be able to understand how value estimation can be framed as a supervised learning problem, and recognize that not all function approximation methods are well suited for reinforcement learning. . Lesson 2: The Objective for On-policy Prediction . Video The Value Error Objective by Adam . By the end of this video you will be able to understand the Mean Squared Value Error objective for policy evaluation and explain the role of the state distribution in the objective. . VE‾=∑sμ(s)[vπ(s)−v^(s,w)]2 overline{VE}= displaystyle sum_{s} mu(s)[v_ pi(s)- hat{v}(s,w)]^2VE=s∑​μ(s)[vπ​(s)−v^(s,w)]2 This is the Mean Squared Value Error Objective where $ mu$ reflects how much we care about each state (a probability distribution) . Video Introducing Gradient Descent by Martha . By the end of this video, you will be able to understand the idea of gradient descent, and understand that gradient descent converges to stationary points. . Video Gradient Monte for Policy Evaluation by Martha . By the end of this video, you will be able to understand how to use gradient descent and stochastic gradient descent to minimize value error and outline the Gradient Monte Carlo algorithm for value estimation. . . Video State Aggregation with Monte Carlo by Adam . By the end of this video, you will be able to understand how state aggregation can be used to approximate the value function and apply gradient Monte Carlo with state aggregation. . Lesson 3: The Objective for TD . Video Semi-Gradient TD for Policy Evaluation by Adam . By the end of this video you will be able to understand the TD update for function approximation, and outline the semi-gradient TD(0) algorithm for value estimation. . . Video Comparing TD and Monte Carlo with State Aggregation by Adam . By the end of this video, you’ll be able to understand that TD converges to a bias value estimate and understand that TD can learn faster than Gradient Monte Carlo. . Video Doina Precup: Building Knowledge for AI Agents with Reinforcement Learning . Lesson 4: Linear TD . Video The Linear TD Update by Martha . By the end of this video, you’ll be able to derive the TD update with linear function approximation, understand that tabular TD(0) as a special case of linear semi gradient TD(0), and understand why we care about linear TD as a special case. . Video The True Objective for TD by Martha . By the end of this video, you will be able to understand the fixed point of linear TD and describe a theoretical guarantee on the mean squared value error at the TD fixed point. . Video Week 1 Summary by Adam . Assignment . TD with State Aggregation . notebooks in github . Course 3 - Week 2 - Constructing Features for Prediction . Module 2 Learning Objectives . Lesson 1: Feature Construction for Linear Methods . Describe the difference between coarse coding and tabular representations | Explain the trade-off when designing representations between discrimination and generalization | Understand how different coarse coding schemes affect the functions that can be represented | Explain how tile coding is a (computationally?) convenient case of coarse coding | Describe how designing the tilings affects the resultant representation | Understand that tile coding is a computationally efficient implementation of coarse coding | . Lesson 2: Neural Networks . Define a neural network | Define activation functions | Define a feedforward architecture | Understand how neural networks are doing feature construction | Understand how neural networks are a non-linear function of state | Understand how deep networks are a composition of layers | Understand the tradeoff between learning capacity and challenges presented by deeper networks | . Lesson 3: Training Neural Networks . Compute the gradient of a single hidden layer neural network | Understand how to compute the gradient for arbitrarily deep networks | Understand the importance of initialization for neural networks | Describe strategies for initializing neural networks | Describe optimization techniques for training neural networks | . Lesson 1: Feature Construction for Linear Methods . Reading Chapter 9.4-9.5.0 (pp. 204-210), 9.5.3-9.5.4 (pp. 215-222) and 9.7 (pp. 223-228) in the Reinforcement Learning textbook . Video Coarse Coding by Adam . By the end of this video, you’ll be able to describe coarse coding and describe how it relates to state aggregation. . . Video Generalization Properties of Coarse Coding by Martha . By the end of this video, you’ll be able to describe how coarse coding parameters affect generalization and discrimination, and understand how that affects learning speed and accuracy. . Video Tile Coding by Adam . By the end of this video, you’ll be able to explain how tile coding achieves both generalization and discrimination, and understand the benefits and limitations of tile coding. . Video Using Tile Coding in TD by Adam . By the end of this video, you’ll be able to explain how to use tile coding with TD learning and identify important properties of tile code representations. . Lesson 2: Neural Networks . Video What is a Neural Network? by Martha . By the end of this video, you’ll be able to define a neural network, define an activation function and understand how a neural network is a parameterized function. . Video Non-linear Approximation with Neural Networks by Martha . By the end of this video, you will understand how neural networks do feature construction, and understand how neural networks are a non-linear function of state. . Video Deep Neural Networks by Adam . By the end of this video, you will understand how deep neural networks are composed of many layers and understand that depth can facilitate learning features through composition and abstraction. . Lesson 3: Training Neural Networks . Video Gradient Descent for Training Neural Networks by Martha . By the end of this video, you’ll be able to derive the gradient of a neural network and implement gradient descent on a neural network. . Video Optimization Strategies for NNs by Martha . By the end of this video, you will be able to understand the importance of initialization for neural networks and describe optimization techniques for training neural networks. . Video David Silver on Deep Learning + RL = AI? . Video Week 2 Review by Adam . Assignment . Semi-gradient TD with a Neural Network . notebooks in github . Course 3 - Week 3 - Control with Approximation . Module 3 Learning Objectives . Lesson 1: Episodic Sarsa with Function Approximation . Explain the update for Episodic Sarsa with function approximation | Introduce the feature choices, including passing actions to features or stacking state features | Visualize value function and learning curves | Discuss how this extends to Q-learning easily, since it is a subset of Expected Sarsa | . Lesson 2: Exploration under Function Approximation . Understanding optimistically initializing your value function as a form of exploration | . Lesson 3: Average Reward . Describe the average reward setting | Explain when average reward optimal policies are different from discounted solutions | Understand how differential value functions are different from discounted value functions | . Lesson 1: Episodic Sarsa with Function Approximation . Reading Chapter 10 (pp. 243-246) and 10.3 (pp. 249-252) in the Reinforcement Learning textbook . Video Episodic Sarsa with Function Approximation by Adam . By the end of this video, you’ll be able to understand how to construct action-dependent features for approximate action values and explain how to use Sarsa in episodic tasks with function approximation. . . Video Episodic Sarsa in Mountain Car by Adam . By the end of this video, you will gain experience analyzing the performance of an approximate TD control method. . Video Expected Sarsa with Function Approximation by Adam . By the end of this video, you’ll be able to explain the update for expected Sarsa with function approximation, and explain the update for Q-learning with function approximation. . . Lesson 2: Exploration under Function Approximation . Video Exploration under Function Approximation by Martha . By the end of this video, you’ll be able to describe how optimistic initial values and $ epsilon$-greedy can be used with function approximation. . Lesson 3: Average Reward . Video Average Reward: A New Way of Formulating Control Problems by Martha . By the end of this video, you’ll be able to describe the average reward setting, explain when average reward optimal policies are different from policies obtained under discounting and understand differential value functions. . Satinder Singh on Intrinsic Rewards . Video Week 3 Review by Martha . Assignment . Function Approximation and Control . notebooks in github . Course 3 - Week 4 - Policy Gradient . Module 4 Learning Objectives . Lesson 1: Learning Parameterized Policies . Understand how to define policies as parameterized functions | Define one class of parameterized policies based on the softmax function | Understand the advantages of using parameterized policies over action-value based methods | . Lesson 2: Policy Gradient for Continuing Tasks . Describe the objective for policy gradient algorithms | Describe the results of the policy gradient theorem | Understand the importance of the policy gradient theorem | . Lesson 3: Actor-Critic for Continuing Tasks . Derive a sample-based estimate for the gradient of the average reward objective | Describe the actor-critic algorithm for control with function approximation, for continuing tasks | . Lesson 4: Policy Parameterizations . Derive the actor-critic update for a softmax policy with linear action preferences | Implement this algorithm | Design concrete function approximators for an average reward actor-critic algorithm | Analyze the performance of an average reward agent | Derive the actor-critic update for a gaussian policy | Apply average reward actor-critic with a gaussian policy to a particular task with continuous actions | . Lesson 1: Learning Parameterized Policies . Reading Chapter 13 (pp. 321-336) in the Reinforcement Learning textbook . Video Learning Policies Directly by Adam . By the end of this video, you’ll be able to understand how to define policies as parameterized functions and define one class of parametrized policies based on the softmax function. . Video Advantages of Policy Parameterization by Adam . By the end of this video, you’ll be able to understand some of the advantages of using parameterized policies. . Lesson 2: Policy Gradient for Continuing Tasks . Video The Objective for Learning Policies by Martha . By the end of this video, you’ll be able to describe the objective for policy gradient algorithms. . . Video The Policy Gradient Theorem by Martha . By the end of this video, you will be able to describe the result of the policy gradient theorem and understand the importance of the policy gradient theorem. . . Lesson 3: Actor-Critic for Continuing Tasks . Video Estimating the Policy Gradient by Martha . By the end of this video, you will be able to derive a sample-based estimate for the gradient of the average reward objective. . . Video Actor-Critic Algorithm by Adam . By the end of this video, you’ll be able to describe the actor-critic algorithm for control with function approximation for continuing tasks. . . Lesson 4: Policy Parameterizations . Video Actor-Critic with Softmax Policies by Adam . By the end of this video you’ll be able to derive the actor critic update for a Softmax policy with linear action preferences and implement this algorithm. . Video Demonstration with Actor-Critic by Adam . By the end of this video, you’ll be able to design a function approximator for an average reward actor-critic algorithm and analyze the performance of an average reward agent. . Video Gaussian Policies for Continuous Actions by Martha . By the end of this video, you’ll be able to derive the actor-critic update for a Gaussian policy and apply average reward actor-critic with a Gaussian policy to task with continuous actions. . Video Week 4 Summary by Martha . . Assignment . Average Reward Softmax Actor-Critic using Tile-coding . notebooks in github .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/reinforcement-learning-specialization-coursera-course3.html",
            "relUrl": "/blog/reinforcement-learning-specialization-coursera-course3.html",
            "date": " • Jun 7, 2021"
        }
        
    
  
    
        ,"post23": {
            "title": "Logbook for June 21",
            "content": "Week 22 - June 21 . Tuesday 6/1 . Machine learning in python with scikit-learn end of module 3. Hyperparameter tuning . Reinforcement Learning Specialization - C2W3 - Temporal Difference for Control - start . Wednesday 6/2 . SHAP: An introduction to explainable AI with Shapley values, Be careful when interpreting predictive models in search of causal insights . Machine learning in python with scikit-learn module 4. Linear models . Thursday 6/3 . Talk (30’) from Michael Bronstein on Geometric Deep Learning - permutations invariant is a domain research I could use . Machine learning in python with scikit-learn end of module 4. Linear models . Friday 6/4 . Reinforcement Learning Specialization - C2W4 - Planning, Learning and Acting . End of course 2 of Reinforcement Learning Specialization . Week 23 - June 21 . Monday 6/7 . Reinforcement Learning Specialization - C3W1 - On-policy Prediction with Approximation . Thursday 6/10 . Reinforcement Learning Specialization - C3W2 - Constructing Features for Prediction . Friday 6/11 . Machine learning in python with scikit-learn module 5. Decision tree models . Reinforcement Learning Specialization - C3W3 - Control with Approximation . Week 24 - June 21 . Monday 6/14 . Reinforcement Learning Specialization - C3W4 - Policy Gradient . Reinforcement Learning Specialization - C4W1 - start of course 4. A Complete Reinforcement Learning System (Capstone) - week 1 to week 4 . Wednesday 6/16 . Reinforcement Learning Specialization - C4W4 - Milestone 3: Identify Key Performance Parameters, C4W5 - Milestone 4: Implement your agent . Thursday 6/17 . Reinforcement Learning Specialization - end of C4W5 - Milestone 4: Implement your agent . Friday 6/18 . Reinforcement Learning Specialization - C4W6 - Milestone 5: Submit your Parameter Study! . End of Specialization . Week 25 - June 21 . Monday 6/21 . Machine learning in python with scikit-learn module 6. Ensemble of models . RL Course by David Silver Integrating learning and planning (lecture 8) . Thursday 6/24 . Machine learning in python with scikit-learn module 7. Evaluating model performance -End of this course . Week 26 - June 21 . Monday 6/28 . AI Tech watchfulness in Manufacturing using Arxiv Sanity Presever . Paper reviewed on arxiv about local post-hoc explanations for predictive process monitoring in manufacturing. arXiv:2009.10513v2. SHAP, ICE and why this approach makes sense in manufacturing domains. . Wednesday 6/30 . Survey Paper reviewed on Journal of Manufacturing Systems about Deep learning for smart manufacturing: Methods and applications. j.jmsy.2018.01.003. Review use of deep learning algorithms: CNN, RBM, auto encoders, RNN and applications for smart manufacturing: quality inspection, fault assessment, defect prognosis (RUL). Unfortunately prescriptive usage are missing. Points to multiple references origins of these algorithms and applications. Great survey paper! .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/logbook-June.html",
            "relUrl": "/blog/logbook-June.html",
            "date": " • Jun 1, 2021"
        }
        
    
  
    
        ,"post24": {
            "title": "Reinforcement Learning Specialization - Coursera - course 2 - Sample-based Learning Methods",
            "content": "Coursera website: course 2 - Sample-based Learning Methods of Reinforcement Learning Specialization . my notes on course 1 - Fundamentals of Reinforcement Learning, course 3 - Prediction and Control with Function Approximation, course 4 - A Complete Reinforcement Learning System (Capstone) . specialization roadmap - course 2 - Sample-based Learning Methods . course 2 - In Course 2, we built on these ideas and design algorithms for learning without a model of the world. We study three classes of methods designed for learning from trial and error interaction. We start with Monte Carlo methods and then move on to temporal difference learning, including Q learning. We conclude Course 2 with an investigation of methods for planning with learned models. . Week 1 - Monte-Carlo Methods for Prediction &amp; Control . Week 2 - Temporal Difference Learning Methods for Prediction . Week 3 - Temporal Difference Learning Methods for Control . Week 4 - Planning, Learning &amp; Acting . Course 2 - Week 1 - Monte-Carlo Methods for Prediction &amp; Control . Module 1 Learning Objectives . Lesson 1: Introduction to Monte-Carlo Methods . Understand how Monte-Carlo methods can be used to estimate value functions from sampled interaction | Identify problems that can be solved using Monte-Carlo methods | Use Monte-Carlo prediction to estimate the value function for a given policy. | . Lesson 2: Monte-Carlo for Control . Estimate action-value functions using Monte-Carlo | Understand the importance of maintaining exploration in Monte-Carlo algorithms | Understand how to use Monte-Carlo methods to implement a GPI algorithm | Apply Monte-Carlo with exploring starts to solve an MDP | . Lesson 3: Exploration Methods for Monte-Carlo . Understand why exploring starts can be problematic in real problems | Describe an alternative exploration method for Monte-Carlo control | . Lesson 4: Off-policy learning for prediction . Understand how off-policy learning can help deal with the exploration problem | Produce examples of target policies and examples of behavior policies | Understand importance sampling | Use importance sampling to estimate the expected value of a target distribution using samples from a different distribution | Understand how to use importance sampling to correct returns | Understand how to modify the Monte-Carlo prediction algorithm for off-policy learning. | . Lesson 1: Introduction to Monte Carlo Methods . Reading Chapter 5.0-5.5 (pp. 91-104) in the Reinforcement Learning textbook . Although a model is required, the model need only generate sample transitions, not the complete probability distributions of all possible transitions that is required for dynamic programming (DP). . Video What is Monte Carlo by Martha . By the end of this video you will be able to understand how Monte Carlo methods can be used to estimate value functions from sampled interaction and identify problems that can be solved using Monte Carlo methods. . Video Using Monte Carlo for Prediction by Martha . By the end of this video, you will be able to use Monte Carlo prediction to estimate the value function for a given policy. . . Lesson 2: Monte Carlo for Control . Video Using Monte Carlo for Action Values by Adam . By the end of this video, you’ll be able to estimate action-value functions using Monte Carlo and understand the importance of maintaining exploration in Monte Carlo algorithms. . Video Using Monte Carlo methods for generalized policy iteration by Adam . By the end of this video, you will understand how to use Monte Carlo methods to implement a generalized policy iteration GPI algorithm. . . Video Solving the BlackJack Example by Adam . By the end of this video, you’ll be able to apply Monte Carlo with Exploring Starts to solve an example MDP. . Lesson 3: Exploration Methods for Monte Carlo . Video Epsilon-soft policies by Adam . By the end of this video you will understand why exploring starts can be problematic in real problems and you will be able to describe an alternative expiration method to maintain exploration in Monte Carlo control. . Lesson 4: Off-policy Learning for Prediction . Video Why does off-policy learning matter? by Martha . By the end of this video you will be able to understand how off policy learning can help deal with the expiration problem. You will also be able to produce examples of Target policies and examples of behavior policies. . The key points to take away from today are that off policy learning is another way to obtain continual exploration. The policy that we are learning is called the target policy and the policy that we are choosing actions from is the behavior policy. . Video Importance Sampling by Martha . By the end of this video, you will be able to use importance sampling to estimate the expected value of a target distribution using samples from a different distribution. . Video Off-Policy Monte Carlo Prediction by Martha . By the end of this video, you will be able to understand how to use important sampling to correct returns, and you will understand how to modify the Monte Carlo prediction algorithm for off-policy learning. . Video Emma Brunskill: Batch Reinforcement Learning . Video Week 1 Summary by Martha . Reading Chapter 5.10 (pp. 115-116) in the Reinforcement Learning textbook . Course 2 - Week 2 - Temporal Difference Learning Methods for Prediction . Module 2 Learning Objectives . Lesson 1: Introduction to Temporal Difference Learning . Define temporal-difference learning | Define the temporal-difference error | Understand the TD(0) algorithm | . Lesson 2: Advantages of TD . Understand the benefits of learning online with TD | Identify key advantages of TD methods over Dynamic Programming and Monte Carlo methods | Identify the empirical benefits of TD learning | . Lesson 1: Introduction to Temporal Difference Learning . Reading Chapter 6-6.3 (pp. 116-128) in the Reinforcement Learning textbook . Video What is Temporal Difference (TD) learning? by Adam . By the end of this video, you’ll be able to define temporal difference learning, define the temporal difference error, and understand the TD(0) algorithm. . . Video Rich Sutton: The Importance of TD Learning by Richard Sutton . Lesson 2: Advantages of TD . Video The advantages of temporal difference learning by Martha . By the end of this video, you will be able to understand the benefits of learning online with TD and identify key advantages of TD methods over dynamic programming and Monte Carlo. . Video Comparing TD and Monte Carlo by Adam . By the end of this video, you’ll be able to identify the empirical benefits of TD Learning. . Video Andy Barto and Rich Sutton: More on the History of RL . Video Week 2 Summary by Adam . Assignment . Policy Evaluation in Cliff Walking Environment . notebooks in github . Course 2 - Week 3 - Temporal Difference Learning Methods for Control . Module 3 Learning Objectives . Lesson 1: TD for Control . Explain how generalized policy iteration can be used with TD to find improved policies | Describe the Sarsa control algorithm | Understand how the Sarsa control algorithm operates in an example MDP | Analyze the performance of a learning algorithm | . Lesson 2: Off-policy TD Control: Q-learning . Describe the Q-learning algorithm | Explain the relationship between Q-learning and the Bellman optimality equations. | Apply Q-learning to an MDP to find the optimal policy | Understand how Q-learning performs in an example MDP | Understand the differences between Q-learning and Sarsa | Understand how Q-learning can be off-policy without using importance sampling | Describe how the on-policy nature of Sarsa and the off-policy nature of Q-learning affect their relative performance | . Lesson 3: Expected Sarsa . Describe the Expected Sarsa algorithm | Describe Expected Sarsa’s behaviour in an example MDP | Understand how Expected Sarsa compares to Sarsa control | Understand how Expected Sarsa can do off-policy learning without using importance sampling | Explain how Expected Sarsa generalizes Q-learning | . Lesson 1: TD for Control . Reading Chapter 6.4-6.6 (pp. 129-134) in the Reinforcement Learning textbook . . . Video Sarsa: GPI with TD by Martha . By the end of this video, you’ll be able to explain how generalized policy iteration can be used with TD to find improved policies, as well as describe the Sarsa control algorithm . Video Sarsa in the Windy Grid World by Adam . By the end of this video, you will understand how the Sarsa control algorithm operates in an example MDP. You will also gain experience analyzing the performance of a learning algorithm. . Lesson 2: Off-policy TD Control: Q-learning . Video What is Q-learning? by Martha . By the end of this video, you will be able to describe the Q-learning algorithm, and explain the relationship between Q-learning and the Bellman optimality equations. . Video Q-learning in the Windy Grid World by Adam . By the end of this video, you will gain insight into how Q-Learning performs in an example MDP. And gain experience comparing the performance of multiple learning algorithms on a single MDP. . Video How is Q-learning off-policy? by Martha . By the end of this video, you will understand how Q-learning can be off-policy without using important sampling and be able to describe how learning on-policy or off-policy might affect performance in control. . Lesson 3: Expected Sarsa . Video Expected Sarsa by Martha . By the end of this video, you will be able to explain the expected Sarsa algorithm. . Video Expected Sarsa in the Cliff World by Adam . By the end of this video, you will be able to describe expected Sarsas’s behavior in an example MDP and empirically compare expected Sarsa and Sarsa. . Video Generality of Expected Sarsa by Martha . By the end of this video, you will understand how Expected Sarsa can do off-policy learning without using importance sampling and explain how Expected Sarsa generalizes Q-learning. . Video Week 3 summary by Adam . . Sarsa uses a sample based version of the Bellman equation. It learns Q-pi. . Q-learning uses the Bellman optimality equation. It learns Q-star. . Expected sarsa uses the same Bellman equation as Sarsa, but samples it differently. It takes an expectation over the next action values. . What’s the story with on-policy and off-policy learning? . Sarsa is a on-policy algorithm that learns the action values for the policy it’s currently following. Q-learning is an off-policy algorithm that learns the optimal action values. And Expected Sarsa is both an on-policy and an off-policy algorithm that can learn the action values for any policy. . Assignment . Q-Learning and Expected Sarsa . notebooks in github . Course 2 - Week 4 - Planning, Learning &amp; Acting . Module 4 Learning Objectives . Lesson 1: What is a model? . Describe what a model is and how they can be used | Classify models as distribution models or sample models | Identify when to use a distribution model or sample model | Describe the advantages and disadvantages of sample models and distribution models | Explain why sample models can be represented more compactly than distribution models | . Lesson 2: Planning . Explain how planning is used to improve policies | Describe random-sample one-step tabular Q-planning | . Lesson 3: Dyna as a formalism for planning . Recognize that direct RL updates use experience from the environment to improve a policy or value function | Recognize that planning updates use experience from a model to improve a policy or value function | Describe how both direct RL and planning updates can be combined through the Dyna architecture | Describe the Tabular Dyna-Q algorithm | Identify the direct-RL and planning updates in Tabular Dyna-Q | Identify the model learning and search control components of Tabular Dyna-Q | Describe how learning from both direct and simulated experience impacts performance | Describe how simulated experience can be useful when the model is accurate | . Lesson 4: Dealing with inaccurate models . Identify ways in which models can be inaccurate | Explain the effects of planning with an inaccurate model | Describe how Dyna can plan successfully with a partially inaccurate model | Explain how model inaccuracies produce another exploration-exploitation trade-off | Describe how Dyna-Q+ proposes a way to address this trade-off | . Lesson 5: Course wrap-up . Lesson 1: What is a model? . Reading Chapter 8.1-8.3 (pp. 159-166) in the Reinforcement Learning textbook . Model-based methods rely on planning as their primary component, while model-free methods primarily rely on learning. . Video What is a Model? by Martha . By the end of the video, you will be able to describe a model and how it can be used, classify models as distribution models or sample models, and identify when to use a distribution model or sample model. . Video Comparing Sample and Distribution Models by Martha . By the end of this video, you will be able to describe the advantages and disadvantages of sample models and distribution models, and you will also be able to explain why sample models can be represented more compactly than distribution models. . Lesson 2: Planning . Video Random Tabular Q-planning by Martha . By the end of this video, you’ll be able to explain how planning is used to improve policies and describe random-sample one-step tabular Q-planning. . Lesson 3: Dyna as a formalism for planning . Video The Dyna Architecture by Adam . By the end of this video, you will be able to understand how simulate experience from the model differs from interacting with the environment. You will also understand how the Dyna architecture mixes direct RL updates and planning updates. . . Video The Dyna Algorithm by Adam . By the end of this video, you should be able to describe how Tabular Dyna-Q works. You will also be able to identify the direct-RL, planning updates in Tabular Dyna-Q, and identify the model learning and search control components of Tabular Dyna-Q. . . Video Dyna &amp; Q-learning in a Simple Maze by Adam . By the end of this video you will be able to describe how learning from both environment-real and model experience impacts performance. You will also be able to explain how an accurate model allows the agent to learn from fewer environment interactions. . Lesson 4: Dealing with inaccurate models . Video What if the model is inaccurate? by Martha . By the end of this video you will be able to identify ways in which models can be inaccurate, explain the effects of planning with an inaccurate model, and describe how Dyna can plan successfully with an incomplete model. . Video In-depth with changing environments by Adam . By the end of this video, you’ll be able to explain how model inaccuracies produce another exploration-exploitation trade-off, and describe how Dyna-Q+ addresses this trade-off. . . Video Drew Bagnell: self-driving, robotics, and Model Based RL . Video week 4 summary by Martha . Assignment . Dyna-Q and Dyna-Q+ . notebooks in github . Chapter summary Chapter 8.12 (pp. 188) . . Planning, acting, and model-learning interact in a circular fashion (as in the figure above), each producing what the other needs to improve; no other interaction among them is either required or prohibited. . Text Book Part 1 Summary . . For a summary of what we’ve covered in the specialization so far, read: pp. 189-191 in Reinforcement Learning: an introduction . . All of the methods we have explored so far in this book have three key ideas in common: first, they all seek to estimate value functions; second, they all operate by backing up values along actual or possible state trajectories; and third, they all follow the general strategy of generalized policy iteration (GPI), meaning that they maintain an approximate value function and an approximate policy, and they continually try to improve each on the basis of the other. These three ideas are central to the subjects covered in this book. We suggest that value functions, backing up value updates, and GPI are powerful organizing principles potentially relevant to any model of intelligence, whether artificial or natural. . Course wrap-up .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/reinforcement-learning-specialization-coursera-course2.html",
            "relUrl": "/blog/reinforcement-learning-specialization-coursera-course2.html",
            "date": " • May 25, 2021"
        }
        
    
  
    
        ,"post25": {
            "title": "Machine learning in python with scikit-learn",
            "content": "This is a MOOC by Inria team, in charge of scikit-learn. . After a fair amount of pedagogical and technical preparation work, we offer you today a practical course with: . 7 modules + 1 introductory module | 9 video lessons to explain the main machine learning concepts | 71 programming notebooks (you don’t have to install anything) to get hands-on skills | 27 quizzes, 7 wrap-up quizzes and 23 exercises to train and deepen your practice | . Syllabus . Introduction: Machine Learning concepts, then . Module 1. The Predictive Modeling Pipeline . Module 2. Selecting the best model . Module 3. Hyperparameter tuning . Module 4. Linear Models . Module 5. Decision tree models . Module 6. Ensemble of models . Module 7. Evaluating model performance . INRIA github contains everything of this mooc: slides, datasets, notebooks (not videos) . I have forked it, and I use local envt for assignments. . ~/git/guillaume$ git clone git@github.com:castorfou/scikit-learn-mooc.git ~/git/guillaume$ cd scikit-learn-mooc/ ~/git/guillaume/scikit-learn-mooc$ conda env create -f environment.yml conda activate scikit-learn-course . Introduction - Machine Learning concepts . slides . Module 1. The Predictive Modeling Pipeline . Module overview . The objective in the module are the following: . build intuitions regarding an unknown dataset; | identify and differentiate numerical and categorical features; | create an advanced predictive pipeline with scikit-learn. | . Tabular data exploration . exploration of data: 01_tabular_data_exploration.ipynb . exercise M1.01: 01_tabular_data_exploration_ex_01.ipynb . Fitting a scikit-learn model on numerical data . first model with scikit-learn: 02_numerical_pipeline_introduction.ipynb . exercise M1.02: 02_numerical_pipeline_ex_00.ipynb . working with numerical data: 02_numerical_pipeline_hands_on.ipynb . exercise M1.03: 02_numerical_pipeline_ex_01.ipynb . preprocessing for numerical features: 02_numerical_pipeline_scaling.ipynb . Handling categorical data . Encoding of categorical variables: 03_categorical_pipeline.ipynb . Thus, in general OneHotEncoder is the encoding strategy used when the downstream models are linear models while OrdinalEncoder is used with tree-based models. . Exercise M1.04: 03_categorical_pipeline_ex_01.ipynb . Using numerical and categorical variables together: 03_categorical_pipeline_column_transformer.ipynb . Exercise M1.05: 03_categorical_pipeline_ex_02.ipynb . Wrap-up quiz . module 1 - wrap-up quizz.ipynb . Module 2. Selecting the best model . Module overview . The objective in the module are the following: . understand the concept of overfitting and underfitting; | understand the concept of generalization; | understand the general cross-validation framework used to evaluate a model. | . Overfitting and Underfitting . video and slides . The framework and why do we need it: cross_validation_train_test.ipynb . Validation and learning curves . video and slides . Overfit-generalization-underfit: cross_validation_validation_curve.ipynb . Effect of the sample size in cross-validation: cross_validation_learning_curve.ipynb . Exercise M2.01: cross_validation_ex_01.ipynb solution . Bias versus variance trade-off . video and slides . . Wrap-up quiz . module 2 - wrap-up quizz.ipynb . Overfitting is caused by the limited size of the training set, the noise in the data, and the high flexibility of common machine learning models. | Underfitting happens when the learnt prediction functions suffer from systematic errors. This can be caused by a choice of model family and parameters, which leads to a lack of flexibility to capture the repeatable structure of the true data generating process. | For a fixed training set, the objective is to minimize the test error by adjusting the model family and its parameters to find the best trade-off between overfitting for underfitting. | For a given choice of model family and parameters, increasing the training set size will decrease overfitting but can also cause an increase of underfitting. | The test error of a model that is neither overfitting nor underfitting can still be high if the variations of the target variable cannot be fully determined by the input features. This irreducible error is caused by what we sometimes call label noise. In practice, this often happens when we do not have access to important features for one reason or another. | . Module 3. Hyperparameter tuning . Module overview . The objective in the module are the following: . understand what is a model hyperparameter; | understand how to get and set the value an hyperparameter of a scikit-learn model; | be able to fine tune a full predictive modeling pipeline; | understand and visualize the combination of parameters that improves the performance of a model. | . Manual tuning . Set and get hyperparameters in scikit-learn: parameter_tuning_manual.ipynb . Exercise M3.01: parameter_tuning_ex_02.ipynb . Automated tuning . Hyperparameter tuning by grid-search: parameter_tuning_grid_search.ipynb . Hyperparameter tuning by randomized-search: parameter_tuning_randomized_search.ipynb . Cross-validation and hyperparameter tuning: parameter_tuning_nested.ipynb . Exercise M3.01: parameter_tuning_ex_03.ipynb solution . . Nice to play with interactive plotly parallel_coordinates to identify best params. . import numpy as np import pandas as pd import plotly.express as px def shorten_param(param_name): if &quot;__&quot; in param_name: return param_name.rsplit(&quot;__&quot;, 1)[1] return param_name cv_results = pd.read_csv(&quot;../figures/randomized_search_results.csv&quot;, index_col=0) fig = px.parallel_coordinates( cv_results.rename(shorten_param, axis=1).apply({ &quot;learning_rate&quot;: np.log10, &quot;max_leaf_nodes&quot;: np.log2, &quot;max_bins&quot;: np.log2, &quot;min_samples_leaf&quot;: np.log10, &quot;l2_regularization&quot;: np.log10, &quot;mean_test_score&quot;: lambda x: x}), color=&quot;mean_test_score&quot;, color_continuous_scale=px.colors.sequential.Viridis, ) fig.show() . Wrap-up quiz . module 3 - wrap-up quizz.ipynb . Hyperparameters have an impact on the models’ performance and should be wisely chosen; | The search for the best hyperparameters can be automated with a grid-search approach or a randomized search approach; | A grid-search is expensive and does not scale when the number of hyperparameters to optimize increase. Besides, the combination are sampled only on a regular grid. | A randomized-search allows a search with a fixed budget even with an increasing number of hyperparameters. Besides, the combination are sampled on a non-regular grid. | . Module 4. Linear models . Module overview . In this module, your objectives are to: . understand the linear models parametrization; | understand the implication of linear models in both regression and classification; | get intuitions of linear models applied in higher dimensional dataset; | understand the effect of regularization and how to set it; | understand how linear models can be used even with data showing non-linear relationship with the target to be predicted. | . Intuitions on linear models . video and slides . For regression: linear regression . from sklearn.linear_model import LinearRegression linear_regression = LinearRegression() linear_regression.fit(X, y) . For classification: logistic regression . from sklearn.linear_model import LogisticRegression log_reg = LogisticRegression() log_reg.fit(X, y) . Linear regression . Linear regression without scikit-learn: linear_regression_without_sklearn.ipynb . Exercise M4.01: linear_models_ex_01.ipynb solution . usage of np.ravel in . def goodness_fit_measure(true_values, predictions): # we compute the error between the true values and the predictions of our model errors = np.ravel(true_values) - np.ravel(predictions) return np.mean(np.abs(errors)) . Linear regression using scjkit-learn: linear_regression_in_sklearn.ipynb . from sklearn.metrics import mean_squared_error inferred_body_mass = linear_regression.predict(data) model_error = mean_squared_error(target, inferred_body_mass) print(f&quot;The mean squared error of the optimal model is {model_error:.2f}&quot;) . Modeling non-linear features-target relationships . Exercise M4.02: linear_models_ex_02.ipynb solution . Linear regression with non-linear link between data and target: linear_regression_non_linear_link.ipynb . Exercise M4.03: linear_models_ex_03.ipynb solution . Regularization in linear model . video and slides . Ridge regression . from sklearn.linear_model import Ridge model = Ridge(alpha=0.01).fit(X, y) . always use Ridge with a carefully tuned alpha! . from sklearn.linear_model import RidgeCV model = RidgeCV( alphas=[0.001, 0.1, 1, 10, 1000] ) model.fit(X, y) print(model.alpha_) . Regularization of linear regression model: linear_models_regularization.ipynb . Exercise M4.04: linear_models_ex_04.ipynb solution . Linear model for classification . Linear model for classification: logistic_regression.ipynb . Exercise M4.05: linear_models_ex_05.ipynb solution . Beyond linear separation in classification: logistic_regression_non_linear.ipynb . Wrap-up quiz . module 4 - wrap-up quizz.ipynb . In this module, we saw that: . the predictions of a linear model depend on a weighted sum of the values of the input features added to an intercept parameter; | fitting a linear model consists in adjusting both the weight coefficients and the intercept to minimize the prediction errors on the training set; | to train linear models successfully it is often required to scale the input features approximately to the same dynamic range; | regularization can be used to reduce over-fitting: weight coefficients are constrained to stay small when fitting; | the regularization hyperparameter needs to be fine-tuned by cross-validation for each new machine learning problem and dataset; | linear models can be used on problems where the target variable is not linearly related to the input features but this requires extra feature engineering work to transform the data in order to avoid under-fitting. | . Module 5. Decision tree models . Module overview . The objective in the module are the following: . understand how decision trees are working in classification and regression; | check which tree parameters are important and their influences. | . Intuitions on tree-based models . video and slides . Decision tree in classification . Build a classification decision tree: trees_classification.ipynb . Exercise M5.01: trees_ex_01.ipynb solution . Fit and decision boundaries . from sklearn.tree import DecisionTreeClassifier import seaborn as sns # create a palette to be used in the scatterplot palette = [&quot;tab:red&quot;, &quot;tab:blue&quot;, &quot;black&quot;] tree = DecisionTreeClassifier(max_depth=2) tree.fit(data_train, target_train) ax = sns.scatterplot(data=penguins, x=culmen_columns[0], y=culmen_columns[1], hue=target_column, palette=palette) plot_decision_function(tree, range_features, ax=ax) plt.legend(bbox_to_anchor=(1.05, 1), loc=&#39;upper left&#39;) _ = plt.title(&quot;Decision boundary using a decision tree&quot;) . Decision tree . from sklearn.tree import plot_tree _, ax = plt.subplots(figsize=(17, 12)) _ = plot_tree(tree, feature_names=culmen_columns, class_names=tree.classes_, impurity=False, ax=ax) . Accuracy . tree.fit(data_train, target_train) test_score = tree.score(data_test, target_test) print(f&quot;Accuracy of the DecisionTreeClassifier: {test_score:.2f}&quot;) . Decision tree in regression . Decision tree for regression: trees_regression.ipynb . Exercise M5.02: trees_ex_02.ipynb solution . Hyperparameters of decision tree . Importance of decision tree hyperparameters on generalization: trees_hyperparameters.ipynb . Wrap-up quiz . module 5 - wrap-up quizz.ipynb . [Main take-away | Main take-away | 41026 Courseware | FUN-MOOC](https://lms.fun-mooc.fr/courses/course-v1:inria+41026+session01/courseware/6565c007789a4812aea0debb1fb22e0f/0ab58cb806034cdba7bd49e8dd784202/) | . In this module, we presented decision trees in details. We saw that they: . are suited for both regression and classification problems; | are non-parametric models; | are not able to extrapolate; | are sensible to hyperparameter tuning. | . Module 6. Ensemble of models . Module overview . The objective in the module are the following: . understanding the principles behind bootstrapping and boosting; | get intuitions with specific models such as random forest and gradient boosting; | identify the important hyperparameters of random forest and gradient boosting decision trees as well as their typical values. | . Intuitions on ensemble of tree-based models . video and slides . “Bagging” stands for Bootstrap AGGregatING. It uses bootstrap resampling (random sampling with replacement) to learn several models on random variations of the training set. At predict time, the predictions of each learner are aggregated to give the final predictions. . from sklearn.ensemble import BaggingClassifier from sklearn.ensemble import RandomForestClassifier . Random Forests are bagged randomized decision trees . At each split: a random subset of features are selected | The best split is taken among the restricted subset | Extra randomization decorrelates the prediction errors | Uncorrelated errors make bagging work better | . Gradient Boosting . Each base model predicts the negative error of previous models | sklearn use decision trees as the base model | . from sklearn.ensemble import GradientBoostingClassifier . Implementation of the traditional (exact) method | Fine for small data sets | Too slow for n_samples &gt; 10,000 | . from sklearn.ensemble import HistGradientBoostingClassifier . Discretize numerical features (256 levels) | Efficient multi core implementation | Much, much faster when n_samples is large | . Take away . Bagging and random forests fit trees independently each deep tree overfits individually | averaging the tree predictions reduces overfitting | . | (Gradient) boosting fits trees sequentially each shallow tree underfits individually | sequentially adding trees reduces underfitting | . | Gradient boosting tends to perform slightly better than bagging and random forest and furthermore shallow trees predict faster. | . Introductory example to ensemble models: ensemble_introduction.ipynb . Ensemble method using bootstrapping . Bagging: ensemble_bagging.ipynb . Wikipedia reference to bootstrapping in statistics. . Exercise M6.01: ensemble_ex_01.ipynb (solution) . Random Forest: ensemble_random_forest.ipynb . Exercise M6.01: ensemble_ex_02.ipynb (solution) . Ensemble method using boosting . Adaptive Boosting (AdaBoost): ensemble_adaboost.ipynb . Exercise M6.03: ensemble_ex_03.ipynb (solution) . Gradient-boosting decision tree (GBDT): ensemble_gradient_boosting.ipynb . Exercise M6.04: ensemble_ex_04.ipynb (solution) . Speeding-up gradient-boosting: ensemble_hist_gradient_boosting.ipynb . Hyperparameter tuning with ensemble methods . Hyperparameter tuning: ensemble_hyperparameters.ipynb . Exercise M6.05: ensemble_ex_05.ipynb (solution) . Wrap-up quiz . module 6 - wrap-up quizz.ipynb . Use of Imbalanced-learn library relying on scikit-learn and provides methods to deal with classification with imbalanced classes. . Module 7. Evaluating model performance . Module overview . The objective in the module are the following: . understand the necessity of using an appropriate cross-validation strategy depending on the data; | get the intuitions behind comparing a model with some basic models that can be used as baseline; | understand the principles behind using nested cross-validation when the model needs to be evaluated as well as optimized; | understand the differences between regression and classification metrics; | understand the differences between metrics. | . Comparing a model with simple baselines . Comparing results with baseline and chance level: cross_validation_baseline.ipynb . Exercise M7.01: cross_validation_ex_02.ipynb (solution) . Choice of cross-validation . Introductory exercise regarding stratification: cross_validation_ex_03.ipynb . Stratification: cross_validation_stratification.ipynb . Introductory exercise for sample grouping: cross_validation_ex_04.ipynb . Sample grouping: cross_validation_grouping.ipynb . Introductory exercise for non i.i.d. data: cross_validation_ex_05.ipynb . Non i.i.d. data: cross_validation_time.ipynb . Nested cross-validation . Nested cross-validation: cross_validation_nested.ipynb . Introduction of the evaluation metrics: Classification metrics . Classification: metrics_classification.ipynb . Exercise M7.02: metrics_ex_01.ipynb (solution) . Introduction of the evaluation metrics: Regression metrics . Regression: metrics_regression.ipynb . Exercise M7.03: metrics_ex_02.ipynb (solution) . Wrap-up quiz . module 7 - wrap-up quizz.ipynb . And this completes the course .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/Machine-learning-in-python-with-scikit-learn.html",
            "relUrl": "/blog/Machine-learning-in-python-with-scikit-learn.html",
            "date": " • May 21, 2021"
        }
        
    
  
    
        ,"post26": {
            "title": "Reinforcement Learning Specialization - Coursera - course 1 - Fundamentals of Reinforcement Learning",
            "content": "Coursera website: course 1 - Fundamentals of Reinforcement Learning of Reinforcement Learning Specialization . my notes on course 2 - Sample-based Learning Methods, course 3 - Prediction and Control with Function Approximation, course 4 - A Complete Reinforcement Learning System (Capstone) . Syllabus . 4 courses on 16 weeks by Martha White and Adam White. . Fundamentals of Reinforcement Learning . | Sample-based Learning Methods . | Prediction and Control with Function Approximation . | A Complete Reinforcement Learning System (Capstone) . | . specialization roadmap . course 1 - we begin our study with multi-arm bandit problems. Here, we get our first taste of the complexities of incremental learning, exploration, and exploitation. After that, we move onto Markov decision processes to broaden the class of problems we can solve with reinforcement learning methods. Here we will learn about balancing short-term and long-term reward. We will introduce key ideas like policies and value functions using almost all RL systems. We conclude Course 1 with classic planning methods called dynamic programming. These methods have been used in large industrial control problems and can compute optimal policies given a complete model of the world. . course 2 - In Course 2, we built on these ideas and design algorithms for learning without a model of the world. We study three classes of methods designed for learning from trial and error interaction. We start with Monte Carlo methods and then move on to temporal difference learning, including Q learning. We conclude Course 2 with an investigation of methods for planning with learned models. . course 3 - In Course 3, we leave the relative comfort of small finite MDPs and investigate RL with function approximation. Here we will see that the main concepts from Courses 1 and 2 transferred to problems with larger infinite state spaces. We will cover feature construction, neural network learning, policy gradient methods, and other particularities of the function approximation setting. . course 4 - The final course in this specialization brings everything together in a Capstone project. Throughout this specialization, as in Rich and Andy’s book, we stress a rigorous and scientific approach to RL. We conduct numerous experiments designed to carefully compare algorithms. It takes careful planning and a lot of hard work to produce a meaningful empirical results. In the Capstone, we will walk you through each step of this process so that you can conduct your own scientific experiment. We will explore all the stages from problem specification, all the way to publication quality plots. This is not just academic. In real problems, it’s important to verify and understand your system. After that, you should be ready to test your own new ideas or tackle a new exciting application of RL in your job. We hope you enjoyed the show half as much as we enjoyed making it for you. . Alberta is in Canada. . 5/3/21 - Course 1 - Week 1 - An introduction to Sequential Decision-Making . I have set recommended goals 3 times a week. . about supervised learning, unsupervised learning and RL . You might wonder what’s the difference between supervised learning, unsupervised learning, and reinforcement learning? The differences are quite simple. In supervised learning we assume the learner has access to labeled examples giving the correct answer. In RL, the reward gives the agent some idea of how good or bad its recent actions were. You can think of supervised learning as requiring a teacher that helps you by telling you the correct answer. A reward on the other hand, is like having someone who can identify what good behavior looks like but can’t tell you exactly how to do it. Unsupervised learning sounds like it could be related but really has a very different goal. Unsupervised learning is about extracting underlying structure in data. It’s about the data representation. It can be used to construct representations that make a supervised or RL system better. In fact, as you’ll see later in this course, techniques from both supervised learning and unsupervised learning can be used within RL to aid generalization . industrial control . So I think the place we’re really going to see it take off is an industrial control. In industrial control, we have experts that are really looking for ways to improve the optimal- how well their systems work. So we’re going to see it do things like reduce energy costs or save on other types of costs that we have in these industrial control systems. In the hands of experts, we can really make these algorithms work well in the near future. So I really see it as a tool that’s going to facilitate experts in their work rather than say, doing something like replacing people or automating them away. . Reinforcement Learning Textbook . as always, Reinforcement Learning: An introduction (Second Edition) by Richard S. Sutton and Andrew G. Barto is THE reference. I didn’t know that Adam White was student from Sutton. Lucky guy ;) . K-armed Bandit problem . . Starts with reading of RLbook p25-36 (Chapter 2 Multi-armed Bandits) . Evaluative vs instructive feedback. Nonassociative refers to cases where you take one action per state. At the end there is a generalization where bandit problem becomes associative, that is, when actions are taken in more than one situation. . It is a stationary case meaning that value of actions are fixed during experiences. If the bandit task were nonstationary, that is, the true values of the actions changed over time. In this case exploration is needed even in the deterministic case to make sure one of the nongreedy actions has not changed to become better than the greedy one. . sample-average action-value estimates . Qt(a)=sum of rewards when a taken prior to tnumber of times a taken prior to tQt(a)=∑i=1t−1Ri.1Ai=a∑i=1t−11Ai=aQ_t(a) = frac{ text{sum of rewards when } mathit{a} text{ taken prior to } mathit{t}}{ text{number of times } mathit{a} text{ taken prior to } mathit{t}} Q_t(a) = frac{ displaystyle sum_{i=1}^{t-1} R_i. mathcal{1}_{A_i=a}}{ displaystyle sum_{i=1}^{t-1} mathcal{1}_{A_i=a}}Qt​(a)=number of times a taken prior to tsum of rewards when a taken prior to t​Qt​(a)=i=1∑t−1​1Ai​=a​i=1∑t−1​Ri​.1Ai​=a​​ . $ epsilon$-greedy action selection . At=argmaxa Qt(a)A_t= underset{a}{ mathrm{argmax}}{ text{ }Q_t(a)}At​=aargmax​ Qt​(a) . With nonstationary problem, we want to give more weights to recent rewards. It can be done with Qn+1=Qn+α[Rn−Qn]Q_{n+1}=Q_n+ alpha[R_n-Q_n]Qn+1​=Qn​+α[Rn​−Qn​] Where α alphaα is a constant step-size parameter, α∈[0,1] alpha in [0,1]α∈[0,1]. So it can be written that way Qn+1=(1−α)nQ1+∑i=1nα(1−α)n−iRiQ_{n+1}=(1- alpha)^nQ_1+ displaystyle sum_{i=1}^{n} alpha(1- alpha)^{n-i}R_iQn+1​=(1−α)nQ1​+i=1∑n​α(1−α)n−iRi​ . Weighted average because the sum of the weights is 1. . 2 other topics are discussed: optimistic initial values (that can push exploration in 1st steps) and upper-confidence-bound (UCB) action selection. With optimistic initial values the idea is too have high initial value for reward so that the 1st actions are disappointing pushing for explorations. With UCB . At=argmaxa [Qt(a)+cln⁡tNt(a)]A_t= underset{a} { mathrm{argmax}} { text{ } bigg[Q_t(a)+c sqrt{ frac{ ln t}{N_t(a)}} bigg]}At​=aargmax​ [Qt​(a)+cNt​(a)lnt​ . ​] . The idea of this upper confidence bound (UCB) action selection is that the square-root term is a measure of the uncertainty or variance in the estimate of a’s value. The quantity being max’ed over is thus a sort of upper bound on the possible true value of action a, with c determining the confidence level. Each time a is selected the uncertainty is presumably reduced: N t (a) increments, and, as it appears in the denominator, the uncertainty term decreases. On the other hand, each time an action other than a is selected, t increases but N t (a) does not; because t appears in the numerator, the uncertainty estimate increases. The use of the natural logarithm means that the increases get smaller over time, but are unbounded; all actions will eventually be selected, but actions with lower value estimates, or that have already been selected frequently, will be selected with decreasing frequency over time. . Exploration vs Exploitation trade-off . How do we choose when to explore, and when to exploit? Randomly . . Assignement . implementation of greedy agent, $ epsilon$-greedy agent. Comparisons. Various $ epsilon$ values, various step-sizes (1/N(a), …) . notebooks in github . end of C1W1 (course 1 week 1) . 5/7/21 - Course 1 - Week 2 - Markov Decision Process . Module 2 Learning Objectives . Lesson 1: Introduction to Markov Decision Processes . Understand Markov Decision Processes, or MDPs . | Describe how the dynamics of an MDP are defined . | Understand the graphical representation of a Markov Decision Process . | Explain how many diverse processes can be written in terms of the MDP framework . | . Lesson 2: Goal of Reinforcement Learning . Describe how rewards relate to the goal of an agent . | Understand episodes and identify episodic tasks . | . Lesson 3: Continuing Tasks . Formulate returns for continuing tasks using discounting . | Describe how returns at successive time steps are related to each other . | Understand when to formalize a task as episodic or continuing . | . Lesson 1: Introduction to Markov Decision Processes . Reading chapter 3.1 to 3.3 (p47-56) in Sutton’s book . Finite Markov Decision Processes . 3.1 - the Agent-Environment Interface | 3.2 - Goals and Rewards | 3.3 - Returns and Episodes | . In a Markov decision process, the probabilities given by p completely characterize the environment’s dynamics. That is, the probability of each possible value for $S_t$ and $R_t$ depends only on the immediately preceding state and action, $S_{t-1}$ and $A_{t-1}$ , and, given them, not at all on earlier states and actions. . p(s′,r∣s,a)≐Pr{St=s′,Rt=r∣St−1=s,At−1=a}p(s&amp;#x27;,r|s,a) doteq Pr {S_t=s&amp;#x27;, R_t=r|S_{t-1}=s, A_{t-1}=a }p(s′,r∣s,a)≐Pr{St​=s′,Rt​=r∣St−1​=s,At−1​=a} . The state must include information about all aspects of the past agent–environment interaction that make a difference for the future. In general, actions can be any decisions we want to learn how to make, and the states can be anything we can know that might be useful in making them. . The agent–environment boundary represents the limit of the agent’s absolute control, not of its knowledge. . Goal can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward). The reward signal is your way of communicating to the agent what you want it to achieve, not how you want it achieved. . Expected return $G_t$ is defined as some specific function of the reward sequence. In the simplest case the return is the sum of the rewards: Gt≐Rt+1+Rt+2+Rt+3+...+RTG_t doteq R_{t+1}+R_{t+2}+R_{t+3}+...+R_{T}Gt​≐Rt+1​+Rt+2​+Rt+3​+...+RT​ where $T$ is the final time step. . With continuing tasks, we can have $T= infty$, we can then introduce discounting. Agent chooses $A_t$ to maximize the expected discounted return: Gt≐Rt+1+γRt+2+γ2Rt+3+...=∑k=0∞γkRt+k+1G_t doteq R_{t+1}+ gamma R_{t+2}+ gamma^2 R_{t+3}+...= displaystyle sum_{k=0}^{ infty} gamma^k R_{t+k+1}Gt​≐Rt+1​+γRt+2​+γ2Rt+3​+...=k=0∑∞​γkRt+k+1​ where $ gamma$ is called the discount rate. Gt=Rt+1+γGt+1G_t = R_{t+1}+ gamma G_{t+1}Gt​=Rt+1​+γGt+1​ Video MDP by Martha. By the end of this video: Understand Markov Decision Process (MDP), Describe how the dynamics of an MDP are defined. . Martha highlights differences between k-armed bandit and MDP. The k-armed bandit agent is presented with the same situation at each time and the same action is always optimal. In many problems, different situations call for different responses. The actions we choose now affect the amount of reward we can get into the future. In particular if state changes, k-armed bandit don’t adapt. It is why we need MDP. . Video examples of MDPs by Adam . By the end of this video: Gain experience formalizing decision-making problems as MDPs, Appreciate the flexibility of the MDP formalism. . Adam uses 2 examples: robot recycling cans and robot arm. . Lesson 2: Goal of Reinforcement Learning . Video the Goal of Reinforcement Learning by Adam. By the end of this video: Describe how rewards relate to the goal of an agent, Identify episodic tasks. . With MDP, agents can have long-term goals. . Video the Reward Hypothesis by Michael Littman. . He gives a nice idea when defining reward hypothesis: a contrast between the simplicity of the idea of rewards with the complexity of the real world. . Lesson 3: Continuing Tasks . Video Continuing Tasks by Martha. By the end of this video: Differentiate between episodic and continuing tasks. Formulate returns for continuing tasks using discounting. Describe how returns at successive time steps are related to each other. . Adam uses a link to Sutton’s book., This is a 2020 version of this book. . Video Examples of Episodic and Continuing Tasks by Martha. By the end of this video: Understand when to formalize a task as episodic or continuing. . Martha gives 2 examples: one of an episodic tasks where episode ends when player is touched by an enemy, one of continuous tasks where an agent accepts or rejects tasks depending on priority and servers available (never ending episode). . Weekly assessment. . This is a quizz and a peer-graded assignment. I had to describe 3 MDPs with all its detail (states actions, rewards). . 5/10/21 - Course 1 - Week 3 - Value Functions &amp; Bellman Equations . Module 3 Learning Objectives . Lesson 1: Policies and Value Functions . Recognize that a policy is a distribution over actions for each possible state | Describe the similarities and differences between stochastic and deterministic policies | Identify the characteristics of a well-defined policy | Generate examples of valid policies for a given MDP | Describe the roles of state-value and action-value functions in reinforcement learning | Describe the relationship between value functions and policies | Create examples of valid value functions for a given MDP | . Lesson 2: Bellman Equations . Derive the Bellman equation for state-value functions | Derive the Bellman equation for action-value functions | Understand how Bellman equations relate current and future values | Use the Bellman equations to compute value functions | . Lesson 3: Optimality (Optimal Policies &amp; Value Functions) . Define an optimal policy | Understand how a policy can be at least as good as every other policy in every state | Identify an optimal policy for given MDPs | Derive the Bellman optimality equation for state-value functions | Derive the Bellman optimality equation for action-value functions | Understand how the Bellman optimality equations relate to the previously introduced Bellman equations | Understand the connection between the optimal value function and optimal policies | Verify the optimal value function for given MDPs | . Lesson 1: Policies and Value Functions . Reading chapter 3.5 to 3.8 (p58-67) in Sutton’s book . Almost all reinforcement learning algorithms involve estimating value functions—functions of states (or of state–action pairs) that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state). . Searching for additional informations, I have fallen into ShangtongZhang page and repos. Only 2 of them but seem to be great: reinforcement-learning-an-introduction contains implementations in Python of all concepts from Sutton’s book. DeepRL seems to be a pytorch implementations (DQN, A2C, PPO, …) . Here we see Bellman equation for state-value function $v_ pi(s)$ . vπ(s)≐E[Gt∣St=s]vπ(s)=∑aπ(a∣s)∑s′,rp(s′,r∣s,a)[r+γ.vπ(s′)]v_ pi(s) doteq mathbb{E}[G_t|S_t=s] v_ pi(s) = displaystyle sum_{a} pi(a|s) displaystyle sum_{s&amp;#x27;,r} p(s&amp;#x27;, r|s, a) big[r+ gamma.v_ pi(s&amp;#x27;) big]vπ​(s)≐E[Gt​∣St​=s]vπ​(s)=a∑​π(a∣s)s′,r∑​p(s′,r∣s,a)[r+γ.vπ​(s′)] . Bellman equation for action-value function $q_ pi(s,a)$ . qπ(s,a)≐E[Rt+1+γ.Gt+1∣St=s,At=a]qπ(s,a)=∑s′,rp(s′,r∣s,a)[r+γ∑a′π(s′,a′)qπ(s′,a′)]q_ pi(s,a) doteq mathbb{E}[R_{t+1}+ gamma.G_{t+1}|S_t=s, A_t=a] q_ pi(s, a) = displaystyle sum_{s&amp;#x27;,r} p(s&amp;#x27;, r|s, a) big[ r + gamma displaystyle sum_{a&amp;#x27;} pi(s&amp;#x27;, a&amp;#x27;)q_ pi(s&amp;#x27;,a&amp;#x27;) big]qπ​(s,a)≐E[Rt+1​+γ.Gt+1​∣St​=s,At​=a]qπ​(s,a)=s′,r∑​p(s′,r∣s,a)[r+γa′∑​π(s′,a′)qπ​(s′,a′)] . Optimal state-value function $v_*$: . v∗(s)≐max⁡πvπ(s),∀s∈Sv_*(s) doteq max limits_{ pi} v_ pi(s), forall s in Sv∗​(s)≐πmax​vπ​(s),∀s∈S . Optimal action-value function $q_*$: q∗(s,a)≐max⁡πqπ(s,a)=E[Rt+1+γ.v∗(St+1)∣St=s,At=a]q_*(s,a) doteq max limits_{ pi} q_ pi(s,a) = mathbb{E}[R_{t+1}+ gamma.v_*(S_{t+1})|S_t=s, A_t=a]q∗​(s,a)≐πmax​qπ​(s,a)=E[Rt+1​+γ.v∗​(St+1​)∣St​=s,At​=a] . We denote all optimal policies by $ pi_*$ . Bellman optimality equation for $v_*$ . v∗(s)=max⁡a∑s′,rp(s′,r∣s,a)[r+γ.v∗(s′)]v_*(s) = max limits_{a} displaystyle sum_{s&amp;#x27;, r} p(s&amp;#x27;,r|s, a) big[ r + gamma .v_*(s&amp;#x27;) big]v∗​(s)=amax​s′,r∑​p(s′,r∣s,a)[r+γ.v∗​(s′)] . Bellman optimality equation for $q_*$ . q∗(s,a)=∑s′,rp(s′,r∣s,a)[r+γ.max⁡a′q∗(s′,a′)]q_*(s,a) = displaystyle sum_{s&amp;#x27;,r} p(s&amp;#x27;, r|s, a) big[ r + gamma. max limits_{a&amp;#x27;} q_*(s&amp;#x27;,a&amp;#x27;) big]q∗​(s,a)=s′,r∑​p(s′,r∣s,a)[r+γ.a′max​q∗​(s′,a′)] . . Video Specifying Policies by Adam. . By the end of this video, you’ll be able to . Recognize that a policy is a distribution over actions for each possible state, describe the similarities and differences between stochastic and deterministic policies, and generate examples of valid policies for a given MDP or Markup Decision Process. . Video Value Functions by Adam. . By the end of this video, you’ll be able to . describe the roles of the state-value and action-value functions in reinforcement learning, describe the relationship between value-functions and policies, and create examples of value-functions for a given MDP. . Video Rich Sutton and Andy Barto: A brief History of RL . Lesson 2: Bellman Equations . Video Bellman Equation Derivation by Martha . By the end of this video, you’ll be able to derive the Bellman equation for state-value functions, derive the Bellman equation for action-value functions, and understand how Bellman equations relate current and future values. . Video Why Bellman Equations? by Martha . By the end of this video, you’ll be able to use the Bellman equations to compute value functions . Lesson 3: Optimality (Optimal Policies &amp; Value Functions) . Video Optimal Policies by Martha . By the end of this video, you will be able to define an optimal policy, understand how policy can be at least as good as every other policy in every state, and identify an optimal policy for a given MDP. . Video Optimal Value Functions by Martha . By the end of this video, you will be able to derive the Bellman optimality equation for the state-value function, derive the Bellman optimality equation for the action-value function, and understand how the Bellman optimality equations relate to the previously introduced Bellman equations. . Video Using Optimal Value Functions to Get Optimal Policies by Martha . By the end of this video, you’ll be able to understand the connection between the optimal value function and optimal policies and verify the optimal value function for a given MDP . Video week 3 summary by Adam . . . . . . 5/18/21 - Course 1 - Week 4 - Dynamic Programming . Module 4 Learning Objectives . Lesson 1: Policy Evaluation (Prediction) . Understand the distinction between policy evaluation and control | Explain the setting in which dynamic programming can be applied, as well as its limitations | Outline the iterative policy evaluation algorithm for estimating state values under a given policy | Apply iterative policy evaluation to compute value functions | . Lesson 2: Policy Iteration (Control) . Understand the policy improvement theorem | Use a value function for a policy to produce a better policy for a given MDP | Outline the policy iteration algorithm for finding the optimal policy | Understand “the dance of policy and value” | Apply policy iteration to compute optimal policies and optimal value functions | . Lesson 3: Generalized Policy Iteration . Understand the framework of generalized policy iteration | Outline value iteration, an important example of generalized policy iteration | Understand the distinction between synchronous and asynchronous dynamic programming methods | Describe brute force search as an alternative method for searching for an optimal policy | Describe Monte Carlo as an alternative method for learning a value function | Understand the advantage of Dynamic programming and “bootstrapping” over these alternative strategies for finding the optimal policy | . Lesson 1: Policy Evaluation (Prediction) . Reading chapter 4.1, 4.2, 4.3, 4.4, 4.6, 4.7 (pages 73-88) in Sutton’s book (with the help of Solutions_to_Reinforcement_Learning_by_Sutton_Chapter_4_r5.pdf) . A common way of obtaining approximate solutions for tasks with continuous states and actions is to quantize the state and action spaces and then apply finite-state DP methods. . Video Policy Evaluation vs. Control by Martha . By the end of this video you will be able to understand the distinction between policy evaluation and control, and explain the setting in which dynamic programming can be applied as well as its limitations. . Video Iterative Policy Evaluation by Martha . By the end of this video you will be able to outline the iterative policy evaluation algorithm for estimating state values for a given policy, and apply iterative policy evaluation to compute value functions. . The magic here is to turn the bellman equation into an iterative evaluation which converges to $v_ pi$. . . Lesson 2: Policy Iteration (Control) . Video Policy Improvement by Marta . By the end of this video, you will be able to understand the policy improvement theorem, and how it can be used to construct improved policies, and use the value function for a policy to produce a better policy for a given MDP. . Greedified policy is a strict improvement. . . Video Policy Iteration by Marta . By the end of this video, you will be able to outline the policy iteration algorithm for finding the optimal policy, understand the dance of policy and value, how policy iteration reaches the optimal policy by alternating between evaluating policy and improving it, and apply policy iteration to compute optimal policies and optimal value functions. . . . Lesson 3: Generalized Policy Iteration . Video Flexibility of the Policy Iteration Framework by Adam . By the end of this video, you’ll be able to understand the framework of generalized policy iteration, outline value iteration and important special case of generalized policy iteration, and differentiate synchronous and asynchronous dynamic programming methods. . Video Efficiency of Dynamic Programming by Adam . By the end of this video, you’ll be able to describe Monte Carlo sampling as an alternative method for learning a value function. Describe brute force-search as an alternative method for finding an optimal policy. And understand the advantages of dynamic programming and bootstrapping over these alternatives. . The most important takeaway is that bootstrapping can save us from performing a huge amount of unnecessary work by exploiting the connection between the value of a state and its possible successors. . Video Warren Powell: Approximate Dynamic Programming for Fleet Management (Short) . Video Week 4 Summary by Adam . Reading chapter summary Chapter 4.8, (pages 88-89) . Assignment . Optimal Policies with Dynamic Programming . notebooks in github . end of C1W4 (course 1 week 4) . end of course 1 (and with a certificate ;) ) .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/reinforcement-learning-specialization-coursera.html",
            "relUrl": "/blog/reinforcement-learning-specialization-coursera.html",
            "date": " • May 3, 2021"
        }
        
    
  
    
        ,"post27": {
            "title": "Logbook for May 21",
            "content": "Week 18 - May 21 . Monday 5/3 . Reinforcement Learning Specialization - enrolled - Coursera (University of Alberta) - start of course 1 (Fundamentals of Reinforcement Learning) week 1 (An introduction to Sequential Decision-Making) . Tuesday 5/4 . Reinforcement Learning Specialization - end of assignement for C1W1 . RL Course by David Silver Policy Gradient Methods (lecture 7) . Friday 5/7 . Reinforcement Learning Specialization - C1W2 - Markov Decision Process . Week 19 - May 21 . Monday 5/10 . Reinforcement Learning Specialization - C1W3 - Value Functions &amp; Bellman Equations . Week 20 - May 21 . Monday 5/17 . Reinforcement Learning Specialization - C1W3 - Value Functions &amp; Bellman Equations . Tuesday 5/18 . Reinforcement Learning Specialization - C1W4 - Dynamic Programming . Friday 5/21 . Reinforcement Learning Specialization - C1W4 - Dynamic Programming - assignment completed and end of course 1 ;) . Start of Machine learning in python with scikit-learn by Inria . Week 21 - May 21 . Tuesday 5/25 . Machine learning in python with scikit-learn end of module 1 . Reinforcement Learning Specialization - C2W1 - Monte-Carlo . Wednesday 5/26 . Reinforcement Learning Specialization - C2W1 - Monte-Carlo - end of week 1 . Thursday 5/27 . Machine learning in python with scikit-learn start of module 2. Selecting the best model . Reinforcement Learning Specialization - C2W2 - Temporal Difference for Prediction - start . Week 22 - May 21 . Monday 5/31 . Machine learning in python with scikit-learn start of module 3. Hyperparameter tuning .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/logbook-May.html",
            "relUrl": "/blog/logbook-May.html",
            "date": " • May 1, 2021"
        }
        
    
  
    
        ,"post28": {
            "title": "setup wsl2 with cuda and conda",
            "content": "wsl2 and network + proxychains . workaround explained in this blog entry . wsl -d Ubuntu-20.04 sudo ~/Applications/wsl-vpnkit/wsl-vpnkit-main/wsl-vpnkit . cuda . https://docs.nvidia.com/cuda/wsl-user-guide/index.html#installing-nvidia-drivers . install nvidia cuda specific driver for WSL: https://developer.nvidia.com/cuda/wsl on windows. (version 470.14_quadro_win10-dch_64bit_international in my case) . proxychains wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600 sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/7fa2af80.pub sudo proxychains add-apt-repository &quot;deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /&quot; sudo proxychains apt-get update sudo proxychains apt-get -y install cuda-toolkit-11-2 . https://christianjmills.com/Using-PyTorch-with-CUDA-on-WSL2/ . new version using WSL-ubuntu as distro . https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=WSLUbuntu&amp;target_version=20&amp;target_type=deblocal . proxychains wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin sudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600 proxychains wget https://developer.download.nvidia.com/compute/cuda/11.2.2/local_installers/cuda-repo-wsl-ubuntu-11-2-local_11.2.2-1_amd64.deb sudo dpkg -i cuda-repo-wsl-ubuntu-11-2-local_11.2.2-1_amd64.deb sudo apt-key add /var/cuda-repo-wsl-ubuntu-11-2-local/7fa2af80.pub sudo proxychains apt-get update sudo proxychains apt-get -y install cuda . test cuda . conda activate pytorch ipython import torch torch.cuda.is_available() . conda . from https://docs.conda.io/en/latest/miniconda.html . download https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh . and install with ./Miniconda3-latest-Linux-x86_64.sh -p $HOME/miniconda3 . pycaret . conda create --name pycaret python=3.7 conda activate pycaret proxychains pip install pycaret shap proxychains conda install -c conda-forge nb_conda jupyter_contrib_nbextensions fire pyfiglet openpyxl jupyter contrib nbextensions install --user proxychains conda upgrade nbconvert . pytorch . proxychains conda create -n pytorch python=3.8 proxychains conda activate pytorch proxychains conda install -c pytorch pytorch=1.7.1 torchvision proxychains conda install jupyter proxychains conda install -c conda-forge jupyter_contrib_nbextensions .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/wsl2-cuda-conda.html",
            "relUrl": "/blog/wsl2-cuda-conda.html",
            "date": " • Apr 9, 2021"
        }
        
    
  
    
        ,"post29": {
            "title": "using SOCKS5 proxy - with git, apt, pip, ...",
            "content": "setup socks5 server . using dante server . Installation . sudo apt-get install dante-server . Conf file . sudo nano /etc/danted.conf logoutput: stderr internal: enp3s0 port = 1080 external: enp3s0 socksmethod: none clientmethod: none user.privileged: proxy user.unprivileged: nobody user.libwrap: nobody client pass { from: 0.0.0.0/0 to: 0.0.0.0/0 log: error connect disconnect } client block { from: 0.0.0.0/0 to: 0.0.0.0/0 log: connect error } socks pass { from: 0.0.0.0/0 to: 0.0.0.0/0 log: error connect disconnect } socks block { from: 0.0.0.0/0 to: 0.0.0.0/0 log: connect error } . Start and monitor usage . sudo service danted restart tail -f /var/log/syslog . Git setup . $ cat .ssh/config Host github.com IdentityFile ~/.ssh/id_rsa_gmail ProxyCommand /bin/nc -X 5 -x 192.168.50.202:1080 %h %p . Proxychains . installation . # to be downloaded from apt mirrors: # libproxychains proxychains sudo dpkg -i libproxychains3_3.1-7_amd64.deb proxychains_3.1-7_all.deb . configuration . sudo vi /etc/proxychains.conf [ProxyList] # add proxy here ... # meanwile # defaults set to &quot;tor&quot; socks5 192.168.50.202 1080 . usage . sudo proxychains apt update sudo proxychains apt upgrade proxychains pip install pycaret .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/git-avec-proxy-socks.html",
            "relUrl": "/blog/git-avec-proxy-socks.html",
            "date": " • Apr 6, 2021"
        }
        
    
  
    
        ,"post30": {
            "title": "Logbook for April 21",
            "content": "Week 13 - Apr 21 . Thursday 4/1 . Aniti RLVS - Deep Q-Networks and its variants . Collège de France - Algorithmes quantiques : quand la physique quantique défie la thèse de Church-Turing Leçon inaugurale . Friday 4/2 . Aniti RLVS - From Policy Gradients to Actor Critic methods . Aniti RLVS - Policy Gradient in pratice . git to use socks server to github (to go through local firewall) . Aniti RLVS - Exploration in Deep RL . Week 14 - Apr 21 . Wednesday 4/7 . tabnet: pytorch (and fastai with Zach Mueller) implementations . Thursday 4/8 . Aniti RLVS - Evolutionary Reinforcement Learning . Jupyter notebook turned into slides with RISE . Aniti RLVS - Micro-data Policy Search . Friday 4/9 . Aniti RLVS - RL in Practice: Tips and Tricks and Practical Session With Stable-Baselines3 . setup wsl-vpnkit to workaround wsl2 and network issues (explained here) . install wsl2-cuda-conda . Week 15 - Apr 21 . Thursday 4/15 . MIT 6S191 Learning for Information Extraction (lecture 9). . Week 16 - Apr 21 . Tuesday 4/20 . How To run macOS on KVM / QEMU . Wednesday 4/21 . Enrolled to Machine learning in python with scikit-learn by scikit-learn team! . Audio classification with pytorch . Week 17 - Apr 21 . Monday 4/26 . datacamp - Bayesian Data Analysis in Python, at the end, Think Bayes 2 by Allen B.Downey . Tuesday 4/27 . RL Course by David Silver Value Function Approximation (lecture 6) . MIT 6S191 Taming Dataset Bias (lecture 10) . Wednesday 4/28 . datacamp - Exploratory Data Analysis in Python, by Allen B.Downey (that’s why) . Thursday 4/29 . datacamp - Working with date and time, quite interesting to see timezones considerations. . Friday 4/30 . datacamp - Cleaning data. Impressed by fuzzywuzzy and recordlinkage packages. End of this track (Data Scientist - new version) . MIT 6S191 Towards AI for 3D Content Creation (lecture 11) and AI in Healthcare (lecture 12) and this is the end of this course .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/logbook-April.html",
            "relUrl": "/blog/logbook-April.html",
            "date": " • Apr 1, 2021"
        }
        
    
  
    
        ,"post31": {
            "title": "ANITI's first Reinforcement Learning Virtual School",
            "content": ". https://rlvs.aniti.fr/ . Schedule is . RLVS schedule . This condensed schedule does not include class breaks and social events. Times are Central European Summer Time (UTC+2). . Schedule       . March 25th | 9:00-9:10 | Opening remarks | S. Gerchinovitz | .   | 9:10-9:30 | RLVS Overview | E. Rachelson | .   | 9:30-13:00 | RL fundamentals | E. Rachelson | .   | 14:00-16:00 | Introduction to Deep Learning | D. Wilson | .   | 16:30-17:30 | Reward Processing Biases in Humans and RL Agents | I. Rish | .   | 17:45-18:45 | Introduction to Hierarchical Reinforcement Learning | D. Precup | . March 26th | 10:00-12:00 | Stochastic bandits | T. Lattimore | .   | 14:00-16:00 | Monte Carlo Tree Search | T. Lattimore | .   | 16:30-17:30 | Multi-armed bandits in clinical trials | D. A. Berry | . April 1st | 9:00-15:00 | Deep Q-Networks and its variants | B. Piot, C. Tallec | .   | 15:15-16:15 | Regularized MDPs | M. Geist | .   | 16:30-17:30 | Regret bounds of model-based reinforcement learning | M. Wang | . April 2nd | 9:00-12:30 | Policy Gradients and Actor Critic methods | O. Sigaud | .   | 14:00-15:00 | Pitfalls in Policy Gradient methods | O. Sigaud | .   | 15:30-17:30 | Exploration in Deep RL | M. Pirotta | . April 8th | 9:00-11:00 | Evolutionary Reinforcement Learning | D. Wilson, J.-B. Mouret | .   | 11:30-12:30 | Evolving Agents that Learn More Like Animals | S. Risi | .   | 14:00-16:00 | Micro-data Policy Search | K. Chatzilygeroudis, J.-B. Mouret | .   | 16:30-17:30 | Efficient Motor Skills Learning in Robotics | D. Lee | . April 9th | 9:00-13:00 | RL tips and tricks | A. Raffin | .   | 14:30-15:30 | Symbolic representations and reinforcement learning | M. Garnelo | .   | 15:45-16:45 | Leveraging model-learning for extreme generalization | L. P. Kaelbling | .   | 17:00-18:00 | RLVS wrap-up | E. Rachelson | . (4/1/21) - Deep Q-Networks and its variants . Speaker is Bilal Piot. . Deep Q network as a solution for a practicable control theory. . Introduction of ALE (Atari Learning Environment) . DQN is (almost) end-to-end: from raw observations to actions. Bilal explains the preprocessing part (from 160x210x3 to 84x84 + stacking 4 frames + downsampling to 15 Hz) . Value Iteration (VI) algorithm: Recurrent algorithm to get Q. $Q_{k+1}=T^*Q$ . But it is not practical in a real-world case. What we can do is use interactions with real world. And estimate $Q^*$ using a regression. . Would be interesting to have slides. I like the link between regression notations and VI notation. . From neural Fitted-$Q$ to DQN. Main difference is data collection (in DQN you have updated interactions and it allows exploration, and size of architecture) . With DQN we have acting part and learning part. Acting is the data collection. (using $ epsilon$-greedy policy) . hands-on based on DQN tutorial notebook. . had to export LD_LIBRARY_PATH=/home/explore/miniconda3/envs/aniti/lib/ . Nice introduction to JAX and haiku. Haiku is similar modules in pytorch and can turn NN into pure version. Which is useful for Jax. . overview of the literature . . (4/2/21) - From Policy Gradients to Actor Critic methods . Olivier Sigaud is the speaker. . He has pre-recorded his lecture in videos. I have missed the start so I will have to watch them later. . Policy Gradient in pratice . Don’t become an alchemist ;) . As stochastic policies, squashed gaussian is interesting because it allows continuous variable + bounds. . Exploration in Deep RL . (4/8/21) - Evolutionary Reinforcement Learning . pdf version of the slides are available here . then Evolving Agents that Learn More Like Animals . This morning was more about what we can do when we have infinite calculation power and data. . Afternoon will be the opposite. . (4/8/21) - Micro-data Policy Search . Most policy search algorithms require thousands of training episodes to find an effective policy, which is often infeasible when experiments takes time or are expensive (for instance, with physical robot or with an aerodynamics simulator). This class focuses on the extreme other end of the spectrum: how can an algorithm adapt a policy with only a handful of trials (a dozen) and a few minutes? By analogy with the word “big-data”, we refer to this challenge as “micro-data reinforcement learning”. We will describe two main strategies: (1) leverage prior knowledge on the policy structure (e.g., dynamic movement primitives), on the policy parameters (e.g., demonstrations), or on the dynamics (e.g., simulators), and (2) create data-driven surrogate models of the expected reward (e.g., Bayesian optimization) or the dynamical model (e.g., model-based policy search), so that the policy optimizer queries the model instead of the real system. Most of the examples will be about robotic systems, but the principle apply to any other expensive setup. . all material: https://rl-vs.github.io/rlvs2021/micro-data.html . (4/9/21) - RL in Practice: Tips and Tricks and Practical Session With Stable-Baselines3 . ​ Abstract: The aim of the session is to help you do reinforcement learning experiments. The first part covers general advice about RL, tips and tricks and details three examples where RL was applied on real robots. The second part will be a practical session using the Stable-Baselines3 library. . Pre-requisites: Python programming, RL basics, (recommended: Google account for the practical session in order to use Google Colab). . Additional material: Website: https://github.com/DLR-RM/stable-baselines3 Doc: https://stable-baselines3.readthedocs.io/en/master/ . Outline: Part I: RL Tips and Tricks / The Challenges of Applying RL to Real Robots . Introduction (3 minutes) | RL Tips and tricks (45 minutes) General Nuts and Bolts of RL experimentation (10 minutes) | RL in practice on a custom task (custom environment) (30 minutes) | Questions? (5 minutes) | | The Challenges of Applying RL to Real Robots (45 minutes) Learning to control an elastic robot - DLR David Neck Example (15 minutes) | Learning to drive in minutes and learning to race in hours - Virtual and real racing car (15 minutes) | Learning to walk with an elastic quadruped robot - DLR bert example (10 minutes) | Questions? (5 minutes+) | | Part II: Practical Session with Stable-Baselines3 . Stable-Baselines3 Overview (20 minutes) | Questions? (5 minutes) | Practical Session - Code along (1h+) | action space . When using continuous space, you need to normalize! (normalized action space -1, -1) . there is a checker for that in stable baselines 3. . reward . start with reward shaping. . termination condition . early stopping makes learning faster (and safer for robots) . . for hyperparameter tuning, Antonin recommends Optuna. . about the Henderson paper: Deep Reinforcement Learning that Matters . . and then the controller will use latent representation / current speed + history as observation space. . Learning to drive takes then 10 min, and to race 2 hours. . handson . slides: https://araffin.github.io/slides/rlvs-sb3-handson/ . notebook: https://github.com/araffin/rl-handson-rlvs21 . RL zoo: https://github.com/DLR-RM/rl-baselines3-zoo . documentation for SB3 usefull for completing exercises: https://stable-baselines3.readthedocs.io/en/master/ . https://excalidraw.com/ .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/Aniti-RLVS-seminaire-RL.html",
            "relUrl": "/blog/Aniti-RLVS-seminaire-RL.html",
            "date": " • Apr 1, 2021"
        }
        
    
  
    
        ,"post32": {
            "title": "Headless raspberry pi: create a wifi to ethernet bridge",
            "content": "After my internet provider router stopped unexpectedly yesterday, I had to find a solution with internet access from phones and raspberry pi to broadcast internet to full home devices. . Headless raspberry pi . Installation on SD from ubuntu . for a reason, raspberry pi imager snap doesn’t work (due to a bug linked to QT+wayland). . I download deb ubuntu version (imager_1.6_amd64.deb) from https://www.raspberrypi.org/software and install with dpkg. (sudo dpkg -i imager_1.6_amd64.deb) . With rpi-imager, I can install by selecting the default OS (raspberry Pi OS 32-bit), and SD card as storage. . Headless wifi . As explained in https://www.raspberrypi.org/documentation/configuration/wireless/headless.md . Create (touch) wpa_supplicant.conf in /boot of SD card and paste this content: . ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 country=FR network={ ssid=&quot;AndroidAP&quot; psk=&quot;&lt;Password for your wireless LAN&gt;&quot; } . Headless ssh . As explained in https://www.raspberrypi.org/documentation/remote-access/ssh/README.md . Create (touch) ssh in /boot of SD card . If it is found, SSH is enabled and the file is deleted. The content of the file does not matter; it could contain text, or nothing at all. . Test installation . Boot. After a couple of minutes, I have a notification on phone saying a device is connected on my phone hotspot. . . And ssh raspberry (default username/password are pi/raspberry) . $ ssh -l pi 192.168.43.179 pi@192.168.43.179&#39;s password: Linux raspberrypi 5.4.83-v7+ #1379 SMP Mon Dec 14 13:08:57 GMT 2020 armv7l The programs included with the Debian GNU/Linux system are free software; the exact distribution terms for each program are described in the individual files in /usr/share/doc/*/copyright. Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent permitted by applicable law. Last login: Thu Mar 25 06:23:17 2021 SSH is enabled and the default password for the &#39;pi&#39; user has not been changed. This is a security risk - please login as the &#39;pi&#39; user and type &#39;passwd&#39; to set a new password. . Headless raspberry is ready to be used. . Wifi to ethernet bridge . I will use https://willhaley.com/blog/raspberry-pi-wifi-ethernet-bridge/ . The only package that is needed is dnsmasq however from a clean install it is a good idea to make sure everything is up-to-date: . get up-to-date system . sudo apt-get update &amp;&amp; sudo apt-get upgrade -y &amp;&amp; sudo apt-get install rpi-update dnsmasq -y sudo rpi-update . Option 1 - Same Subnet . Save this script as a file named bridge.sh on your Pi. . #!/usr/bin/env bash set -e [ $EUID -ne 0 ] &amp;&amp; echo &quot;run as root&quot; &gt;&amp;2 &amp;&amp; exit 1 ########################################################## # You should not need to update anything below this line # ########################################################## # parprouted - Proxy ARP IP bridging daemon # dhcp-helper - DHCP/BOOTP relay agent apt update &amp;&amp; apt install -y parprouted dhcp-helper systemctl stop dhcp-helper systemctl enable dhcp-helper # Enable ipv4 forwarding. sed -i&#39;&#39; s/#net.ipv4.ip_forward=1/net.ipv4.ip_forward=1/ /etc/sysctl.conf # Service configuration for standard WiFi connection. Connectivity will # be lost if the username and password are incorrect. systemctl restart wpa_supplicant.service # Enable IP forwarding for wlan0 if it&#39;s not already enabled. grep &#39;^option ip-forwarding 1$&#39; /etc/dhcpcd.conf || printf &quot;option ip-forwarding 1 n&quot; &gt;&gt; /etc/dhcpcd.conf # Disable dhcpcd control of eth0. grep &#39;^denyinterfaces eth0$&#39; /etc/dhcpcd.conf || printf &quot;denyinterfaces eth0 n&quot; &gt;&gt; /etc/dhcpcd.conf # Configure dhcp-helper. cat &gt; /etc/default/dhcp-helper &lt;&lt;EOF DHCPHELPER_OPTS=&quot;-b wlan0&quot; EOF # Enable avahi reflector if it&#39;s not already enabled. sed -i&#39;&#39; &#39;s/#enable-reflector=no/enable-reflector=yes/&#39; /etc/avahi/avahi-daemon.conf grep &#39;^enable-reflector=yes$&#39; /etc/avahi/avahi-daemon.conf || { printf &quot;something went wrong... n n&quot; printf &quot;Manually set &#39;enable-reflector=yes in /etc/avahi/avahi-daemon.conf&#39; n&quot; } # I have to admit, I do not understand ARP and IP forwarding enough to explain # exactly what is happening here. I am building off the work of others. In short # this is a service to forward traffic from WiFi to Ethernet. cat &lt;&lt;&#39;EOF&#39; &gt;/usr/lib/systemd/system/parprouted.service [Unit] Description=proxy arp routing service Documentation=https://raspberrypi.stackexchange.com/q/88954/79866 Requires=sys-subsystem-net-devices-wlan0.device dhcpcd.service After=sys-subsystem-net-devices-wlan0.device dhcpcd.service [Service] Type=forking # Restart until wlan0 gained carrier Restart=on-failure RestartSec=5 TimeoutStartSec=30 # clone the dhcp-allocated IP to eth0 so dhcp-helper will relay for the correct subnet ExecStartPre=/bin/bash -c &#39;/sbin/ip addr add $(/sbin/ip -4 -br addr show wlan0 | /bin/grep -Po &quot; d+ . d+ . d+ . d+&quot;)/32 dev eth0&#39; ExecStartPre=/sbin/ip link set dev eth0 up ExecStartPre=/sbin/ip link set wlan0 promisc on ExecStart=-/usr/sbin/parprouted eth0 wlan0 ExecStopPost=/sbin/ip link set wlan0 promisc off ExecStopPost=/sbin/ip link set dev eth0 down ExecStopPost=/bin/bash -c &#39;/sbin/ip addr del $(/sbin/ip -4 -br addr show wlan0 | /bin/grep -Po &quot; d+ . d+ . d+ . d+&quot;)/32 dev eth0&#39; [Install] WantedBy=wpa_supplicant.service EOF systemctl daemon-reload systemctl enable parprouted systemctl start parprouted dhcp-helper . Step 2: Execute the script on your Pi like so. . sudo bash bridge.sh . Step 3: Reboot. . sudo reboot . Done! .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/headless-raspberry-pi-bridge-network.html",
            "relUrl": "/blog/headless-raspberry-pi-bridge-network.html",
            "date": " • Mar 25, 2021"
        }
        
    
  
    
        ,"post33": {
            "title": "Git - How To Contribute To A Project",
            "content": "Based on http://qpleple.com/how-to-contribute-to-a-project-on-github/ . Using clustergit as an example . Fork . Make your own working copy of the project by forking it: go on the project page (https://github.com/mnagel/clustergit) and click “Fork”. You can access you copy at: https://github.com/castorfou/clustergit . Clone . Clone your fork git repository on your local computer: . git clone git@github.com:castorfou/clustergit.git . Branch . git branch master-to-main git checkout master-to-main . This is very important, create one branch per patch. And never submit a patch that has been done on the branch master or main! . Develop . Here I want to reflect change from Oct/20 where default branch name in github is now main . sed -i &#39;s/master/main/g&#39; clustergit . Commit . git add -u git commit -m &quot;default branch name &#39;main&#39;&quot; . Push to github . git push origin master-to-main . Create pull request . Go on your fork page (https://github.com/castorfou/clustergit), then select master-to-main in the branch list and click “Pull Request”. . Submit patch . Check the diff, write a message explaining what you have done and why the repository owner should accept your pull request and submit. .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/contribute-to-a-project-with-git.html",
            "relUrl": "/blog/contribute-to-a-project-with-git.html",
            "date": " • Mar 25, 2021"
        }
        
    
  
    
        ,"post34": {
            "title": "Stable baselines 3 - 1st steps",
            "content": "What is stable baselines 3 (sb3) . I have just read about this new release. This is a complete rewrite of stable baselines 2, without any reference to tensorflow, and based on pytorch (&gt;1.4+). . There is a lot of running implementations of RL algorithms, based on gym. A very good introduction in this blog entry . Stable-Baselines3: Reliable Reinforcement Learning Implementations | Antonin Raffin | Homepage . Links . GitHub repository: https://github.com/DLR-RM/stable-baselines3 . | Documentation: https://stable-baselines3.readthedocs.io/ . | RL Baselines3 Zoo: https://github.com/DLR-RM/rl-baselines3-zoo . | Contrib: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib . | RL Tutorial: https://github.com/araffin/rl-tutorial-jnrr19 . | . My installation . Standard installation . conda create --name stablebaselines3 python=3.7 conda activate stablebaselines3 pip install stable-baselines3[extra] conda install -c conda-forge jupyter_contrib_nbextensions conda install nb_conda . !conda list . # packages in environment at /home/explore/miniconda3/envs/stablebaselines3: # # Name Version Build Channel _libgcc_mutex 0.1 main _pytorch_select 0.1 cpu_0 absl-py 0.12.0 pypi_0 pypi atari-py 0.2.6 pypi_0 pypi attrs 20.3.0 pyhd3deb0d_0 conda-forge backcall 0.2.0 pyh9f0ad1d_0 conda-forge backports 1.0 py_2 conda-forge backports.functools_lru_cache 1.6.1 py_0 conda-forge blas 1.0 mkl bleach 3.3.0 pyh44b312d_0 conda-forge box2d 2.3.10 pypi_0 pypi box2d-py 2.3.8 pypi_0 pypi ca-certificates 2021.1.19 h06a4308_1 cachetools 4.2.1 pypi_0 pypi certifi 2020.12.5 py37h06a4308_0 cffi 1.14.5 py37h261ae71_0 chardet 4.0.0 pypi_0 pypi cloudpickle 1.6.0 pypi_0 pypi cudatoolkit 11.0.221 h6bb024c_0 cycler 0.10.0 pypi_0 pypi decorator 4.4.2 py_0 conda-forge defusedxml 0.7.1 pyhd8ed1ab_0 conda-forge entrypoints 0.3 pyhd8ed1ab_1003 conda-forge fire 0.4.0 pyh44b312d_0 conda-forge freetype 2.10.4 h5ab3b9f_0 future 0.18.2 pypi_0 pypi google-auth 1.28.0 pypi_0 pypi google-auth-oauthlib 0.4.3 pypi_0 pypi grpcio 1.36.1 pypi_0 pypi gym 0.18.0 pypi_0 pypi icu 58.2 hf484d3e_1000 conda-forge idna 2.10 pypi_0 pypi importlib-metadata 3.7.3 py37h89c1867_0 conda-forge intel-openmp 2019.4 243 ipykernel 5.5.0 py37h888b3d9_1 conda-forge ipython 7.21.0 py37h888b3d9_0 conda-forge ipython_genutils 0.2.0 py_1 conda-forge jedi 0.18.0 py37h89c1867_2 conda-forge jinja2 2.11.3 pyh44b312d_0 conda-forge jpeg 9b h024ee3a_2 jsonschema 3.2.0 pyhd8ed1ab_3 conda-forge jupyter_client 6.1.12 pyhd8ed1ab_0 conda-forge jupyter_contrib_core 0.3.3 py_2 conda-forge jupyter_contrib_nbextensions 0.5.1 pyhd8ed1ab_2 conda-forge jupyter_core 4.7.1 py37h89c1867_0 conda-forge jupyter_highlight_selected_word 0.2.0 py37h89c1867_1002 conda-forge jupyter_latex_envs 1.4.6 pyhd8ed1ab_1002 conda-forge jupyter_nbextensions_configurator 0.4.1 py37h89c1867_2 conda-forge kiwisolver 1.3.1 pypi_0 pypi lcms2 2.11 h396b838_0 ld_impl_linux-64 2.33.1 h53a641e_7 libffi 3.3 he6710b0_2 libgcc-ng 9.1.0 hdf63c60_0 libmklml 2019.0.5 0 libpng 1.6.37 hbc83047_0 libsodium 1.0.18 h36c2ea0_1 conda-forge libstdcxx-ng 9.1.0 hdf63c60_0 libtiff 4.2.0 h85742a9_0 libuv 1.40.0 h7b6447c_0 libwebp-base 1.2.0 h27cfd23_0 libxml2 2.9.10 hb55368b_3 libxslt 1.1.34 hc22bd24_0 lxml 4.6.3 py37h9120a33_0 lz4-c 1.9.3 h2531618_0 markdown 3.3.4 pypi_0 pypi markupsafe 1.1.1 py37hb5d75c8_2 conda-forge matplotlib 3.3.4 pypi_0 pypi mistune 0.8.4 py37h4abf009_1002 conda-forge mkl 2020.2 256 mkl-service 2.3.0 py37he8ac12f_0 mkl_fft 1.3.0 py37h54f3939_0 mkl_random 1.1.1 py37h0573a6f_0 nb_conda 2.2.1 py37_0 nb_conda_kernels 2.3.1 py37h06a4308_0 nbconvert 5.6.1 py37hc8dfbb8_1 conda-forge nbformat 5.1.2 pyhd8ed1ab_1 conda-forge ncurses 6.2 he6710b0_1 ninja 1.10.2 py37hff7bd54_0 notebook 5.7.10 py37hc8dfbb8_0 conda-forge numpy 1.20.1 pypi_0 pypi numpy-base 1.19.2 py37hfa32c7d_0 oauthlib 3.1.0 pypi_0 pypi olefile 0.46 py37_0 opencv-python 4.5.1.48 pypi_0 pypi openssl 1.1.1j h27cfd23_0 packaging 20.9 pyh44b312d_0 conda-forge pandas 1.2.3 pypi_0 pypi pandoc 2.12 h7f98852_0 conda-forge pandocfilters 1.4.2 py_1 conda-forge parso 0.8.1 pyhd8ed1ab_0 conda-forge pexpect 4.8.0 pyh9f0ad1d_2 conda-forge pickleshare 0.7.5 py_1003 conda-forge pillow 7.2.0 pypi_0 pypi pip 21.0.1 py37h06a4308_0 prometheus_client 0.9.0 pyhd3deb0d_0 conda-forge prompt-toolkit 3.0.18 pyha770c72_0 conda-forge protobuf 3.15.6 pypi_0 pypi psutil 5.8.0 pypi_0 pypi ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge pyasn1 0.4.8 pypi_0 pypi pyasn1-modules 0.2.8 pypi_0 pypi pycparser 2.20 py_2 pyglet 1.5.0 pypi_0 pypi pygments 2.8.1 pyhd8ed1ab_0 conda-forge pyparsing 2.4.7 pyh9f0ad1d_0 conda-forge pyrsistent 0.17.3 py37h4abf009_1 conda-forge python 3.7.10 hdb3f193_0 python-dateutil 2.8.1 py_0 conda-forge python_abi 3.7 1_cp37m conda-forge pytorch 1.7.1 py3.7_cuda11.0.221_cudnn8.0.5_0 pytorch pytz 2021.1 pypi_0 pypi pyyaml 5.3.1 py37hb5d75c8_1 conda-forge pyzmq 19.0.2 py37hac76be4_2 conda-forge readline 8.1 h27cfd23_0 requests 2.25.1 pypi_0 pypi requests-oauthlib 1.3.0 pypi_0 pypi rsa 4.7.2 pypi_0 pypi scipy 1.6.1 pypi_0 pypi send2trash 1.5.0 py_0 conda-forge setuptools 52.0.0 py37h06a4308_0 six 1.15.0 pyh9f0ad1d_0 conda-forge sqlite 3.35.2 hdfb4753_0 stable-baselines3 1.0 pypi_0 pypi tensorboard 2.4.1 pypi_0 pypi tensorboard-plugin-wit 1.8.0 pypi_0 pypi termcolor 1.1.0 py37h06a4308_1 terminado 0.9.3 py37h89c1867_0 conda-forge testpath 0.4.4 py_0 conda-forge tk 8.6.10 hbc83047_0 torchaudio 0.7.2 py37 pytorch torchvision 0.8.2 py37_cu110 pytorch tornado 6.1 py37h4abf009_0 conda-forge traitlets 5.0.5 py_0 conda-forge typing-extensions 3.7.4.3 0 typing_extensions 3.7.4.3 py_0 conda-forge urllib3 1.26.4 pypi_0 pypi wcwidth 0.2.5 pyh9f0ad1d_2 conda-forge webencodings 0.5.1 py_1 conda-forge werkzeug 1.0.1 pypi_0 pypi wheel 0.36.2 pyhd3eb1b0_0 xz 5.2.5 h7b6447c_0 yaml 0.2.5 h516909a_0 conda-forge zeromq 4.3.4 h2531618_0 zipp 3.4.1 pyhd8ed1ab_0 conda-forge zlib 1.2.11 h7b6447c_3 zstd 1.4.5 h9ceee32_0 . SB3 tutorials . import gym from stable_baselines3 import A2C from stable_baselines3.common.monitor import Monitor from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback # Save a checkpoint every 1000 steps checkpoint_callback = CheckpointCallback(save_freq=5000, save_path=&quot;/home/explore/git/guillaume/stable_baselines_3/logs/&quot;, name_prefix=&quot;rl_model&quot;) # Evaluate the model periodically # and auto-save the best model and evaluations # Use a monitor wrapper to properly report episode stats eval_env = Monitor(gym.make(&quot;LunarLander-v2&quot;)) # Use deterministic actions for evaluation eval_callback = EvalCallback(eval_env, best_model_save_path=&quot;/home/explore/git/guillaume/stable_baselines_3/logs/&quot;, log_path=&quot;/home/explore/git/guillaume/stable_baselines_3/logs/&quot;, eval_freq=2000, deterministic=True, render=False) # Train an agent using A2C on LunarLander-v2 model = A2C(&quot;MlpPolicy&quot;, &quot;LunarLander-v2&quot;, verbose=1) model.learn(total_timesteps=20000, callback=[checkpoint_callback, eval_callback]) # Retrieve and reset the environment env = model.get_env() obs = env.reset() # Query the agent (stochastic action here) action, _ = model.predict(obs, deterministic=False) . Issues and fix . CUDA error: CUBLAS_STATUS_INTERNAL_ERROR . Downgrade pytorch to 1.7.1 . to avoid RuntimeError: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling cublasCreate(handle) . pip install torch==1.7.1 . RuntimeError: CUDA error: invalid device function . !nvidia-smi . Thu Mar 25 09:13:49 2021 +--+ | NVIDIA-SMI 450.102.04 Driver Version: 450.102.04 CUDA Version: 11.0 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Quadro RTX 4000 Off | 00000000:01:00.0 On | N/A | | N/A 41C P5 18W / N/A | 2104MiB / 7982MiB | 32% Default | | | | N/A | +-+-+-+ +--+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1153 G /usr/lib/xorg/Xorg 162MiB | | 0 N/A N/A 1904 G /usr/lib/xorg/Xorg 268MiB | | 0 N/A N/A 2076 G /usr/bin/gnome-shell 403MiB | | 0 N/A N/A 2697 G ...gAAAAAAAAA --shared-files 54MiB | | 0 N/A N/A 7220 G ...AAAAAAAAA= --shared-files 84MiB | | 0 N/A N/A 57454 G /usr/lib/firefox/firefox 2MiB | | 0 N/A N/A 59274 C ...ablebaselines3/bin/python 1051MiB | +--+ . CUDA version is 11.0 on my workstation. . !nvcc --version . nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2019 NVIDIA Corporation Built on Sun_Jul_28_19:07:16_PDT_2019 Cuda compilation tools, release 10.1, V10.1.243 . !conda install pytorch==1.7.1 torchvision==0.8.2 torchaudio==0.7.2 cudatoolkit=11.0 -c pytorch . Collecting package metadata (current_repodata.json): done Solving environment: done # All requested packages already installed. . Everything seems fine after these updates. . Stable baselines 3 user guide . There is an impressive documentation associated with stable baselines 3. Quickstart . Tips and tricks . This page covers general advice about RL (where to start, which algorithm to choose, how to evaluate an algorithm, …), as well as tips and tricks when using a custom environment or implementing an RL algorithm. . Be familiar with RL, see resource page | read SB3 documentation | do the tutorial | . Tune hyperparameters RL zoo is introduced. It contains some hyperparameter optimization. . RL evaluation We suggest you reading Deep Reinforcement Learning that Matters for a good discussion about RL evaluation. . which algorithm to choose 1st criteria is discrete vs continuous actions. And 2nd is capacity to parallelize training. . Discrete Actions . Discrete Actions - Single Process | . DQN with extensions (double DQN, prioritized replay, …) are the recommended algorithms. We notably provide QR-DQN in our contrib repo. DQN is usually slower to train (regarding wall clock time) but is the most sample efficient (because of its replay buffer). . Discrete Actions - Multiprocessed | . You should give a try to PPO or A2C. . Continuous Actions . Continuous Actions - Single Process | . Current State Of The Art (SOTA) algorithms are SAC, TD3 and TQC (available in our contrib repo). Please use the hyperparameters in the RL zoo for best results. . Continuous Actions - Multiprocessed | . Take a look at PPO or A2C. Again, don’t forget to take the hyperparameters from the RL zoo for continuous actions problems (cf Bullet envs). . Creating a custom env . multiple times there are advices about normalizing: observation and action space. A good practice is to rescale your actions to lie in [-1, 1]. This does not limit you as you can easily rescale the action inside the environment . tips and tricks to reproduce a RL paper . Reinforcement Learning Tips and Tricks — Stable Baselines3 1.1.0a1 documentation . A personal pick (by @araffin) for environments with gradual difficulty in RL with continuous actions:&gt; &gt; 1. Pendulum (easy to solve) . HalfCheetahBullet (medium difficulty with local minima and shaped reward) . | BipedalWalkerHardcore (if it works on that one, then you can have a cookie) . | in RL with discrete actions:&gt; &gt; 1. CartPole-v1 (easy to be better than random agent, harder to achieve maximal performance) . LunarLander . | Pong (one of the easiest Atari game) . | other Atari games (e.g. Breakout) . | Resource page . Reinforcement Learning Resources — Stable Baselines3 1.1.0a1 documentation . Stable-Baselines3 assumes that you already understand the basic concepts of Reinforcement Learning (RL). . However, if you want to learn about RL, there are several good resources to get started: . OpenAI Spinning Up . | David Silver’s course . | Lilian Weng’s blog . | Berkeley’s Deep RL Bootcamp . | Berkeley’s Deep Reinforcement Learning course . | More resources . | . Examples . I will run these examples in 01 -hands-on.ipynb from handson_stablebaselines3 . DQN lunarlander . My module is never landing :( . Note: animated gif created with peek. . PPO with multiprocessing cartpole . . Monitor training using callback . This could be useful when you want to monitor training, for instance display live learning curves in Tensorboard (or in Visdom) or save the best agent. . . Atari game such as pong (A2C with 6 envt) or breakout . . Here the list of valid gym atari environments: https://gym.openai.com/envs/#atari . . pybullet . This is a SDK to real-time collision detection and multi-physics simulation for VR, games, visual effects, robotics, machine learning etc. . https://github.com/bulletphysics/bullet3/ . . We need to install it: pip install pybullet . I don&#39;t have rendering capacity when playing with it. Because robotic is far from my need, I will skip on this one . Hindsight Experience Replay (HER) . using Highway-Env . installation with pip install highway-env . After 1h15m of training, some 1st results: . . And after that some technical stuff such as: . Learning Rate Schedule: start with high value and reduce it as learning goes | Advanced Saving and Loading: how to easily create a test environment to evaluate an agent periodically, use a policy independently from a model (and how to save it, load it) and save/load a replay buffer. | Accessing and modifying model parameters: These functions are useful when you need to e.g. evaluate large set of models with same network structure, visualize different layers of the network or modify parameters manually. | Record a video or make a gif | . Make a GIF of a Trained Agent . pip install imageio . and this time the lander is getting closer to moon but not at all between flags. .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/stable-baselines-3.html",
            "relUrl": "/blog/stable-baselines-3.html",
            "date": " • Mar 24, 2021"
        }
        
    
  
    
        ,"post35": {
            "title": "Git - How to find all *unpushed* commits for all projects in a directory?",
            "content": "Very basic question to help keep my repo clean. . Installation clustergit . clustergit seems a good candidate . . cd ~/Applications git clone git@github.com:mnagel/clustergit.git # add export PATH=&quot;$PATH:$HOME/Applications/clustergit&quot; to ~.bashrc source ~.bashrc . or using .local/bin . cd ~/Applications/ git clone git@github.com:castorfou/clustergit.git cd ~ mkdir -p .local/bin cd .local/bin/ ln -s ~/Applications/clustergit/clustergit . source .profile . Usage clustergit . clustergit status . $ clustergit Scanning sub directories of . ./Deep-Reinforcement-Learning-Hands-On : Changesn . (1/17) ./Deep_reinforcement_learning_Course : Changes ./ReinforcementLearning_references : On branch main, Untracked files ./blog : Untracked files ./d059 : On branch main, Changes ./data-scientist-skills : Clean ./deeplearning_specialization : Clean ./fastai : Changes ./fastai_experiments : Changes ./fastbook : Changes ./gan_specialization : Clean ./hello_nbdev : Clean ./introduction-reinforcement-learning-david-silver: On branch main, Untracked files ./mit_600.2x Introduction to Computational Thinking and Data Science: Clean ./mit_6S191_Intro_to_deep_learning : On branch main, No Changes ./pytorch_tutorial : On branch main, Changes ./squeezebox : On branch main, No Changes Done . clustergit status (detailed) . $ clustergit -v [...] - ./squeezebox -- running LC_ALL=C git status On branch main Your branch is up to date with &#39;origin/main&#39;. nothing to commit, working tree clean ./squeezebox : On branch main, No Changes - ./squeezebox -- Done . clustergit status (less detailed: hide Clean) . $ clustergit -H Scanning sub directories of . ./d059 : On branch main, Changes ./fastai : Changes ./fastai_experiments : Changes ./fastbook : Changes ./introduction-reinforcement-learning-david-silver: On branch main, Untracked files ./mit_6S191_Intro_to_deep_learning : On branch main, No Changes ./pytorch_tutorial : On branch main, Changes ./squeezebox : On branch main, No Changes Done . Clean vs On branch main, No Changes . seems related to branch name. If branch is named master, then clean is displayed. . (Mar/25 21) I have just changed clustergit to have main as default branch name instead of master (github having set main as the new standard) . Rename everything from master to main . Git pull, push . I am not sure I will use it. But allows to recursively launch pull commands to update repos (if no local changes) . Rename branches from main to master . Renaming a branch from github website. . Rename branch main to master from github website . . Update local clones . git branch -m main master git fetch origin git branch -u origin/master master . Rename branches from master to main (I know) . Renaming a branch from github website. . Rename branch master to main from github website . . Update local clones . git branch -m master main git fetch origin git branch -u origin/main main . RabbitVCS . From this page . Installation . sudo apt install rabbitvcs-nautilus . Result . . These overlay icons are not automatically updated (have to hit Ctrl-F5, it is a cache issue?) Which is not a surprise: number of actions are fired based on file modifications, and here status (commited, pushed) is not at all linked to file modifications. The system doesn’t know that overlay icon should be changed because file was not touched. . git-nautilus-icons . Just to check if it works better than RabbitVCS regarding overlay icon cache issue. . No I didn’t manage to make it work. Back to RabbitVCS. . Activate git with GlobalProtect . move from ssh to https, keeping password . $ git remote -v origin git@github.com:castorfou/guillaume_blog.git (fetch) origin git@github.com:castorfou/guillaume_blog.git (push) . move to https://github.com/castorfou/guillaume_blog.git . git remote set-url origin https://github.com/castorfou/guillaume_blog.git . Make Git store the username and password and it will never ask for them. | . git config --global credential.helper store . Save the username and password for a session (cache it); | . git config --global credential.helper cache . and to activate trace . $ GIT_TRACE_PACKET=1 GIT_TRACE=1 GIT_CURL_VERBOSE=1 git fetch . we can enrich certificates with Global Protect CA . ~/anaconda3/ssl$ sudo cp certPG.pem /etc/ssl/certs/ . Add a ca-certificate in ubuntu . Go to /usr/local/share/ca-certificates/ | Create a new folder, i.e. sudo mkdir school | Copy the . crt file into the school folder. | Make sure the permissions are OK (755 for the folder, 644 for the file) | Run sudo update-ca-certificates | We should see effects in /etc/ssl/certs . /etc/ssl/certs$ ll -tr [..] lrwxrwxrwx 1 root root 86 mars 24 10:02 cert_M_X5C_sase-net-sslfwd-trust-ca.pem -&gt; /usr/local/share/ca-certificates/globalprotect/cert_M_X5C_sase-net-sslfwd-trust-ca.crt lrwxrwxrwx 1 root root 39 mars 24 10:02 0dc7de9e.0 -&gt; cert_M_X5C_sase-net-sslfwd-trust-ca.pem .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/clustergit.html",
            "relUrl": "/blog/clustergit.html",
            "date": " • Mar 9, 2021"
        }
        
    
  
    
        ,"post36": {
            "title": "Introduction to Reinforcement Learning with David Silver",
            "content": "This classic 10 part course, taught by Reinforcement Learning (RL) pioneer David Silver, was recorded in 2015 and remains a popular resource for anyone wanting to understand the fundamentals of RL. . Website with 10 lectures: videos and slides . My repo with slides . . 3/9/21 - Lecture 1: Introduction to Reinforcement Learning . This introduction is essentially about giving examples of RL to have a good intuition about this field and to provide definitions or context: . Definitions: rewards, actions, agent, environment, state (and history) | Major components: policy, value function, model | Categorizing RL agents (taxonomy): value based, policy based, actor critic, model free, model based | Learning and planning | Prediction and control | . And David gives 2 references: . well known Introduction to Reinforcement Learning, Sutton and Barto, 1998 | Algorithms for Reinforcement Learning, Szepesvari. Available online. | . Policy π piπ(s): essentially a map from state to action. Can be deterministic π piπ(s) or stochastic π piπ(a|s). . Value function v$ pi$(s): is a prediction of expected future reward. . Model: it is not the environment itself but useful to predict what the environment will do next. 2 types of models: transitions model and rewards model. Transition model predicts the next state (e.g. based on dynamics). Reward model predicts the next immediate reward. . A lot of algorithms are model-free and doesn’t require these models. It is a fundamental distinctions in RL. . . And then David explains 2 fundamental different problems with Learning vs Planning. . With Learning, environment is unknown, agent interacts directly with the environment and improves its policy. . With Planning, a model of environment is known, and agent “plays” with this model and improves its policy. . These 2 problems may be linked where you start to learn from the environment and apply planning then. . 2 examples based on atari games. . Another topic is exploration vs exploitation then prediction and control. . 3/10/21 - Lecture 2: Markov Decision Processes . Markov decision processes formally describe an environment for reinforcement learning. . Markov property: the future is independent of the past given the present. . Markov Process (or Markov Chain) is the tuple (S, P) . . We can take sample episodes from this chain. (e.g. C1 FB FB C1 C2 C3 Pub C1 FB FB FB C1 C2 C3 Pub C2 Sleep) . We can formalize the transition matrix from s to s’. . When you add reward you get Markov reward process (S, P, R, γ gammaγ) . Reward here is a function to map for each state the immediate reward. . γ gammaγ is the discounted factor, ϵ epsilonϵ [0,1]. David explains why we could need such discount. . Return Gt is the total discounted reward at time-step t for a given sample. . . Value function v(s) is really what we care about, it is the long-term value of state s. . . Bellman Equation for MRPs . The value function can be decomposed into two parts: . immediate reward Rt+1 | discounted value of next state γ gammaγ.v (St+1) | . . We use that to calculate value function with γ gammaγ $ neq$ 0. . And calculating value function can be seen as the resolution of this linear equation: . . And now we introduce actions and it gives Markov Decision Process . . And we introduce policy . . Then we can define the state-value function v$ pi$(s,a) for a given policy π piπ . . and action-value function q$ pi$(s,a) for a given policy π piπ . . And impact on Bellman Equation ends like that: . . v is giving us how good it is to be in a state. q is giving us how good is it to take an action. . And then we have the Bellman equation expressed with v and q. . We don’t care much about a given v$ pi$, we want to get the best policy. And ultimately to get q* which is the optimal action value function. . . The optimal value function specifies the best possible performance in the MDP. A MDP is “solved” when we know the optimal value function q*. . What we really care about is optimal policy π piπ*. There is a partial ordering about policies. And a theorem saying that for any MDP, there exists at least one optimal policy. . So the optimal value function calculation is similar to what we did earlier when we averaged the value of the next state but now we take the max instead of average. . So no we can write the Bellman Optimality Equation. Unfortunately this is non-linear. . There are many approaches such as iterative ones. . Value Iteration | Policy Iteration | Q-learning | Sarsa | . 3/12/21 - Lecture 3: Planning by Dynamic Programming . Will discuss from the agent side: how to solve these MDP problems. . David starts with general ideas on dynamic programming. (programming in a sense of policy) . Value function is an important idea for RL because it sotres valuable information that you can later reuse (it embeds solutions). And Bellman equation gives the recursive decomposition. . Planning by Dynamic Programming . We assume full knowledge of the MDP. Dynamic programming is used for planning in an MDP. With 2 usages: . prediction: given MDP and policy π piπ, we predict the value of this policy v$ pi$. | control: given MDP, we get optimal value function v&amp;ast; and optimal policy $ pi$&amp;ast;. | . And by full MDP it would mean for an atari game to have access to internal code to calculate everything. . We need the 2 aspects to solve MDP: prediction to value policy, and control to get the best one. . Policy Evaluation . Problem: evaluate a given policy π Solution: iterative application of Bellman expectation backup . (Bellman expectation is used in prediction, Bellman optimality is used in control) . David takes an example with a small grid-world and calculates iteratively (k=0, 1, 2, …) v(s) for a uniform random policy (north, south, east, west with prob 0.25) (left column). And then we follow policy greedily using v function. (right column) . Policy Iteration . In small grid-world example, just by evaluating the policy and act greedily were sufficient to get the optimal policy. This is not generally the case. In general, need more iterations of evaluation (iterative policy evaluation) / improvement (greedy policy). But this process of policy iteration always converges to π∗ . David uses Jack’s Car Rental where it needs 4 steps to get the optimal policy. And explains why acting greedy improves the policy. And if improvement stops, Bellman optimality equation is satisfied, we have our optimal policy. . Some question then about convergence of v$ pi$ . Why not update policy at each step of evaluation -&gt; this is value iteration. . Value Iteration . Problem: find optimal policy π Solution: iterative application of Bellman optimality backup . Extensions to dynamic programming . DP uses full-width backups. It is effective for medium-sized problems. Curse of dimensionality for large problems. Even one backup can be too expensive. . One solution is to sample backups. . Advantages: Model-free: no advance knowledge of MDP required Breaks the curse of dimensionality through sampling Cost of backup is constant, independent of n = |S| . 3/15/21 - Lecture 4: Model-Free Prediction . Model-Free: no-one gives us the MDP. And we still want to solve it. . Monte-Carlo learning: basically methods which goes all the way to the end of trajectory and estimates value by looking at sample returns. . | Temporal-Difference learning: goes one step ahead and estimates after one step | TD(λ lambdaλ): unify both approaches | . We give up the assumption giving how the environment works (which is highly unrealistic for interesting problems). We break it down in 2 pieces (as with previous lecture with planning): . policy evaluation case (this lecture) - how much reward we get from that policy (in model-free envt) | control (next lecture) - find the optimum value function and then optimum policy | . Monte-Carlo Reinforcement Learning . We go all the way through the episodes and we take sample returns. So the estimated value function can be the average of all returns. You have to terminate to perform this mean. . It means we use the empirical mean return in place of expected return. (by law of large numbers, this average returns will converge to value function as the number of episodes for that state tends to infinity) . Temporal-Difference Reinforcement Learning . TD learns from incomplete episodes, by bootstrapping . David takes an example from Sutton about predicting time to commute home, comparing MC and TD. . TD target (Rt+1+γ gammaγVt+1) is biased estimate of v$ pi$(St), but has lower variance than the return Gt. . David compares perf of MC, TD(0), … using Random Walk example and different values of α alphaα. . 3/18/21 - Lecture 5: Model-Free Control . Distinction between on-policy (learning by doing the job) and off-policy (following someone else behavior) . on-policy . In Monte-Carlo approach, we have 2 issues. First is that we don’t have access to model so we should use Q(s, a) instead of v(s). Second is lack of exploration so we should use ϵ epsilonϵ-greedy policy. . With GLIE (Greedy in the Limit with Infinite Exploration), we can update Q after each episodes. . We will now use TD: . Natural idea: use TD instead of MC in our control loop . Apply TD to Q(S, A) | Use ϵ epsilonϵ-greedy policy improvement | Update every time-step | . This is SARSA update. Every single time-step we update our diagram. . A generalisation is n-step Sarsa. n=1 is standard Sarsa. n=∞ infty∞ is MC. . To get the best of both worlds, we consider Sarsa(λ lambdaλ). We have a forward version . . And a backward version which allows online experience. Thanks to eligibility traces. . off-policy . Why is this important? . Learn from observing humans or other agents | Re-use experience generated from old policies π 1 , π 2 , …, π t−1 | Learn about optimal policy while following exploratory policy | Learn about multiple policies while following one policy | . We can apply it in importance sampling for off-policy. With Monte-Carlo it is however useless due to high variance. It is imperative to to TD. . We can apply that to Q-learning. We can use greedy slection on target policy π piπ and ϵ epsilonϵ greedy on behaviour policy μ muμ. . 4/27/21 - Lecture 6: Value Function Approximation . How to scale up value function approach. . Value Function Approximation . So far we have represented value function by a lookup table Every state s has an entry V (s) Or every state-action pair s, a has an entry Q(s, a) . Solution for large MDPs: Estimate value function with function approximation v̂ (s, w) ≈ v π (s) or q̂(s, a, w) ≈ q π (s, a) Generalise from seen states to unseen states Update parameter w using MC or TD learning . There are many function approximators, e.g. . Linear combinations of features | Neural network | Decision tree | Nearest neighbour | Fourier / wavelet bases | . We focus on differentiable function approximators. . 5/4/21 - Lecture 7: Policy Gradient Methods . 3 methods: . finite difference | MC policy gradient | Actor-Critic Policy Gradient | . advantages of policy based RL vs value based RL: . convergence (w/o oscillation that one can see in value based) | effective in continuous action spaces (in some cases taking the max (of q value) can be quite expensive) | policy based RL can learn stochastic policies which can be beneficial in some cases (e.g. rock scissor paper) (usually where you don’t fall into MDP with perfect states representation but we get partially observed environments) | . Some examples of policy: softmax policy and gaussian policy. . One-step MDP: terminating after 1 time-step. No sequence. In that case we have J(θ)=Eπθ[r]=∑s∈Sd(s)∑a∈Aπθ(s,a)Rs,aand∇θJ(θ)=∑s∈Sd(s)∑a∈Aπθ(s,a)∇θlog⁡πθ(s,a)Rs,a∇θJ(θ)=Eπθ[∇θlog⁡πθ(s,a)r]J( theta)= mathbb{E}_{ pi_ theta}[r]= sum_{s in mathcal{S}}d(s) sum_{a in mathcal{A}} pi_ theta(s, a) mathcal{R}_{s,a} and nabla_ theta J( theta) = sum_{s in mathcal{S}}d(s) sum_{a in mathcal{A}} pi_ theta(s, a) nabla_ theta log pi_ theta(s, a) mathcal{R}_{s,a} nabla_ theta J( theta) = mathbb{E}_{ pi_ theta}[ nabla_ theta log pi_ theta(s, a)r]J(θ)=Eπθ​​[r]=∑s∈S​d(s)∑a∈A​πθ​(s,a)Rs,a​and∇θ​J(θ)=∑s∈S​d(s)∑a∈A​πθ​(s,a)∇θ​logπθ​(s,a)Rs,a​∇θ​J(θ)=Eπθ​​[∇θ​logπθ​(s,a)r] Generalization is to replace instantaneous reward r with long-term value $Q_ pi(s,a)$ . 1st algorithm is Monte-Carlo policy gradient (REINFORCE) - tend to be slow, very high variance . Where we sample $Q_ pi(s,a)$ in $v_t$ and regularly update $ theta$. . Reducing variance using a critic. . We use a critic to estimate the action-value function, $Q_w (s, a) ≈ Q_{π_θ} (s, a)$ Actor-critic algorithms maintain two sets of parameters: . Critic Updates action-value function parameters w | Actor Updates policy parameters θ, in direction suggested by critic | . Actor-critic algorithms follow an approximate policy gradient ∇θJ(θ)≈Eπθ[∇θlog⁡πθ(s,a)Qw(s,a)]Δθ=α∇θlog⁡πθ(s,a)Qw(s,a) nabla_ theta J( theta) approx mathbb{E}_{ pi_ theta}[ nabla_ theta log pi_ theta(s, a)Q_w(s, a)] Delta theta= alpha nabla_ theta log pi_ theta(s, a)Q_w(s, a)∇θ​J(θ)≈Eπθ​​[∇θ​logπθ​(s,a)Qw​(s,a)]Δθ=α∇θ​logπθ​(s,a)Qw​(s,a) Critic will use a policy evaluation (several options seen so far: monte-carlo policy evaluation, TD, TD($ lambda$)) . 6/21/21 - Lecture 8: Integrating Learning and Planning . 3 parts in this lecture: . model based reinforcement learning | integrated architecture | simulation-based search | . Learn by model. What we mean by the model is 2 parts: understand transitions (how one state will transition to another state) and reward. If the agent has this understanding, then one can plan with that. . Model-Free RL . No model | Learn value function (and/or policy) from experience | . Model-Based RL . Learn a model from experience | Plan value function (and/or policy) from model | . Model-Based RL (using Sample-Based Planning) . Learn a model from real experience | Plan value function (and/or policy) from simulated experience | . Dyna-Q is a way to combine real experience with simulation. . Simulation-Based Search . Forward search paradigm using sample-based planning | Simulate episodes of experience from now with the model | Apply model-free RL to simulated episodes | . Simulate episodes of experience from now with the model . {Stk,Atk,Rt+1k,...,STk}k=1K∼Mv Big { S_t^k, A_t^k, R_{t+1}^k, ..., S_T^k Big }_{k=1}^K sim mathcal{M}_v{Stk​,Atk​,Rt+1k​,…,STk​}k=1K​∼Mv​ . Apply model-free RL to simulated episodes . Monte-Carlo control → Monte-Carlo search | Sarsa → TD search | . . . 7/1/21 - Lecture 9: Exploration and exploitation . David starts with a multi-armed bandit case. We can think about it as a one-step MDP. . But in that case we don’t have states anymore. . Definition of regret as the total opportunity loss (how far we are from the best value). And maximizing cumulative reward is the same as minimizing total regret. . Greedy and $ epsilon$-greedy have linear total regret. . Optimism in face of uncertainty: don’t play the one with best mean value but play the one with best potential (characterized with the highest tail) = select action maximising Upper Confidence Bound (UCB) . $ epsilon$-greedy is behaving right when properly tuned or can be a disaster otherwise. UCB is comparable to properly tuned $ epsilon$-greedy. . thesis from a French guy about thompson sampling in optimisation control problems: Exploration-Exploitation with Thompson Sampling in Linear .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/Introduction-to-Reinforcement-Learning-with-David-Silver.html",
            "relUrl": "/blog/Introduction-to-Reinforcement-Learning-with-David-Silver.html",
            "date": " • Mar 9, 2021"
        }
        
    
  
    
        ,"post37": {
            "title": "Use of gpg under linux",
            "content": "from best ways to encrypt files on linux . gpg . setup the key . gpg --gen-key . and enter a strong passphrase. . export public key . gpg --armor --output mypubkey.gpg --export &lt;E-mail that you registered&gt; . import from windows box . gpg --import mypubkey.gpg . encrypt files from windows box . gpg --output test.txt.gpg --encrypt --recipient &lt;Receiver&#39;s E-Mail ID&gt; test.txt . decrypt files on linux box . gpg --output test.txt --decrypt test.txt.gpg . find + gpg + tmpfs . encrypt from Windows . find . -name &#39;df_76*.csv&#39; -exec gpg --output {}.gpg --encrypt --recipient guillaume.ramelet@michelin.com {} ; . decrypt from Linux . There should be better ways to do it. . Here is my process: . Before starting: call mount_decrypt.sh. It mounts a tmpfs in secured_data/data, and decrypt all gpg files to this directory | | After work is done: call umount_decrypt.sh | gpg_decrypt.sh . #!/bin/bash gpg_name=&quot;$1&quot; src_name=${gpg_name%.*} TARGET_DATA=/home/explore/git/guillaume/d059/secured_data/data echo &quot;gpg decrypt $gpg_name -&gt; $src_name&quot; gpg --output $TARGET_DATA/$src_name --decrypt $gpg_name(base) . mount_decrypt.sh . #!/bin/bash GPG_DEC_CMD=/home/explore/git/guillaume/d059/secured_data/gpg_decrypt.sh TARGET_DATA=/home/explore/git/guillaume/d059/secured_data/data sudo mount -t tmpfs -o size=1G tmpfs $TARGET_DATA cd /media/explore/CHACLEF/janus find . -name &#39;df_76*.csv.gpg&#39; -exec $GPG_DEC_CMD {} ; . umount_decrypt.sh . #!/bin/bash TARGET_DATA=/home/explore/git/guillaume/d059/secured_data/data sudo umount $TARGET_DATA .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/gpg-linux.html",
            "relUrl": "/blog/gpg-linux.html",
            "date": " • Mar 3, 2021"
        }
        
    
  
    
        ,"post38": {
            "title": "Logbook for March 21",
            "content": "Week 9 - Mar 21 . Monday 3/1 . MIT 6S191 Deep Generative Modeling (lecture 4) - vaes and gans. . MIT 6S191 De-biasing Facial Recognition Systems (lab 2): CNN, VAE, DB-VAE . Tuesday 3/2 . College de France Approximations non linéaires et réseaux de neurones (lecture 4) . RL Course by David Silver lecture 1 - intro (22’/88’) . Future of Manufacturing@MIT - interesting landscape about Manufacturing and AI . Wednesday 3/3 . Interpretable Machine Learning by Christoph Molnar. LIME reading to understand context of local surrogate models. SHAP chapter using Janus data. . Deep Reinforcement Learning by Thomas Simonini (Chapter 3 v1) on DQN with temporal limitation using LSTM, and experience replay. (replay buffer) . Thursday 3/4 . Interpretable Machine Learning by Christoph Molnar. PDP chapter using Janus data. . Friday 3/5 . RL - Sutton book (p220-223) - full vs sample backups, trajectory sampling, heuristic search . RL - Sutton book (p223+) - start of Approximate Solution Methods, why to use NN. . Week 10 - Mar 21 . Monday 3/8 . MIT 6S191 Deep Reinforcement Learning. Q-learning vs Policy Gradient. . Tuesday 3/9 . College de France Ondelettes et échantillonnage (lecture 5) . RL Course by David Silver Introduction to Reinforcement Learning (lecture 1) . Installation of clustergit to detect local (=uncommited or unpushed) changes in repo . Wednesday 3/10 . Deep Reinforcement Learning by Thomas Simonini (Chapter 4 v1) on four strategies to improve DQN (fixed Q-targets, double DQN, dueling DQN (DDQN), Prioritized Experience Replay (PER)) . t-SNE using Janus data. . RL Course by David Silver Markov Decision Processes (lecture 2) . Friday 3/12 . RL - Sutton book (p287-352) - Applications and case studies, end of the book . RL Course by David Silver Planning by Dynamic Programming (lecture 3) . Week 11 - Mar 21 . Monday 3/15 . MIT 6S191 Limitations and New Frontiers. . MIT 6S191 Pixels-to-Control Learning (lab 3): Cartpole and Pong . RL Course by David Silver Model-Free Prediction (lecture 4) . Tuesday 3/16 . College de France Multi-résolutions (lecture 6) . Wednesday 3/17 . Deep Reinforcement Learning by Thomas Simonini (Chapter 5 v1) - Policy Gradient . Thursday 3/18 . Deep Reinforcement Learning by Thomas Simonini (Chapter 5 v1) - Policy Gradient notebooks . RL Course by David Silver Model-Free Control (lecture 5) . Friday 3/19 . Deep Reinforcement Learning by Thomas Simonini (Chapter 6 v1) - Advantage Actor Critic (A2C) and Asynchronous Advantage Actor Critic (A3C) . College de France - l’apprentissage profond par Yann Lecunn 2016 - Pourquoi l’apprentissage profond ? . Week 12 - Mar 21 . Monday 3/22 . MIT 6S191 Evidential Deep Learning and Uncertainty (lecture 7). . Deep Reinforcement Learning by Thomas Simonini (v1 Part 5) - Advantage Actor Critic (A2C) - implementation and video . Tuesday 3/23 . College de France Bases orthonormales d’ondelettes (lecture 7) . Wednesday 3/24 . Deep Reinforcement Learning by Thomas Simonini (Chapter 7 v1) - Proximal Policy Optimization PPO . Stable baselines 3 - init and 1st tutorial . Thursday 3/25 . setup headless raspberry pi to bridge wifi (tethering from phone) to ethernet (to wifi-router) . Stable baselines 3 - finalize init and go through documentation . Create a patch for a github project (by forking and pulling request) . Friday 3/26 . Stable baselines 3 - Documentation &gt; Examples . Rename my branches named master to main . Week 13 - Mar 21 . Monday 3/29 . MIT 6S191 Bias and Fairness (lecture 8). . Wednesday 3/31 . College de France Parcimonie et compression d’images (lecture 8) .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/logbook-March.html",
            "relUrl": "/blog/logbook-March.html",
            "date": " • Mar 1, 2021"
        }
        
    
  
    
        ,"post39": {
            "title": "Logbook for February 21",
            "content": "This is a test. I will try to keep words on a monthly (this page), weekly (per heading), daily basis. Just some short entries with possibly some links to more detailed materials. . Week 8 - Feb 21 . Monday 2/22 . To develop knowledge about RL, here is my learning process on a weekly basis. . Monday MIT 6S191 . Tuesday College de France . Wednesday Deep Reinforcement Learning by Thomas Simonini . Friday RL readings: papers, books, … . Friday 2/26 . blog fastpages - setup automated upgrade (instructions from _fastpages_docs) v2.1.42 . blog fastpages - display image preview (update of _config.yml) . RL - understood differences between Q-learning and Sarsa algorithms in end of step2 part2 . RL - Sutton book (p200-220) - eligibility traces, and start of planning vs learning .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/logbook-Februrary.html",
            "relUrl": "/blog/logbook-Februrary.html",
            "date": " • Feb 26, 2021"
        }
        
    
  
    
        ,"post40": {
            "title": "Practicing: Deep Reinforcement Learning Course by Thomas Simonini",
            "content": "A course by Thomas Simonini . Syllabus (from 2018) . Course introduction (from 2020) . Everything available in github . I appreciate the effort to update examples, and some 2018 implementations became obsolete. Historical Atari VC2600 games are now Starcraft 2 or minecraft, and news series on building AI for video games in Unity and Unreal Engine.. . (2/19/21) - Chapter 1 - An Introduction to Deep Reinforcement Learning? . Previous version from 2018: What is Deep Reinforcement Learning? is quite interesting. With 3 parts: . What Reinforcement Learning is, and how rewards are the central idea | The three approaches of Reinforcement Learning | What the “Deep” in Deep Reinforcement Learning means | . . Rewards, long-term future reward, discount rate. . . Episodic (starting and ending point) vs Continuous (e.g. stock trading) tasks. . Way of learning: Monte Carlo (MC: rewards collected at the end of an episode) vs Temporal Difference (TD: estimate rewards at each step) . . Exploration/Exploitation trade off. Will see later different ways to handle that trade-off. . . Three approaches to Reinforcement Learning . These are value-based, policy-based, and model-based. . Value Based . In value-based RL, the goal is to optimize the value function V(s). . The value function is a function that tells us the maximum expected future reward the agent will get at each state. . . Policy Based . In policy-based RL, we want to directly optimize the policy function π(s) without using a value function. . The policy is what defines the agent behavior at a given time. . We have two types of policy: . Deterministic: a policy at a given state will always return the same action. | Stochastic: output a distribution probability over actions. | . . Model Based . In model-based RL, we model the environment. This means we create a model of the behavior of the environment. Not addressed in this course. . Deep Reinforcement Learning . In Q-learning, we keep a table of actions to take for each state (based on reward). This can be huge. . Deep Learning allows to approximate this Q function. . . Updated version from 2020 (and video version) . This is a good starting point, well explained. . Reinforcement Learning is just a computational approach of learning from action. . A formal definition . Reinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that learn from the environment by interacting with it through trial and error and receiving rewards (positive or negative) as unique feedback. . Some explanations about observations (partial description) vs states (fully observed envt). Only differs in implementation, all theoretical background stays the same. . Action space where we can distinguish discrete (e.g. fire, up) actions from continuous (e.g. turn 23deg) ones. . Reward part is the same as the one from 2018. With cheese, mouse, maze example. . Episodic and continuous tasks part is the same as the one from 2018. . Exploration/Exploitation trade-off is explained the same way + an additional example taken from berkley - CS 294-112 - Deep Reinforcement Learning course. I want to learn more about this course! . About solving RL problems, it is now presented as 2 main approaches: . policy-based methods | value-based methods | . . And bedore to explain that, nice presentation of what is a policy $ pi$. Solving RL problem is to find that optimal policy: directly with policy-based method, indirectly (through value function) with value-based method. . There is an explanation about different types of policy: deterministic and stochastic. . And that we use deep neural networks to estimate the action to take (policy based) or to estimate the value of a state (value based). Thomas suggests to go further with deep learning with MIT 6.S191, which is the one (version 2021) I follow these days. . (2/19/21) - Chapter 2 - part 1 - Q-Learning, let’s create an autonomous Taxi . And in video (I like to read + watch the video at the same time) . Here in Step 2 we focus on a value-based method: Q-learning. And what is seen in part 1 and 2: . . Value-based method . Remember what we mean in value-based method . . you don’t train your policy, you define a simple function such as greedy function to select the best association State-Action, so the best action. . Bellman equation . each value as the sum of the expected return, which is a long process. This is equivalent to the sum of immediate reward + the discounted value of the state that follows. . . Monte Carlo vs Temporal Difference . And then an explanation about 2 types of method to learn a policy or a value-function: . Monte Carlo: learning at the end of the episode. With Monte Carlo, we update the value function from a complete episode and so we use the actual accurate discounted return of this episode. | TD learning: learning at each step. With TD learning, we update the value function from a step, so we replace Gt that we don’t have with an estimated return called TD target. (chich is the immediate reward + the discounted value of the next state) | . . It was not clear to me that these methods could be used for policy-based approach. It is now! . (2/24/21) - Chapter 2 - part 2 - Q-Learning, let’s create an autonomous Taxi . But the video is not yet available. . What is Q-Learning? . Q-Learning is an off-policy value-based method that uses a TD approach to train its action-value function: . “Off-policy”: we’ll talk about that at the end of this chapter. | “Value-based method”: it means that it finds its optimal policy indirectly by training a value-function or action-value function that will tell us what’s the value of each state or each state-action pair. | “Uses a TD approach”: updates its action-value function at each step. | . Q stands for quality (quality of action). After training we’ll get the optimal Q-function. . When choosing an action, we have to balance between exploration and exploitation with ϵ epsilonϵ - greedy: . . But at beginning Q table is not trained yet so we have to increase exploitation. It is done with some decreasing ϵ epsilonϵ. . . The Q-learning algorithm is a 4-step process: . step1: Q-Table init | step2: Choose action (ϵ epsilonϵ - greedy strategy) | step3: Perform action At and get Rt+1 and St+1 | step4: Update Q(St, At) | . . Why it is called off-policy? Because we don’t have the same logic to select action (ϵ epsilonϵ - greedy) and update Q (greedy). . With On-policy: we use the same policy for acting and updating. Sarsa is such an algorithm. . . Nice and simple manual example with mouse, cheese in a maze. We run Q-learning and make all calculation by hands. . . implement with numpy+gym this algorithm should be a nice exercise. There is an exercise to implement a taxi, within this notebook at colab google. Taxi V3 is an env from opengym. . (3/3/21) - back to 2018 - Chapter 3 - Deep Q-learning with Doom . Article, Notebook, Video . We’ll create an agent that learns to play Doom. Doom is a big environment with a gigantic state space (millions of different states). Creating and updating a Q-table for that environment would not be efficient at all. . The best idea in this case is to create a neural network that will approximate, given a state, the different Q-values for each action. . . . Addresses pb of temporal limitation: get multiple frames to have sense of motion. . Video is nice because it goes from start and follows closely all steps. . I wil try to implement in my own by creating an environment and running under a clone of Deep_reinforcement_learning_Course Thomas’s repo . Here at Deep Q learning with Doom.ipynb . I had to switch to tensorflow-gpu 1.13. Manage some cuda memory issue. But then was able to run it. . However as Thomas says, I should do it step by step on my own. . (3/10/21) - Chapter 4: Improvements in Deep Q Learning V1 . Article, Notebook, Video . four strategies that improve — dramatically — the training and the results of our DQN agents: . fixed Q-targets | double DQNs | dueling DQN (aka DDQN) | Prioritized Experience Replay (aka PER) | . fixed Q-targets to avoid chasing a moving target . Using a separate network with a fixed parameter (let’s call it w-) for estimating the TD target. | At every T TauT step, we copy the parameters from our DQN network to update the target network. | . . Improvements in Deep Q Learning: Dueling Double DQN, Prioritized Experience Replay, and fixed… . Implementation . Implementing fixed q-targets is pretty straightforward: . First, we create two networks (DQNetwork, TargetNetwork) . | Then, we create a function that will take our DQNetwork parameters and copy them to our TargetNetwork . | Finally, during the training, we calculate the TD target using our target network. We update the target network with the DQNetwork every T TauT step (T TauT is an hyper-parameter that we define). . | . double DQNs to handle overestimating of Q-values (at the beginning of training, taking the maximum q value (which is noisy) as the best action to take can lead to false positives) . we move from this TD target logic . . to the use of 2 networks . use our DQN network to select what is the best action to take for the next state (the action with the highest Q value). | use our target network to calculate the target Q value of taking that action at the next state. | . . Implementation . . Dueling DQN (aka DDQN) . based on this paper Dueling Network Architectures for Deep Reinforcement Learning. . With DDQN, we want to separate the estimator of these two elements, using two new streams: . one that estimates the state value V(s) | one that estimates the advantage for each action A(s,a) | . . and this can be combined with Prioritized experience replay. . This is nicely explained in this article. DDQN explanation is clearer than Thomas’. . The key here is to deal efficiently with experiences. When treating all samples the same, we are not using the fact that we can learn more from some transitions than from others. Prioritized Experience Replay (PER) is one strategy that tries to leverage this fact by changing the sampling distribution. . I guess there are several options to manage this prioritization (we would prefer transitions that do not fit well to our current estimate of Q function). And a key aspect is the performance of this selection. One implementation is SumTree. . I have to see full implementation in the notebook to fully understand the logic. . About the video . Thomas has insisted about the importance to master these architecture (DQN then DDQN, etc) before going further with state of the art architectures (Policy Gradient, PPO…) . Approach in videos is now different. In previous videos it was about explaining articles. Now it is more turned to implementation details based on notebooks. . Thomas has given a reference to Arthur Juliani who is a senior ML engineer at Unity. I would like to browse though this reference and see what can be done. . Should follow video and run/update notebook in //. . (3/17/21) - Chapter 5: Policy Gradients V1 . Article, Notebook, Video . In policy-based methods, instead of learning a value function that tells us what is the expected sum of rewards given a state and an action, we learn directly the policy function that maps state to action (select actions without using a value function). . 3 main advantages to use Policy Gradients vs Q learning: . convergence - have better convergence properties | effective in high dimension, or with continuous actions | stochastic policy - no need for exploration,/exploitation tradeoff | . But can be longer to train. . Policy search . We can dfine our policy as the probability distribution of actions (for a given state) . . And how good is this policy? Measured with J(θ thetaθ) . . We must find θ thetaθ to maximize J(θ thetaθ). How? . 2 steps: . Measure the quality of a π (policy) with a policy score function J(θ) | Use policy gradient ascent to find the best parameter θ that improves our π. | . the Policy Score function J(θ) . 3 ways (maybe more) . Calculate the mean of the return from the first time step (G1). This is the cumulative discounted reward for the entire episode. . . In a continuous environment, we can use the average value, because we can’t rely on a specific start state. Each state value is now weighted (because some happen more than others) by the probability of the occurrence of the respected state. . . Third, we can use the average reward per time step. The idea here is that we want to get the most reward per time step. . . Policy gradient ascent . because we want to maximize our Policy score function . . The solution will be to use the Policy Gradient Theorem. This provides an analytic expression for the gradient ∇ of J(θ) (performance) with respect to policy θ that does not involve the differentiation of the state distribution. (using likelihood ratio trick) . . It gives . . R(τ tauτ) is like a scalar value score. . Implementation . As with the previous section, this is good to watch the video at the same time. . And now this is the implementation in . doom deathmatch notebook . . as with Pong, we stack frames to understand dynamic with deque. . Even with GPU growth setup, I run an error after the 1st epoch. . ========================================== Epoch: 1 / 5000 Number of training episodes: 15 Total reward: 7.0 Mean Reward of that batch 0.4666666666666667 Average Reward of all training: 0.4666666666666667 Max reward for a batch so far: 7.0 . ResourceExhaustedError: OOM when allocating tensor with shape[5030,32,24,39] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [[]] Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. . I have to reduce batch size (to 1000) to make it work. . And I can monitor gpu memory consumption with watch nvidia-smi . . or we can use gpustat -i 2 . [0] Quadro RTX 4000 | 59’C, 34 %, 39 W | 7819 / 7982 MB | explore(6729M) gdm(162M) explore(388M) explore(282M) explore(86M) explore(89M) explore(3M) | . (3/19/21) - Chapter 6: Advantage Actor Critic (A2C) and Asynchronous Advantage Actor Critic (A3C) V1 . Article, Notebook, Video . “hybrid method”: Actor Critic. We’ll using two neural networks: . an Actor that controls how our agent behaves (policy-based) | a Critic that measures how good the action taken is (value-based) | . . Actor is using a policy function π(s,a,θ) pi(s, a, theta)π(s,a,θ) Critic is using a value function . qundefined(s,a,w) widehat{q}(s,a,w)q​(s,a,w) Which means 2 sets of weights to be optimized separately θ thetaθ and w. . . We can use advantage function to stabilize learning: . . Two different strategies: Asynchronous or Synchronous . We have two different strategies to implement an Actor Critic agent: . A2C (aka Advantage Actor Critic) | A3C (aka Asynchronous Advantage Actor Critic) | . Here we focus on A2C. . (3/22/21) - Implementation and video . It is a little bit confusing. I won’t run it. I would have liked a more pregressive approach and to understand all steps Thomas did to get to that final implementation. . (3/24/21) - Chapter 7: Proximal Policy Optimization PPO V1 . Article, Notebook . The central idea of Proximal Policy Optimization is to avoid having too large policy update. (we use a ratio that will tells us the difference between our new and old policy and clip this ratio from 0.8 to 1.2) . Clipped Surrogate Objective Function . . We will penalize changes that lead to a ratio that will away from 1 (in the paper ratio can only vary from 0.8 to 1.2). By doing that we’ll ensure that not having too large policy update because the new policy can’t be too different from the older one. . 2 implementations are known TRPO (Trust Region Policy Optimization) and PPO clip. TRPO being complex and costly, we focus on PPO: . . And the final loss will be: . . Now the implementation . By looking at the implementation, I ran into Stable baselines3. . This is a major update of Stable Baselines based on pytorch. It seems interesting! . I like this comment from Stable Baselines3 in the v1.0 blog post: . Motivation . Deep reinforcement learning (RL) research has grown rapidly in recent years, yet results are often difficult to reproduce. A major challenge is that small implementation details can have a substantial effect on performance – often greater than the difference between algorithms. It is particularly important that implementations used as experimental baselines are reliable; otherwise, novel algorithms compared to weak baselines lead to inflated estimates of performance improvements. . To help with this problem, we present Stable-Baselines3 (SB3), an open-source framework implementing seven commonly used model-free deep RL algorithms, relying on the OpenAI Gym interface. . I will create a new blog entry about Stable Baselines3. . as for previous notebook, I need to purchase Sonic2-3 to make it worked. Not for now maybe later. .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/Deep-Reinforcement-Learning-Course-by-Thomas-Simonini.html",
            "relUrl": "/blog/Deep-Reinforcement-Learning-Course-by-Thomas-Simonini.html",
            "date": " • Feb 19, 2021"
        }
        
    
  
    
        ,"post41": {
            "title": "Conda and jupyter tips",
            "content": "Keeping track of python environments . I manage all my python environments with conda from miniconda. . manual way . However I don&#39;t have a strong process to keep track of my environment specifications. Usually I manually create an env.txt file under my projects. Keeping all commands I have used to create that environment. . !cat /home/explore/git/guillaume/mit_6S191_Intro_to_deep_learning/env mit_6S191.txt . env_name: mit_6S191 libraries: python 3.7, tensorflow 2 Installation commands: conda create -n mit_6S191 python=3.7 conda activate mit_6S191 conda install tensorflow tensorflow-gpu conda install -c conda-forge jupyter_contrib_nbextensions conda install matplotlib numpy opencv conda install -c pytorch torchvision conda install nb_conda . What happens if I add packages in that environment. Or want to use that environment in another project. I have to remember the link between env name and project name. . That is not robust. . yml way . Keeping a yml file could be a solution to keep track of environment specifications. It doesn&#39;t answer to my last concern though (linking env name and project name) . But there is a limitation linked with channels. . !conda env export --from-history . name: fastai channels: - defaults dependencies: - python=3.8 - fastai - jupyter - jupyter_contrib_nbextensions - fastbook prefix: /home/explore/miniconda3/envs/fastai . In that example, fastai package should come from fastai channel but conda doesn&#39;t keep that information. . Using . conda install -n my_env rdkit::rdkit . could be an option. . automate yml way . Since conda keeps active environment in env variable CONDA_DEFAULT_ENV, we can automatically create up-to-date yml file. . !echo $CONDA_DEFAULT_ENV . fastai . !conda env export --from-history &gt; ~/temp/env_`echo $CONDA_DEFAULT_ENV`.yml !ls ~/temp/env_`echo $CONDA_DEFAULT_ENV`.yml . /home/explore/temp/env_fastai.yml . But for it to be usable, I will have to install package using the &lt;channel&gt;::&lt;package&gt; way. . !cat /home/explore/git/guillaume/mit_6S191_Intro_to_deep_learning/create_yml.sh . #!/bin/bash conda env export --from-history &gt; env_`echo $CONDA_DEFAULT_ENV`.yml . Conda commands . When managing conda environments, I very often fall on this documentation page which is simply great: https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html . Next time I visit this page, I will enter entries here to track my common commands. . Jupyter installation . Jupyter extensions . I have already explained how to install jupyter extensions and the one I use. update jupyter to include extensions) . nb_conda . This is usefull to switch from environment to another without having to stop/restart jupyter. . .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/conda-and-jupyter-tips.html",
            "relUrl": "/blog/conda-and-jupyter-tips.html",
            "date": " • Feb 16, 2021"
        }
        
    
  
    
        ,"post42": {
            "title": "Learning: Collège de France - Représentations parcimonieuses",
            "content": "Un exposé en 8 cours au collège de France de Stéphane Mallat sur les représentations parcimonieuses - 2021. . Cela donne envie d’aller voir ses autres cours: . 2018: L’apprentissage face à la malédiction de la grande dimension | 2019: L’apprentissage par réseaux de neurones profonds | 2020: Modèles multi-échelles et réseaux de neurones convolutifs | . A peu près 16 vidéos de 1h30 par cours. Et des notes de cours en pdf. . 2/15/21 - Le triangle « Régularité, Approximation, Parcimonie » (lecture 1) . C’est l’introduction du cours. J’apprécie les références historiques et philosphiques partant du rasoir d’Ockam. C’est le principe d’économie ou de parcimonie: le beau, le vrai viendrait du simple. . La 1ere fois que j’entends une référence précise sur l’opposition entre biais (erreur sur modèle) et variance (erreur sur données ou mesures) . Et une invitation à consulter une méthodologie d’analyse de données par Pierre Courtiol en utilisant Kaggle. L’idée d’une approche simple linéaire pour bien comprendre quelles étapes successives à emprunter pour améliorer son approche. Me semble assez orthogonal à ce que peut proposer Jeremy Howard: commencer tôt, overfitting n’est pas un probleme, pas de early stopping, etc. . 2/10/21 - Approximations linéaires et analyse de Fourier (lecture 2) . J’ai commencé par ce cours conseillé par Rémi mon pote enseignant chercheur en math. C’est un peu le grand écart avec des méthodes d’enseignement anglo-saxonnes mais ça fait du bien. C’est finalement plus proche de ce que j’ai connu dans ma formation initiale. . S.Mallat présente les équivalences (sous certaines conditions) entre . Régularité | Approximation en basse dimension | et représentation parcimonieuse | . dans le cadre des approximations linéaires. Il parle des 2 mondes: traitement du signal et analyse de la donnée. Je suis moins intéressé par le 1er monde, mais j’apprécie la piqure de rappel. Je ne me rappelais pas du tout l’importance de l’analyse de Fourier et la construction des bases de L[0,1] par exemple. . Et il revient sur les singularités, beaucoup d’informations sont portées par les singularités (par exemple les frontières dans une image) . Je crois bien que je vais me faire toute la session, et sans doute les autres années. . 2/23/21 - Grande dimension et composantes principales (lecture 3) . Dans ce cadre linéaire grande dimension, quelle meilleure base - approche PCA et base Karhunen-Loeve. . Quid quand on passe en non linéaire. . Réseau neurone à 1 couche cachée, théoreme de representation universel. . Retour sur les bases de L²[0,1] qui sont les bases de Fourier en variables complexes. . Pour un passage en dimension q, on remplace n par (n1, …, nq) et la multiplication n*u par le produit scalaire &lt;n, u&gt;. . En travaillant sur les équivalences du triangle, il montre pourquoi on est très limité en approximation lineaire quand la dimension augmente. . En approximation lineaire, il suffit de prendre les 1ers vecteurs (se limiter à une dimension q) (en base de fourier par exemple) pour avoir une assez bonne approximation. Dans des signaux plus perturbés (avec des singularités) on perd plus d’énergie: il faudrait échantilloner plus fin dans ces zones de singularités et si on dispose d’une base orthonormée il s’agirait non plus de prendre les q 1ers vecteurs mais de prendre ceux d’intéret. . 3/2/21 - Approximations non linéaires et réseaux de neurones (lecture 4) . Le triangle (approximation basse dimensions, représentation parcimonieuse, régularité) d’un point de vue non linéaire. . Ici plutôt qu’approximer un signal en prenant les M 1ers coefficients de Fourier (basses dimensions), on va prendre M coefficients mais dépendamment de x. C’est ici qu’on introduit la non-linéarité. L’erreur est alors la queue de distribution des coefficients ordonnés. On veut que l’énergie des plus petits coefficients soit négligeable. . Pas facile d’obtenir cet ordre, on cherche une façon de limiter les coefficients non ordonnés nous donnant une représentation parcimonieuse. En utilisant la nome l$ alpha$ avec α alphaα petit (inférieur à 2 et proche de 0), on introduit cette décroissance mais cette fois-ci sur les coefficients non ordonnés. . Intéressant d’avoir des normes convexes, et dans ce cas on ne peut prendre que α alphaα=1. C’est pour ça qu’on voit apparaître partout les normes l1 dans les algorithmes d’apprentissage (norme convexe garantissant une forme de sparsité). . On passe aux réseaux de neurones à 1 couche cachée. Et on va basculer dans les notations de x(u) à f(x)., avec x ϵ epsilonϵ [0, 1]d. . . Ici on projette f dans l’espace engendré par ces vecteurs { ρ rhoρ(x.wm+bm) }n&lt;=M. . On peut facilement calculer l’erreur quadratique comme l’intégrale sur les x ϵ epsilonϵ [0, 1]d de la norme l² ( f(x)-ftilde(x) ) et il y a un belle démonstration qui est le théorème d’approximation universelle (démontrée entre 1988 et 1992) qui montre que l’erreur tend vers 0 quand M tend vers l’infini. . La démonstration avec ρ rhoρ = eia revient à une décomposition d’en Fourier. Et pour d’autres non régularité comme reLu ou sigmoid, il s’agit d’un changement de base. . Et là on arrive à la malédiction de la dimensionnalité car quand d est grand (disons 1M), les coefficients baissent à une faible vitesse. Que faut-il faire pour battre cette malédiction? . Baron en 1993 introduit une hypothèse de regularité qui permet de borner l’erreur par un terme qui ne dépend pas de la dimension. C’est donc gagné sauf que l’hypothèse de régularité n’est généralement pas valide dans les cas qui nous intéressent. . Stéphane Mallat, de façon brillante mais est-ce étonnant, explique pourquoi l’approche des mathématiciens est une impasse et pourquoi ce qu’on cherche à faire se ramène à un problème bayésien. Car les problèmes qui nous intéressent (par exemple la classification d’objets, ne va solliciter qu’un minuscule espace (même si de grande dimension) parmi toutes les images possibles). On va donc chercher à caractériser x pour chaque y (classe). (revoir vidéo entre 49’ et 1h03) . L’enjeu est de caractériser le support qui est beaucoup plus concentré que [0,1]d. . Donc on va retravailler sur les approximations non linéaires de x, le signal lui-même (et non plus f), et d’essayer de comprendre pourquoi on peut faire beaucoup mieux que la transformée de Fourier et quelle genre de bases vont nous permettre de faire bcp mieux. Une des applications va être la compression, qui va nous amener à étudier la théorie de l’information et la théorie de l’information c’est exactement la théorie probabiliste qui explique ces phénomènes de concentration et les mesure avec l’entropie. . Introduction des bases d’ondelettes qui vont permettre de représenter les singularités locales. Les ondelettes sont à la fois localisées (paramètre v) et dilatées (paramètre s). Il faudra à partir de ces ondelettes construire des bases orthogonales pour arriver à des approximations basses dimensions (et garder les grands coefficients) . On introduit la notion de régularité locale exprimée avec lipchitz α alphaα. Avec α alphaα &lt;1 pour exprimer les singularités. . 3/9/21 - Ondelettes et échantillonnage (lecture 5) . On était resté sur une représentation de signaux qui ne présentent pas de régularité uniforme mais qui présentent des singularités que nous voulons capter, ces singularités étant porteuses d’informations importantes (par exemple les contours dans une image). Ces singularités n’étant pas très nombreuses, on peut toujours parler de régularité locale. . On va donc utiliser des ondelettes pour décomposer ces signaux, d’où la notion de représentation parcimonieuse, exprimée sur la base d’ondelettes orthonormales. Et enfin en en sélectionnant un petit nombre nous revenons sur nos approximations en basse dimension. . Le produit scalaire du signal x(u) par l’ondelette ψ psiψv,s revient à un produit de convolution de x par l’ondelette conjuguée. Ca veut dire que sur les points de singularités les produits scalaires vont être maximisés. . Stéphane Mallat passe un long moment pour nous amener à la construction de ces bases d’ondelettes orthonormales. Il part des bases de Haar puis de Shannon et arrive à une construction plus récente par Yves Meyer en 1986. . 3/16/21 - Multi-résolutions (lecture 6) . On a vu la dernière fois qu’on pouvait construire une base d’ondelette le long des indices de dilatations en 2j. . On va voir maintenant qu’on peut translater les ondelettes par des facteurs 2j.n. . Donc quand j est grand, les échelles sont de plus en plus grande. Et j petit va amener un échantillonnage de plus en plus fin. . {Ψ(j,n)(u)=12jΨ(u−2jn2j)}(j,n)ϵZ2 left { Psi_{(j,n)}(u)= frac{1}{ sqrt{2^j}} Psi left( frac{u-2^jn}{2^j} right) right }_{(j, n) epsilon Z^2}{Ψ(j,n)​(u)=2j . ​1​Ψ(2ju−2jn​)}(j,n)ϵZ2​ . sont-elles des bases orthonormales. Ensuite on appliquerait les techniques d’approximations consistant à éliminer les petits coefficients. . Les multi-résolutions sont des espaces linéaires sur lesquels nous allons projeter ces signaux. On va chercher à réduire les dimensions (par ex d’une image) en projetant sur ces espaces emboîtés. Et conserver le maximum d’information. . Un produit scalaire avec une fonction translatée peut toujours s’écrire comme un produit de convolution (Stéphane Mallat répète souvent cette propriété) . Stéphane Mallat fait ensuite le lien avec les algorithmes en bancs de filtre (cascades de filtrage + échantillonnage). . Dans ces opérations il y a sans arrêt des passages du continu au discret. Par exemple si je prends un signal et que je le projette sur ces espaces je me retrouve avec les coordonnées, qui sont les produits scalaires avec mes ϕ phiϕj,n (car base orthogonale), ce qui revient à filtrer et sous échantillonner. . 3/23/21 - Bases orthonormales d’ondelettes (lecture 7) . On repart sur notre triangle. Depuis 2 cours on est sur l’approximation basse dimension. . Stéphane Mallat applique le théorème sur des cas particuliers de la base de Haar, puis de la base de Shannon. Et revient sur la construction d’une base orthonormales avec des ondelettes “optimales”. . . Quand on prend le produit scalaire de notre signal f avec les ondelettes, on obtient des résultats presque nuls lorsque le signal est régulier. Et plus on a de moments nuls avec nos ondelettes, plus la régularité est ignorée (l’approximation par projection sur un espace vectorielle des monômes à l’ordre n). . On va cascader les projections aj (et les détails dj), et ça va revenir à cascader les filtres (les coefficients et les ondelettes). . Pour cela on calcule les valeurs des aj et dj en fonction de aj-1. On montre que cela s’obtient en filtrant (respectivement avec les h‾ overline{h}h et g‾ overline{g}g​) puis en sous-échantillonnant. En cascadant on obtient une série de filtrages, sous-échantillonnages, filtrages, sous-échantillonnages, , etc. . Les filtrages sont des convolutions. Si h a un support compact, ça va réduire le temps de calcul.(le nombre d’éléments non nuls correspond à la taille du filtre). Le nombre d’opérations pour passer de aL à aL-1, dL-1 est N*2m (où N: nombre de coefficients de aL et m est le nombre de moments nuls) . Le nombre d’opérations est linéaire, et la constante correspond à la taille des filtres. . On peut inverser cet algorithmes (car base o.n.) et la structure emboîtée va nous donner algorithme de reconstruction. On va sur-échantillonner (augmenter d’un facteur 2 en intercalant des 0) et appliquer les filtres g et h, et sommer pour obtenir le résultat. . Donc en gardant la base fréquence aJ et tous les détails {dj}, on reconstitue aL. (les signaux sur des grilles de plus en plus fines) . Stéphane Mallat finit sur des exemples en 2 dimensions. En 2 dimensions on aura 3 ondelettes à chaque échelle (1 avec les hautes fréquences dans une direction, 1 avec les hautes fréquences dans l’autre direction, et la dernière avec haute fréquence sur les 2 directions (les coins)). . 3/30/21 - Parcimonie et compression d’images (lecture 8) . Stéphane Mallat propose un survol de tout le cours pour montrer la logique dans laquelle on a évolué. . En reprenant le triangle Régularité - Approximation en basse dimension (au cœur du traitement de donnée) - Représentation parcimonieuse. Les équivalences entre régularité et la construction de représentations parcimonieuses permettent de construire des approximations en basse dimension. . Mais on peut les interpréter différemment : . d’un point de vue linéaire : on peut construire des approximations linéaires qui vont correspondre à des formes de régularité et certains types de représentations parcimonieuses (en particulier dans la base de Fourrier quand on a des invariants par translation) | en prenant un point de vue non linéaire : qui consiste non pas à faire des projections dans des espaces linéaires mais plutôt des projections dans des unions d’espaces linéaires obtenus en sélectionnant de façon libre dans une base orthogonale les plans les plus représentatifs. | . Il reprend en détail ce qu’on a vu en repartant de la théorie développée par Fourier (1822 ça ne date pas d’hier). Et reprend les réseaux de neurones à 1 couche cachée. . fM(x)=∑mw(m)ρ(⟨x,wm⟩+bm)f_M(x)= displaystyle sum_{m}w(m) rho( langle{x,w_m} rangle+b_m)fM​(x)=m∑​w(m)ρ(⟨x,wm​⟩+bm​) . L’entrée est x en dimension d, dans la première couche on calcule des produits scalaires avec les vecteurs $v_m$ qui sont les colonnes d’un opérateur linéaire $W_1$ et ces M produits scalaires vont être regroupés avec un relu (ou toute autre non-régularité) et un biais, et dans la dernière couche on fait une combinaison linéaire pour construire l’approximation. M est le nombre d’éléments dans la couche cachée, peut-on bien approximer f(x) à partir de cette construction ? . Ces réseaux, en prenant comme non-régularité un cosinus, nous font retomber sur des séries de Fourier. . fM(x)=∑∥vm∥&lt;Rw(m)cos⁡(⟨x,wm⟩+bm)f_M(x)= displaystyle sum_{ | v_m |&lt;R}w(m) cos ( langle{x,w_m} rangle+b_m)fM​(x)=∥vm​∥&lt;R∑​w(m)cos(⟨x,wm​⟩+bm​) . Faire une décomposition avec un réseau de neurone à 1 couche cachée est très similaire à décomposer la fonction dans une base de Fourier. Prendre un relu consisterait à faire un changement de base entre le relu et le cosinus. . Si on veut approximer une fonction uniformément régulière, il va falloir garder les basses fréquences. Mais $x$ n’est pas en dimension 1 mais en dimension $d$. Les fréquences qu’il va falloir prendre ici sont dans $ Z^d$, il va falloir garder toutes les fréquences dans une boule de rayon plus petit que $R$. Mais quand on est en dimension $q$, le nombre d’éléments dans une boule plus petit que $R$ va croître comme $R^q$. Donc il va falloir garder énormément d’éléments. . On a la possibilité d’approximer n’importe quelle fonction dans $L^2$ avec une erreur qui va décroître vers 0 quand le nombre de termes $M$ tend vers $ infty$ parce qu’on a une base orthogonale et donc n’importe quelle fonction peut être représentée à partir de la base . f∈L2  ⟹  lim⁡M→∞∥f−fM∥=0f in L^2 implies lim limits_{M to infty} | f-f_M |=0f∈L2⟹M→∞lim​∥f−fM​∥=0 . C’est le théorème d’approximation universelle. . Par contre si on a une régularité on peut préciser la vitesse de décroissance de l’erreur et en particulier si ma fonction est $ alpha$ dérivée dans un espace de Sobolev de degré $ alpha$, l’erreur va décroître d’autant plus vite que la régularité est grande, parce que les coefficients de Fourier vont décroître, et la vitesse de décroissance dépend de $ alpha/d$. . f∈Hα  ⟹  ∥f−fM∥=o(M−α/d)f in H^ alpha implies |f-f_M | = o(M^{- alpha/d})f∈Hα⟹∥f−fM​∥=o(M−α/d) . C’est la malédiction de la dimensionnalité. . Une autre approche consiste à reprendre ce cercle d’un point de vue non-linéaire. Au lieu de toujours prendre les mêmes coefficients pour approximer les fonctions qui m’intéressent, je vais adapter les coefficients à la fonction. C’est l’esprit des approximations non-linéaires. . Si je considère les vecteurs de Fourier, et ses coefficients ont une norme $L^p$ qui converge, pour un $p&lt;2$. Alors on a vu que les coefficients vont décroître à une vitesse qui dépend de $p$. Ca veut dire qu’il y a quelques grands coefficients et beaucoup de petits. Si on choisit les grands coefficients alors on va avoir une erreur qui décroît comme $-2/(p+1)$, l’erreur décroît lorsque $M$ augmente, indépendamment de la dimension. . SparseFouriercoefficients:Barronp&lt;2∑v∈Zd∣⟨f(x),Fv(x)⟩∣p&lt;∞  ⟹  ∥f−fM∥=o(M−2/p+1)Sparse quad Fourier quad coefficients: Barron quad p&lt;2 displaystyle sum_{v in Z^d} | langle{f(x), F_v(x)} rangle|^p &lt; infty implies |f-f_M |=o(M^{-2/p+1})SparseFouriercoefficients:Barronp&lt;2v∈Zd∑​∣⟨f(x),Fv​(x)⟩∣p&lt;∞⟹∥f−fM​∥=o(M−2/p+1) . no curse. Mais résultat tautologique. Pourquoi cette fonction serait approximable avec quelques coefficients de Fourier. Ça n’explique en rien pourquoi on peut améliorer fortement ce résultat en augmentant le nombre de couche. C’est simple mais ça n’explique pas les performances des réseaux de neurones profonds. . D’où l’approche par ondelettes. . Et la nécessité de construire des bases orthogonales d’ondelettes à décroissance rapide. Travaux de Yves Meyer. (en essayant de démontrer que ça n’était pas possible il a réussi à en construire ;)) Et S.Mallat a amélioré cette approche en se basant sur des approches de multi résolutions avec des espaces imbriqués. . On peut construire ces ondelettes en cascadant des filtres à différentes échelles (passe bas et passe bande à différentes échelles). . I.Daubechies a montré qu’on peut construire des ondelettes à support compact. . Y.Meyer a montré ce que ça donnait en dimension 2 (et c’est généralisable en dimension q) avec 3 ondelettes. . .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/college-de-france-representations-parcimonieuses.html",
            "relUrl": "/blog/college-de-france-representations-parcimonieuses.html",
            "date": " • Feb 10, 2021"
        }
        
    
  
    
        ,"post43": {
            "title": "Learning: MIT 6.S191 Introduction to Deep Learning - 2021",
            "content": "From http://introtodeeplearning.com/ . I keep all content (lectures, notebooks) in github . This is done with google contribution, and therefore all examples are in tensorflow. I will try to adapt notebooks in PyTorch. . 2/5/21 - Intro to Deep Learning - lecture 1 . Lecturer: Alexander Amini . Intro is just jaw-dropping! . 2020 intro was top. . 2021 intro is just awesome. . It is a standard overview of simple deep learning concepts: Perceptron, multi-perceptron, dense layers, loss, gradient-descent, backprop, SGD, regularization, dropout, early stoppping . 2/15/21 - Deep Sequence Modeling - lecture 2 . New lecturer: Ava Soleimany . Nice introduction to sequence modeling with Many-to-One, One-to-Many, Many-to-Many. . RNN and implementation in TensorFlow. And NLP examples: next word problem. (and NLP concepts such as Vocabulary, Indexing, Embedding) . And what we need for sequence modeling: . handle variable-length sequences | track long-term dependencies | maintain information about order | share parameters across the sequence | . Backpropagation through time and problem of exploding/vanishing gradients. . Against exploding: gradient clipping. Against vanishing: 3 ways explained - activation functions, weight init, network arch. . Gated cell: to control what information is passed through. Ex: LSTM Long Short Term Memory. They support something closed to Forget Store Update Output. Ava explains graphically which part of LSTM cells is providing which function. . And then examples: Music generation (to generate 4th movement of last symphony from Schubert!), sentiment classification, machine translation (with Attention mechanisms which provide learnable memory access to solve Not long memory), trajectory prediction, environmental modeling. . 2/16/21 - Intro to TensorFlow; Music Generation - software lab 1 . As an exercise I have completed labs in TensorFlow and adapted them in PyTorch. . With LSTM, I ran into this error: UnknownError: Fail to find the dnn implementation. [Op:CudnnRNN] . Which is solved by calling tf.config.experimental.set_memory_growth. . import tensorflow as tf gpus = tf.config.list_physical_devices(&#39;GPU&#39;) if gpus: try: # Currently, memory growth needs to be the same across GPUs for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True) logical_gpus = tf.config.experimental.list_logical_devices(&#39;GPU&#39;) print(len(gpus), &quot;Physical GPUs,&quot;, len(logical_gpus), &quot;Logical GPUs&quot;) except RuntimeError as e: # Memory growth must be set before GPUs have been initialized print(e) . Music lab is nice to play with. I am not sure I would be able to convert to PyTorch. It would require time! . 2/22/21 - Deep Computer Vision - lecture 3 . I have never been a big fan of computer vision. . I like the idea developed by Alexander Amini about hierarchy of features. (low level: edges, spots; mid level: eyes, noses) . And how he explains limitation of FC layers for visual detection, and introduction of spatial structure (feature extraction with convolutions) . Some nice examples of hand-engineered convolution filters for different needs: sharpen, edge detect, strong edge detect. . Then classic explanations of CNN with convolution, max pooling. . I like the way classification problems are broken down between feature learning (convolution+relu, pooling, repeated several times) and classification (flatten, FC, softmax) which is a task learning part. . The second part (task learning part) can be anything: classification, object detection, segmentation, probabilistic control, … . . Nice explanation of R-CNN to learn region proposals. . Introduction to Software lab2: de-biaising facial recognition systems. . 3/1/21 - Deep Generative Modeling - lecture 4 . From pattern discovered from data (underlying structure of the data), generate examples following these patterns. . Autoencoder: foundational generative model which builds up latent variable representation by self-encoding the input. To train such network, we create a decoder to go from latent variable to generated output, and then compare input to generated output. . . Variational autoencoder (vae): with vae we try to encode inputs as distributions defined by mean μ muμ and variance σ sigmaσ. And we want to achieve continuity and completeness: . continuity: points that are close in latent space –&gt; similar content after decoding | completeness: sampling from latent space –&gt; ‘meaningful’ content after decoding | . Regularization is pushing to get these properties. . . And the learning process is about minimizing reconstruction loss + a regularization term: . . Ava is then explaining the smart trick to allow backpropagation to happen. Indeed by introducing stochastic term in the sampling layer, we are breaking the backpropagation logic. . We are moving z from a normal distribution to μ muμ+σ sigmaσ.ϵ epsilonϵ where ϵ epsilonϵ follow a normal distribution of mean 0, std 1. . Explanation then of space disentanglement via β betaβ-VAEs. It allows latent variables to be independent. . . And then some introduction about *GANs (Generative Adversarial Network) which are a way to make a generative model by having 2 neural networks (generator and discriminator) compete with each other. . And share some recent advances on GAN such as StyleGAN(2), conditional GAN, CycleGAN. CycleGAN is famous for turning horses in zebras, but it can be used to transform speech as well (used in the synthesis of Obama’s voice) . . 3/1/21 - De-biasing Facial Recognition Systems - Software Lab 2 . Part 1 MNIST . starts with FC layers. With some overfitting but a good accuracy of 96%. . then move to a CNN architecture. I ran into gpu issues. Accuracy is now 99%. . I didn’t manage to make the last part working. (using tape.gradient) . Part 2 Debiasing . Fit a CNN model to classify faces based on celebA dataset. And see the bias effect by predicting on Fitzpatrick scale skin type classification system. . Use VAE to learn latent structure. . . To then debias using DB-VAE model. . . There is a lack of progressive unit tests to validate each step. Cannot go to the end. . Would be interested to see how to apply to non computer vision problems. . 3/8/21 - Deep Reinforcement Learning - lecture 5 . Q-function captures the expected total future reward an agent in state s can receive by executing a certain action a. . Distinction between Value Learning (learn Q function) and Policy Learning (find directly π piπ(s)). . . Value Learning or DQN . . . The key thing is about handling of continuous actions. . Let’s see how to do it with policy learning: . Policy learning or Policy Gradient (PG) . . . Alexanders ends the lecture by discussing about Deepmind progress: . alphaGo - 2016: with a pretrain in supervised mode then standard DRL | alphaGo Zero - 2017: standard DRL without pretraining | alphaZero - 2018: standard DRL without pretraining and applied to several games (Go, Chess, Shogi) | MuZero - 2020: learns the rules of the game by itself, create unknown dynamics | . 3/15/21 - Limitations and New Frontiers - lecture 6 . Universal Approximation Theorem: A feedforward network with a single layer is sufficient to approximate, to an arbitrary precision, any continuous function. . Ava emphasizes importance of training data (e.g. for generalization) and mentions a paper called “Understanding Deep Neural Networks Requires Rethinking Generalization”. . Some fail examples with dogs colorization (BW -&gt; colors) creating pink zone under the mouth. . And another one with Tesla autopilot. It motivates working on uncertainty in Deep Learning. . we need uncertainty metrics to assess the noise inherent to the data: aleatoric uncertainty | we need uncertainty metrics to assess the network’s confidence in its predictions: epistemic uncertainty | . Ava cites an example of a real 3D printed turtle designed to fool a classifier from turtle to rifle. . New frontier: Encoding Structure into Deep Learning. . CNN is a nice way to extract features from an image. But not all kind of data can express features in an euclidean way. Graphs is used as a structure for representing data in a lot of cases. . It drives us to Graph Convolutional Networks (GCNs). The graph convolutional operator is going to associate weights with each of the edges and apply the weights across the graph and then the kernel is going to be moved to the next node in the graph extracting information about its local connectivity. That local information is going to be aggregated and the NN is going to then learn a function that encodes that local information into a higher level representation. . New frontier: Automated Machine Learning &amp; AI. . Using a neural architecture search algorithm. At each step the model samples a brand new network. For each layer, defines number of fileters, filet height, width, stride height, width, nbr of fileters, etc. Update RNN controller based on the accuracy of the child network after training. . From autoML to autoAI: an automated complete pipeline for designing and deploying ML and AI models. . 3/15/21 - Pixels-to-Control Learning - Software Lab 3 . This is about reinforcement learning. . . We install (apt) xvfb and python-opengl. . And will learn with cartpole and pong. . Still this issue . UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [[node sequential_8/conv2d_4/Conv2D (defined at :19) ]] [Op:__inference_distributed_function_2442603] . Solved by running . import tensorflow as tf gpus = tf.config.list_physical_devices(&#39;GPU&#39;) if gpus: try: # Currently, memory growth needs to be the same across GPUs for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True) logical_gpus = tf.config.experimental.list_logical_devices(&#39;GPU&#39;) print(len(gpus), &quot;Physical GPUs,&quot;, len(logical_gpus), &quot;Logical GPUs&quot;) except RuntimeError as e: # Memory growth must be set before GPUs have been initialized print(e) . I couldn’t go through the training of Pong agent due to GPU limitation? . 2021-03-15 10:54:19.479775: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at conv_grad_input_ops.cc:1254 : Resource exhausted: OOM when allocating tensor with shape[3944,48,10,10] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfcbash . 3/22/21 - Evidential Deep Learning and Uncertainty - lecture 7 . . . . 3/29/21 - Bias and Fairness - lecture 8 . This starts as a standard lecture about bias. . I like emphasis about bias that could stand in all stages of AI life cycle: . data (obviously) | model | training and deployment | evaluation | interpretation | . Good explanation about biases due to class imbalance. It develops my intuition about it. . Balanced batches can be the answer. . Example weighting is another option using inverse frequency as a weight. . . Adversarial learning to mitiage Bias. . Application in NLP to complete analogies. He is to she, as doctor is to ? . Same thing with Learned Latent Structure. (can be used to create fair and representative dataset) . . 4/15/21 - Learning for Information Extraction - lecture 9. . Deep CPCFG for Information Extraction . Lecturer: Nigel Duffy and Freddy Chua, Ernst &amp; Young AI Labs . Focus is about document intelligence (extract info from business documents) . e.g. extract information from semi-structured documents such as tax forms (souvenirs ;)) . . 4/27/21 - Taming Dataset Bias - lecture 10 . video . dataset bias and training shift . (from one city to another (summer vs winter), from simulated to real control, from one culture to another) . Can fix with more data …(can be very expensive if we want to address all combinations) or use unlabeled data ? . . Adversarial approach to fool a domain discriminator. (domain discriminator trained to distinguished source and target domains) . Another approach is pixel alignment. . 4/30/21 - Towards AI for 3D Content Creation - lecture 11 . video . Sanja Fidler; Professor U. of Toronto and Head of AI at NVIDIA . 4/30/21 - AI in Healthcare - lecture 12 . video . Katherine Chou; Director of Research and Innovations, Google .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/learning-MIT-6.S191-2021.html",
            "relUrl": "/blog/learning-MIT-6.S191-2021.html",
            "date": " • Feb 5, 2021"
        }
        
    
  
    
        ,"post44": {
            "title": "Reinforcement learning readings",
            "content": "1/26/21 - Reinforcement learning for real-world robotics . from https://www.youtube.com/watch?v=Obek04C8L5E&amp;feature=youtu.be . at 26’ idea that you can tackle over-optimism models by using ensemble models. See paper at 2018 Model-Ensemble Trust-Region Policy Optimization . 1/26/21 - Reinforcement Learning algorithms — an intuitive overview . from https://medium.com/@SmartLabAI/reinforcement-learning-algorithms-an-intuitive-overview-904e2dff5bbc . . give an overview of various RL models. Model-based vs model-free. . And papers and codes. . 1/26/21 - Reinforcement learning, partie 1 : introduction (in French) . There is a reference to an introduction paper: from Sutton, Richard S., and Andrew G. Barto « Reinforcement learning : an introduction. » (2011). (I have an updated version from 2015) . There is a reference to a blog article [2] Steeve Huang. “Introduction to Various Reinforcement Learning Algorithms. Part I” (Q-Learning, SARSA, DQN, DDPG)”. (2018) . And the paper for OpenAI Gym [3] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba. “OpenAI Gym”. (2016) . 1/27/21 - Reinforcement learning : an introduction - I tabular solution methods . as a ref. from Reinforcement learning, partie 1 : introduction (in French) . I like this summary about RL . Reinforcement learning is a computational approach to understanding and automating goal-directed learning and decision-making. It is distinguished from other computational approaches by its emphasis on learning by an agent from direct interaction with its environment, without relying on exemplary supervision or complete models of the environment. In our opinion, reinforcement learning is the first field to seriously address the computational issues that arise when learning from interaction with an environment in order to achieve long-term goals. Reinforcement learning uses a formal framework defining the interaction between a learning agent and its environment in terms of states, actions, and rewards. This framework is intended to be a simple way of representing essential features of the artificial intelligence problem. These features include a sense of cause and effect, a sense of uncertainty and nondeterminism, and the existence of explicit goals. . There is some history about RL. Bellman equation and dynamic programming are at the beginning of RL. . I read about HJB equation from Huyên PHAM (from a French Math magazine). It is funny to see why dynamic programming has been named that way, and how to deal with management. . The class of methods for solving optimal control problems by solving this equation came to be known as dynamic programming (Bellman, 1957a). Bellman (1957b) also introduced the discrete stochastic version of the optimal control problem known as Markovian decision processes (MDPs), and Ronald Howard (1960) devised the policy iteration method for MDPs. All of these are essential elements underlying the theory and algorithms of modern reinforcement learning. . All the vocabulary around RL is coming from dynamic programming and MDP. . Markov decision process - Wikipedia . . Interesting to read that the famous cart pole experiment (learning to balance a pole hinged to a movable cart) came from Michie and Chambers in 1968, 53 years ago! (and derived from tic-tac-toe experiment) . I don’t understand the subtlety behind the move from “learning with a teacher” to “learning with a critic” following the modified Least-Mean-Square (LMS) algorithm; Widrow and Hoff (1973) . And some explanations about temporal-difference. I have just understood that a convergence effort happened (in 1989) by Chris Watkin who brought together temporal-difference and optimal control by developing Q-learning. . After this introduction, here is the content: . 1st part is about finite markov decision processes—and its main ideas including Bellman equations and value functions. . 2nd part is about describing three fundamental classes of methods for solving finite Markov decision problems: dynamic programming, Monte Carlo methods, and temporal-difference learning. Each class of methods has its strengths and weaknesses. Dynamic programming methods are well developed mathematically, but require a complete and accurate model of the environment. Monte Carlo methods don’t require a model and are conceptually simple, but are not suited for step-by-step incremental computation. Finally, temporal-difference methods require no model and are fully incremental, but are more complex to analyze. . 3rd part is about combining these methods to offer a complete and unified solution to the tabular reinforcement learning problem. . We can think of terms agent, environment, and action as engineers’ terms controller, controlled system (or plant), and control signal. . . Explanation about agent vs environment. Often not the same as physical boundaries of a robot: this boundary represents the limit of the agent’s absolute control, not of its knowledge. Many different agents can be operated at once. . The agent’s goal is to maximize the total mount of reward it receives. . I should re-read the full chapter3 because a lot of concepts coming from MDP is exposed, and their links to RL. At the end I should be able to answer most of end-of-chapter exercises. Have clearer view about how to define what are my agents/environment in my case; how to define actions (low-level definition (e.g. V in level1 electrical grid vs high level decision)); everything related to q* and Q-learning. . dynamic programming (DP) (chap4 - 103-126) . What is key here is to have an exact way to describe your environment. Which is not always feasible. And we need computer power to go through all states, compute value function. There is a balance between policy evaluation and policy improvement but this is not crystal clear to me. And I don’t understand asynchronous DP. I haven’t developed enough intuitions behind DP, and I am unable to answer exercises. I understand though that reinforcement learning can solve some problems by approximating part of it (evaluation, environment, …) . monte carlo (MC) methods (chap5 - 127-156) . first-visit vs every-visit methods. First-visit has been widely studied. Blackjack example. Explanation of Monte Carlo ES (exploratory starts); and how to avoid this unlikely assumption thanks to on-policy or off-policy methods (on-policy estimate the value of a policy while using it for control. In off-policy methods these two functions are separated (behavior and target)). . One issue with MC methods is to ensure sufficient exploration. One approach is to start with a random state-action pair, could work with simulated episodes but unlikely to learn from real experience. . MC methods do not bootstrap (i.e. they don’t update their value estimates based on other value estimates) (TODO learn more about bootstrapping) . temporal-difference (TD) learning (chap6 - 157-180) . TD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (they bootstrap, or said differently they learn a guess from a guess). . If you consider optimization as a 2 phases approach: prediction problem (ie policy evaluation) and control problem (ie optimal policy), DP, TD, MC differences are at the prediction problem. On control problem they use variations of generalized policy iteration (GPI). . TD methods combine the sampling of Monte Carlo with the bootstrapping of DP. . Example based on Driving Home. In TD you update prediction at each step, not waiting for the final return as in MC. . eligibility traces (chap7 - 181-208) . TD(λ lambdaλ) is a way to integrate MC and TD. . If one wants to use TD methods because of their other advantages, but the task is at least partially non-Markov, then the use of an eligibility trace method is indicated. Eligibility traces are the first line of defense against both long-delayed rewards and non-Markov tasks. . I am not sure to understand the effect of bootstrap. . Planning and Learning with Tabular Methods (chap8 - 209-220-236) . planning = require a model (dynamic programming, heuristic search) . learning = can be used without a model (MC, TD) . The difference is that whereas planning uses simulated experience generated by a model, learning methods use real experience generated by the environment. . 2/18/21 - from A Deep Reinforcement Learning Based Multi-Criteria Decision Support System for Optimizing Textile Chemical Process . This is a more practical paper and should help to figure out what could be our own implementation. . Overall MDP (markov decision process) structure is quite interesting with 3 blocks: . RF (random forest) models (one per objective) | AHP (analytic hierarchy process) which is a MCDM (Multiple criteria decision-making) method | DQN which is the reinforcement learning part to approximate the Q function | . . there are interesting references. . [2] K. Suzuki, ARTIFICIAL NEURAL NETWORKS - INDUSTRIAL AND CONTROL ENGINEERING APPLICATIONS. 2011. . It is nearly impossible to upgrade the textile chemical manufacturing processes directly by only following the cases from other industries without considering the detailed characteristics of this sector and specific investigations in the applicable advanced technologies. To this end, the construction of accurate models for simulating manufacturing processes using intelligent techniques is rather necessary[2] . [4]A. Ghosh, P. Mal, and A. Majumdar, Advanced Optimization and Decision-Making Techniques in Textile Manufacturing.2019. . [..] Therefore, production decision-makers cannot effectively control the processes in order to obtain desired product functionalities [4] . [53] T.L. Saaty, “What is the analytic hierarchy process?” Mathematical models for decision support, Springer, 1988, pp.109 121. . The AHP is a MCDM method introduced by Saaty [53] . [54]R. S. Sutton and A. G. Barto, Introduction to reinforcement learning, vol. 135. MIT press Cambridge, 1998. . The Markov property indicates that the state transitions are only dependent on the current state and current action is taken, but independent to all prior states and actions[54]. . [66] Z. Chourabi, F.Khedher, A. Babay and M. Cheikhrouhou, “Multi-criteria decision making in workforce choice using AHP, WSM and WPM”, J.Text.Inst., 2018 . However, it is worth remarking that certain features of this framework may hinder the massive promotion and application of it. The AHP has been successfully implemented in MCDM problems [41], [66] . 2/18/21 - The Complete Reinforcement Learning Dictionary . recommandations: . If you’re looking for a quick, 10-minutes crash course into RL with code examples, checkout my Qrash Course series: Introduction to RL and Q-Learning and Policy Gradients and Actor-Critics. | I you’re into something deeper, and would like to learn and code several different RL algorithms and gain more intuition, I can recommend this series by Thomas Simonini and this series by Arthur Juliani. | If you’re ready to master RL, I will direct you to the “bible” of Reinforcement Learning — “Reinforcement Learning, an introduction” by Richard Sutton and Andrew Barto. The second edition (from 2018) is available for free (legally) as a PDF file. | . 3/5/21 - Reinforcement learning : an introduction - II Approximate Solution Methods . This is the 2nd part of the book. . On-policy Approximation of Action Values . As mentioned in introduction of part II, what is developed in part I (our estimates of value functions are represented as a table with one entry for each state or for each state–action pair) is instructive, but of course it is limited to tasks with small numbers of states and actions. . How can experience with a limited subset of the state space be usefully generalized to produce a good approximation over a much larger subset? . This is a generalization issue (or function approximation) one could consider as an instance of supervised learning, where we use the s-&gt;v of each backup as a training example, and then interpret the approximate function produced as an estimated value function. . Bertsekas and Tsitsiklis (1996) present the state of the art in function approximation in reinforcement learning. . Policy approximation . Actor-Critic: The policy structure is known as the actor, because it is used to select actions, and the estimated value function is known as the critic, because it criticizes the actions made by the actor. . (3/12/21) end of book. Chapter 14 - Applications and case studies. . I like this statement: . Applications of reinforcement learning are still far from routine and typically require as much art as science. Making applications easier and more straightforward is one of the goals of current research in reinforcement learning. . TD backgammon (1995). It uses a neural net (1 hidden layer, from 40 to 80 units) to approximate the predicted probability of winning v(s) for a given state. In later version, some domain features were used but still using self-play TD learning method. (I don’t know specifics for these domain features). And last versions give an interest to opponent reactions (possible dice rolls and moves) . Samuel’s Checkers Player (~1960). (Checkers c’est le jeu de dames). It is based on minimax procedure to find the best move from current position. 1st learning used was rote learning (storing position(s value). 2nd learning used alpha-beta (linked to minimax procedure) and hierarchical lookup tables instead of linear function approximation. . Acrobot (1993). Use of Sarsa(λ lambdaλ). Interesting to see that an exploration step can spoil a whole sequence of good actions. This is why greedy policy is used (ϵ epsilonϵ=0). . Elevator dispatching (1996). With a reward being the negative of the sum of the squared waiting times of all waiting passengers. (squared to push the system to avoid big waiting times). We use an extension of Q-learning to semi-Markov decision problems. For function approximation, a nonlinear neural network trained by back-propagation was used to represent the action-value function. . Dynamic Channel Allocation (1997). The channel assignment problem can be formulated as a semi-Markov decision process much as the elevator dispatching problem was in the previous section. . Job-Shop Scheduling (1996). Zhang and Dietterich’s job-shop scheduling system is the first successful instance of which we are aware in which reinforcement learning was applied in plan-space, that is, in which states are complete plans (job-shop schedules in this case), and actions are plan modifications. This is a more abstract application of reinforcement learning than we are used to thinking about. . Chapter 15 - Prospects . . This is a map to distinguish where to use different techniques. And considerations of a 3rd dimension regarding function approximation, or on/off-policy. . And then opening to non markov case such as the theory of partially observable MDPs (POMDPs). (StarCraft!) . References . 25 pages of references! Woawww. .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/reinforcement-learning-readings.html",
            "relUrl": "/blog/reinforcement-learning-readings.html",
            "date": " • Jan 26, 2021"
        }
        
    
  
    
        ,"post45": {
            "title": "Aristotle and Deep learning",
            "content": "By reading some references in recent paper, I have started to read “artificial intelligence structures and strategies for complex problem solving” by George Luger. . . This is a massive book from 2005 in its 6th edition. I don’t think it has been updated since that. And the author starts a writing of AI history. . I have been intrigued by the use of the opening sentence from Aristotle in the Metaphysics: “All men by nature desire to know…”. I remembered that sentence (without knowing it was from Aristotle), and I jumped to The Metaphysics Aristotle’s [wikipedia page](https://fr.wikipedia.org/wiki/M%C3%A9taphysique%28Aristote%29) (the French one). There is a nice presentation of The MetaPhysics and some extracts that I have found quite interesting. One of them following “All men by nature desire to know” is detailing what is art and science; and for art: one need to be able to recognize similar cases and be able to generalize to an (more) universal rule. . I cannot not see a link with what is happening in what we do on a daily basis in AI and deep learning. I had been surprised by Jeremy Howard’s curriculum (if I am not wrong he has a major) in Philosophy, and I better understand why he is so good in what he does. . Should have studied Philosophy and ancient Greek! . Would love to know your thoughts about that. (and if anyone can ask Jeremy’s without directly @ him) . Here is a more detailed analysis of Aristotle thought: (again from wikipedia, not my own ;)) . By nature, all animals are sentient; but sensation is not yet sufficient to produce knowledge: indeed, remarks Aristotle, sensation engenders memory or not. But animals endowed with memory are the most intelligent and the best able to learn. However, man “lives on art and reasoning.” To learn, you have to feel, remember, but man has the capacity to draw experience from these simple images and from a multitude of experimental notions emerges a single judgment that is universal in all similar cases: it is what constitutes art: “Science and art arise for men through experience” 10. Art therefore presupposes: the ability to recognize similar cases and the ability to apply a universal rule to these cases. Of experience and art, which is more perfect? In practical life, experience seems superior to art, because it is knowledge of the particular, of the individual: sensations, the foundation of knowledge of the particular, are not science and do not teach us the why ( διότι). Art, for its part, knows the universal and goes beyond individual things, it is to art that knowledge and the faculty of understanding belong: men of art know the why and the cause. The wisest are wise not by practical skill, but by theory (λόγος) and knowledge of the causes. This explains the superiority of the architect over the maneuver. The sign of this knowledge is that it can be taught; now men of art can teach. However, among the arts some relate to the necessities of life and others come from “leisure” which is knowledge sought for itself, as in mathematics. And through these appears the highest knowledge, wisdom, which has for its object the first causes and the first principles of what-is; therefore the theoretical sciences are superior to the practical sciences. . From observations (rows of data) we can recognize similar cases (patterns or embeddings) and identify universal rules (models?) .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/Aristotle-and-deep-learning.html",
            "relUrl": "/blog/Aristotle-and-deep-learning.html",
            "date": " • Jan 21, 2021"
        }
        
    
  
    
        ,"post46": {
            "title": "seaborn cheatsheet",
            "content": "Introduction to Data Visualization with Seaborn . pdf lectures in github . Introduction to Seaborn . Introduction to Seaborn . # Getting started import seaborn as sns import matplotlib.pyplot as plt # Example 1: Scatter plot import seaborn as sns import matplotlib.pyplot as plt height = [62, 64, 69, 75, 66, 68, 65, 71, 76, 73] weight = [120, 136, 148, 175, 137, 165, 154, 172, 200, 187] sns.scatterplot(x=height, y=weight) plt.show() # Example 2: Create a count plot import seaborn as sns import matplotlib.pyplot as plt gender = [&quot;Female&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;] sns.countplot(x=gender) plt.show() . Using pandas with Seaborn . # Using DataFrames with countplot() import pandas as pd import matplotlib.pyplot as plt import seaborn as sns df = pd.read_csv(&quot;masculinity.csv&quot;) sns.countplot(x=&quot;how_masculine&quot;, data=df) plt.show() . Adding a third variable with hue . # Tips dataset import pandas as pd import seaborn as sns tips = sns.load_dataset(&quot;tips&quot;) tips.head() # A basic scatter plot import matplotlib.pyplot as plt import seaborn as sns sns.scatterplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, data=tips) plt.show() # A scatter plot with hue import matplotlib.pyplot as plt import seaborn as sns sns.scatterplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, data=tips, hue=&quot;smoker&quot;) plt.show() # Setting hue order import matplotlib.pyplot as plt import seaborn as sns sns.scatterplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, data=tips, hue=&quot;smoker&quot;, hue_order=[&quot;Yes&quot;,&quot;No&quot;]) plt.show() # Specifying hue colors import matplotlib.pyplot as plt import seaborn as sns hue_colors = {&quot;Yes&quot;: &quot;black&quot;, &quot;No&quot;: &quot;red&quot;} sns.scatterplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, data=tips, hue=&quot;smoker&quot;, palette=hue_colors) plt.show() # Using HTML hex color codes with hue import matplotlib.pyplot as plt import seaborn as sns hue_colors = {&quot;Yes&quot;: &quot;#808080&quot;, &quot;No&quot;: &quot;#00FF00&quot;} sns.scatterplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, data=tips, hue=&quot;smoker&quot;, palette=hue_colors) plt.show() # Using hue with count plots import matplotlib.pyplot as plt import seaborn as sns sns.countplot(x=&quot;smoker&quot;, data=tips, hue=&quot;sex&quot;) plt.show() . Visualizing Two Quantitative Variables . Introduction to relational plots and subplots . # Using relplot() import seaborn as sns import matplotlib.pyplot as plt sns.relplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, data=tips, kind=&quot;scatter&quot;) plt.show() # Subplots in columns import seaborn as sns import matplotlib.pyplot as plt sns.relplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, data=tips, kind=&quot;scatter&quot;, col=&quot;smoker&quot;) plt.show() # Subplots in rows import seaborn as sns import matplotlib.pyplot as plt sns.relplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, data=tips, kind=&quot;scatter&quot;, row=&quot;smoker&quot;) plt.show() # Subplots in rows and columns import seaborn as sns import matplotlib.pyplot as plt sns.relplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, data=tips, kind=&quot;scatter&quot;, col=&quot;smoker&quot;, row=&quot;time&quot;) plt.show() # Wrapping columns import seaborn as sns import matplotlib.pyplot as plt sns.relplot(x=&quot;total_bill&quot;,y=&quot;tip&quot;,data=tips,kind=&quot;scatter&quot;,col=&quot;day&quot;,col_wrap=2) plt.show() # Ordering columns import seaborn as sns import matplotlib.pyplot as plt sns.relplot(x=&quot;total_bill&quot;,y=&quot;tip&quot;,data=tips,kind=&quot;scatter&quot;,col=&quot;day&quot;,col_wrap=2,col_order=[&quot;Thur&quot;,&quot;Fri&quot;,&quot;Sat&quot;,&quot;Sun&quot;]) plt.show() . Customizing scatter plots . # Subgroups with point size import seaborn as sns import matplotlib.pyplot as plt sns.relplot(x=&quot;total_bill&quot;,y=&quot;tip&quot;,data=tips,kind=&quot;scatter&quot;,size=&quot;size&quot;) plt.show() # Point size and hue import seaborn as sns import matplotlib.pyplot as plt sns.relplot(x=&quot;total_bill&quot;,y=&quot;tip&quot;,data=tips,kind=&quot;scatter&quot;,size=&quot;size&quot;,hue=&quot;size&quot;) plt.show() # Subgroups with point style import seaborn as sns import matplotlib.pyplot as plt sns.relplot(x=&quot;total_bill&quot;,y=&quot;tip&quot;,data=tips,kind=&quot;scatter&quot;,hue=&quot;smoker&quot;,style=&quot;smoker&quot;) plt.show() # Changing point transparency import seaborn as sns import matplotlib.pyplot as plt # Set alpha to be between 0 and 1 sns.relplot(x=&quot;total_bill&quot;,y=&quot;tip&quot;,data=tips,kind=&quot;scatter&quot;,alpha=0.4) plt.show() . Introduction to line plots . # Line plot import matplotlib.pyplot as plt import seaborn as sns sns.relplot(x=&quot;hour&quot;, y=&quot;NO_2_mean&quot;,data=air_df_mean,kind=&quot;line&quot;) plt.show() # Subgroups by location import matplotlib.pyplot as plt import seaborn as sns sns.relplot(x=&quot;hour&quot;, y=&quot;NO_2_mean&quot;,data=air_df_loc_mean,kind=&quot;line&quot;,style=&quot;location&quot;,hue=&quot;location&quot;) plt.show() # Adding markers import matplotlib.pyplot as plt import seaborn as sns sns.relplot(x=&quot;hour&quot;, y=&quot;NO_2_mean&quot;,data=air_df_loc_mean,kind=&quot;line&quot;,style=&quot;location&quot;,hue=&quot;location&quot;,markers=True) plt.show() # Turning off line style import matplotlib.pyplot as plt import seaborn as sns sns.relplot(x=&quot;hour&quot;, y=&quot;NO_2_mean&quot;,data=air_df_loc_mean,kind=&quot;line&quot;,style=&quot;location&quot;,hue=&quot;location&quot;,markers=True,dashes=False) plt.show() # Multiple observations per x-value # Line plot import matplotlib.pyplot as plt import seaborn as sns sns.relplot(x=&quot;hour&quot;, y=&quot;NO_2&quot;,data=air_df,kind=&quot;line&quot;) plt.show() # Replacing confidence interval with standard deviation import matplotlib.pyplot as plt import seaborn as sns sns.relplot(x=&quot;hour&quot;, y=&quot;NO_2&quot;,data=air_df,kind=&quot;line&quot;,ci=&quot;sd&quot;) plt.show() # Turning off confidence interval import matplotlib.pyplot as plt import seaborn as sns sns.relplot(x=&quot;hour&quot;, y=&quot;NO_2&quot;,data=air_df,kind=&quot;line&quot;,ci=None) plt.show() . Visualizing a Categorical and a Quantitative Variable . Count plots and bar plots . # countplot() vs. catplot() import matplotlib.pyplot as plt import seaborn as sns sns.catplot(x=&quot;how_masculine&quot;,data=masculinity_data,kind=&quot;count&quot;) plt.show() # Changing the order import matplotlib.pyplot as plt import seaborn as sns category_order = [&quot;No answer&quot;,&quot;Not at all&quot;,&quot;Not very&quot;,&quot;Somewhat&quot;,&quot;Very&quot;] sns.catplot(x=&quot;how_masculine&quot;,data=masculinity_data,kind=&quot;count&quot;,order=category_order) plt.show() # Bar plots import matplotlib.pyplot as plt import seaborn as sns sns.catplot(x=&quot;day&quot;,y=&quot;total_bill&quot;,data=tips,kind=&quot;bar&quot;) plt.show() # Turning off confidence intervals import matplotlib.pyplot as plt import seaborn as sns sns.catplot(x=&quot;day&quot;,y=&quot;total_bill&quot;,data=tips,kind=&quot;bar&quot;,ci=None) plt.show() # Changing the orientation import matplotlib.pyplot as plt import seaborn as sns sns.catplot(x=&quot;total_bill&quot;,y=&quot;day&quot;,data=tips,kind=&quot;bar&quot;) plt.show() . Box plots . # How to create a box plot import matplotlib.pyplot as plt import seaborn as sns g = sns.catplot(x=&quot;time&quot;,y=&quot;total_bill&quot;,data=tips,kind=&quot;box&quot;) plt.show() # Change the order of categories import matplotlib.pyplot as plt import seaborn as sns g = sns.catplot(x=&quot;time&quot;,y=&quot;total_bill&quot;,data=tips,kind=&quot;box&quot;,order=[&quot;Dinner&quot;,&quot;Lunch&quot;]) plt.show() # Omitting the outliers using `sym` import matplotlib.pyplot as plt import seaborn as sns g = sns.catplot(x=&quot;time&quot;,y=&quot;total_bill&quot;,data=tips,kind=&quot;box&quot;,sym=&quot;&quot;) plt.show() # Changing the whiskers using `whis` import matplotlib.pyplot as plt import seaborn as sns g = sns.catplot(x=&quot;time&quot;,y=&quot;total_bill&quot;,data=tips,kind=&quot;box&quot;,whis=[0, 100]) plt.show() . Point plots . # Creating a point plot import matplotlib.pyplot as plt import seaborn as sns sns.catplot(x=&quot;age&quot;,y=&quot;masculinity_important&quot;,data=masculinity_data,hue=&quot;feel_masculine&quot;,kind=&quot;point&quot;) plt.show() # Disconnecting the points import matplotlib.pyplot as plt import seaborn as sns sns.catplot(x=&quot;age&quot;,y=&quot;masculinity_important&quot;,data=masculinity_data,hue=&quot;feel_masculine&quot;,kind=&quot;point&quot;,join=False) plt.show() # Displaying the median import matplotlib.pyplot as plt import seaborn as sns from numpy import median sns.catplot(x=&quot;smoker&quot;,y=&quot;total_bill&quot;,data=tips,kind=&quot;point&quot;,estimator=median) plt.show() # Customizing the confidence intervals import matplotlib.pyplot as plt import seaborn as sns sns.catplot(x=&quot;smoker&quot;,y=&quot;total_bill&quot;,data=tips,kind=&quot;point&quot;,capsize=0.2) plt.show() # Turning off confidence intervals import matplotlib.pyplot as plt import seaborn as sns sns.catplot(x=&quot;smoker&quot;,y=&quot;total_bill&quot;,data=tips,kind=&quot;point&quot;,ci=None) plt.show() . Customizing Seaborn Plots . Changing plot style and color . # Figure style: &quot;whitegrid&quot; sns.set_style(&quot;whitegrid&quot;) sns.catplot(x=&quot;age&quot;,y=&quot;masculinity_important&quot;,data=masculinity_data,hue=&quot;feel_masculine&quot;,kind=&quot;point&quot;) plt.show() # Other styles: sns.set_style(&quot;ticks&quot;) sns.set_style(&quot;dark&quot;) sns.set_style(&quot;darkgrid&quot;) # Example (diverging palette) sns.set_palette(&quot;RdBu&quot;) category_order = [&quot;No answer&quot;,&quot;Not at all&quot;,&quot;Not very&quot;,&quot;Somewhat&quot;,&quot;Very&quot;] sns.catplot(x=&quot;how_masculine&quot;,data=masculinity_data,kind=&quot;count&quot;,order=category_order) plt.show() # Custom palettes custom_palette = [&quot;red&quot;, &quot;green&quot;, &quot;orange&quot;, &quot;blue&quot;,&quot;yellow&quot;, &quot;purple&quot;] sns.set_palette(custom_palette) # Custom palettes custom_palette = [&#39;#FBB4AE&#39;, &#39;#B3CDE3&#39;, &#39;#CCEBC5&#39;,&#39;#DECBE4&#39;, &#39;#FED9A6&#39;, &#39;#FFFFCC&#39;,&#39;#E5D8BD&#39;, &#39;#FDDAEC&#39;, &#39;#F2F2F2&#39;] sns.set_palette(custom_palette) # Larger context: &quot;talk&quot; #Smallest to largest: &quot;paper&quot;, &quot;notebook&quot;, &quot;talk&quot;, &quot;poster&quot; sns.set_context(&quot;talk&quot;) . Adding titles and labels: Part 1 . # Adding a title to FacetGrid g = sns.catplot(x=&quot;Region&quot;,y=&quot;Birthrate&quot;,data=gdp_data,kind=&quot;box&quot;) g.fig.suptitle(&quot;New Title&quot;) plt.show() # Adjusting height of title in FacetGrid g = sns.catplot(x=&quot;Region&quot;,y=&quot;Birthrate&quot;,data=gdp_data,kind=&quot;box&quot;) g.fig.suptitle(&quot;New Title&quot;,y=1.03) plt.show() . Adding titles and labels: Part 2 . # Adding a title to AxesSubplot g = sns.boxplot(x=&quot;Region&quot;,y=&quot;Birthrate&quot;,data=gdp_data) g.set_title(&quot;New Title&quot;,y=1.03) # Titles for subplots g = sns.catplot(x=&quot;Region&quot;,y=&quot;Birthrate&quot;,data=gdp_data,kind=&quot;box&quot;,col=&quot;Group&quot;) g.fig.suptitle(&quot;New Title&quot;,y=1.03) g.set_titles(&quot;This is {col_name}&quot;) # Adding axis labels g = sns.catplot(x=&quot;Region&quot;,y=&quot;Birthrate&quot;,data=gdp_data,kind=&quot;box&quot;) g.set(xlabel=&quot;New X Label&quot;,ylabel=&quot;New Y Label&quot;) plt.show() # Rotating x-axis tick labels g = sns.catplot(x=&quot;Region&quot;,y=&quot;Birthrate&quot;,data=gdp_data,kind=&quot;box&quot;) plt.xticks(rotation=90) plt.show() . Intermediate Data Visualization with Seaborn . pdf lectures in github . Seaborn Introduction . Introduction to Seaborn . # Seaborn distplot import seaborn as sns sns.distplot(df[&#39;alcohol&#39;]) . Using the distribution plot . # Creating a histogram sns.distplot(df[&#39;alcohol&#39;], kde=False, bins=10) # Alternative data distributions sns.distplot(df[&#39;alcohol&#39;], hist=False, rug=True) # Further Customizations sns.distplot(df[&#39;alcohol&#39;], hist=False,rug=True, kde_kws={&#39;shade&#39;:True}) . Regression Plots in Seaborn . # Introduction to regplot sns.regplot(x=&quot;alcohol&quot;, y=&quot;pH&quot;, data=df) # lmplot faceting sns.lmplot(x=&quot;quality&quot;, y=&quot;alcohol&quot;,data=df, hue=&quot;type&quot;) sns.lmplot(x=&quot;quality&quot;, y=&quot;alcohol&quot;,data=df, col=&quot;type&quot;) . Customizing Seaborn Plots . Using Seaborn Styles . # Setting Styles # Seaborn has default configurations that can be applied with sns.set() # These styles can override matplotlib and pandas plots as well sns.set() # Theme examples with sns.set_style() for style in [&#39;white&#39;,&#39;dark&#39;,&#39;whitegrid&#39;,&#39;darkgrid&#39;,&#39;ticks&#39;]: sns.set_style(style) sns.distplot(df[&#39;Tuition&#39;]) plt.show() # Removing axes with despine() sns.set_style(&#39;white&#39;) sns.distplot(df[&#39;Tuition&#39;]) sns.despine(left=True) . Colors in Seaborn . # Defining a color for a plot sns.set(color_codes=True) sns.distplot(df[&#39;Tuition&#39;], color=&#39;g&#39;) # Palettes for p in sns.palettes.SEABORN_PALETTES: sns.set_palette(p) sns.distplot(df[&#39;Tuition&#39;]) # Displaying Palettes for p in sns.palettes.SEABORN_PALETTES: sns.set_palette(p) sns.palplot(sns.color_palette()) plt.show() # Defining Custom Palettes # Circular colors = when the data is not ordered sns.palplot(sns.color_palette(&quot;Paired&quot;, 12)) # Sequential colors = when the data has a consistent range from high to low sns.palplot(sns.color_palette(&quot;Blues&quot;, 12)) # Diverging colors = when both the low and high values are interesting sns.palplot(sns.color_palette(&quot;BrBG&quot;, 12)) . Customizing with matplotlib . # Matplotlib Axes fig, ax = plt.subplots() sns.distplot(df[&#39;Tuition&#39;], ax=ax) ax.set(xlabel=&quot;Tuition 2013-14&quot;) # Further Customizations fig, ax = plt.subplots() sns.distplot(df[&#39;Tuition&#39;], ax=ax) ax.set(xlabel=&quot;Tuition 2013-14&quot;,ylabel=&quot;Distribution&quot;, xlim=(0, 50000),title=&quot;2013-14 Tuition and Fees Distribution&quot;) # Combining Plots fig, (ax0, ax1) = plt.subplots(nrows=1,ncols=2, sharey=True, figsize=(7,4)) sns.distplot(df[&#39;Tuition&#39;], ax=ax0) sns.distplot(df.query(&#39;State == &quot;MN&quot;&#39;)[&#39;Tuition&#39;], ax=ax1) ax1.set(xlabel=&quot;Tuition (MN)&quot;, xlim=(0, 70000)) ax1.axvline(x=20000, label=&#39;My Budget&#39;, linestyle=&#39;--&#39;) ax1.legend() . Additional Plot Types . Categorical Plot Types . # Plots of each observation - stripplot sns.stripplot(data=df, y=&quot;DRG Definition&quot;, x=&quot;Average Covered Charges&quot;, jitter=True) # Plots of each observation - swarmplot sns.swarmplot(data=df, y=&quot;DRG Definition&quot;, x=&quot;Average Covered Charges&quot;) # Abstract representations - boxplot sns.boxplot(data=df, y=&quot;DRG Definition&quot;, x=&quot;Average Covered Charges&quot;) # Abstract representation - violinplot sns.violinplot(data=df, y=&quot;DRG Definition&quot;, x=&quot;Average Covered Charges&quot;) # Abstract representation - lvplot sns.lvplot(data=df, y=&quot;DRG Definition&quot;, x=&quot;Average Covered Charges&quot;) # Statistical estimates - barplot sns.barplot(data=df, y=&quot;DRG Definition&quot;, x=&quot;Average Covered Charges&quot;, hue=&quot;Region&quot;) # Statistical estimates - pointplot sns.pointplot(data=df, y=&quot;DRG Definition&quot;, x=&quot;Average Covered Charges&quot;, hue=&quot;Region&quot;) # Statistical estimates - countplot sns.countplot(data=df, y=&quot;DRG_Code&quot;, hue=&quot;Region&quot;) . Regression Plots . # Plotting with regplot() sns.regplot(data=df, x=&#39;temp&#39;, y=&#39;total_rentals&#39;, marker=&#39;+&#39;) # Evaluating regression with residplot() sns.residplot(data=df, x=&#39;temp&#39;, y=&#39;total_rentals&#39;) # Polynomial regression sns.regplot(data=df, x=&#39;temp&#39;, y=&#39;total_rentals&#39;, order=2) # residplot with polynomial regression sns.residplot(data=df, x=&#39;temp&#39;, y=&#39;total_rentals&#39;, order=2) # Categorical values sns.regplot(data=df, x=&#39;mnth&#39;, y=&#39;total_rentals&#39;, x_jitter=.1, order=2) # Estimators sns.regplot(data=df, x=&#39;mnth&#39;, y=&#39;total_rentals&#39;, x_estimator=np.mean, order=2) # Binning the data sns.regplot(data=df,x=&#39;temp&#39;,y=&#39;total_rentals&#39;, x_bins=4) . Matrix plots . # Getting data in the right format pd.crosstab(df[&quot;mnth&quot;], df[&quot;weekday&quot;], values=df[&quot;total_rentals&quot;],aggfunc=&#39;mean&#39;).round(0) # Build a heatmap sns.heatmap(pd.crosstab(df[&quot;mnth&quot;], df[&quot;weekday&quot;], values=df[&quot;total_rentals&quot;], aggfunc=&#39;mean&#39;) ) # Customize a heatmap sns.heatmap(df_crosstab, annot=True, fmt=&quot;d&quot;, cmap=&quot;YlGnBu&quot;, cbar=False, linewidths=.5) # Centering a heatmap sns.heatmap(df_crosstab, annot=True, fmt=&quot;d&quot;, cmap=&quot;YlGnBu&quot;, cbar=True, center=df_crosstab.loc[9, 6]) # Plotting a correlation matrix sns.heatmap(df.corr()) . Creating Plots on Data Aware Grids . Using FacetGrid, factorplot and lmplot . # FacetGrid Categorical Example g = sns.FacetGrid(df, col=&quot;HIGHDEG&quot;) g.map(sns.boxplot, &#39;Tuition&#39;, order=[&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;]) # factorplot() sns.factorplot(x=&quot;Tuition&quot;, data=df, col=&quot;HIGHDEG&quot;, kind=&#39;box&#39;) # FacetGrid for regression # FacetGrid() can also be used for sca er or regression plots g = sns.FacetGrid(df, col=&quot;HIGHDEG&quot;) g.map(plt.scatter, &#39;Tuition&#39;, &#39;SAT_AVG_ALL&#39;) # lmplot # lmplot plots sca er and regression plots on a FacetGrid sns.lmplot(data=df, x=&quot;Tuition&quot;, y=&quot;SAT_AVG_ALL&quot;, col=&quot;HIGHDEG&quot;, fit_reg=False) # lmplot with regression sns.lmplot(data=df, x=&quot;Tuition&quot;, y=&quot;SAT_AVG_ALL&quot;, col=&quot;HIGHDEG&quot;, row=&#39;REGION&#39;) . Using PairGrid and pairplot . # Creating a PairGrid g = sns.PairGrid(df, vars=[&quot;Fair_Mrkt_Rent&quot;, &quot;Median_Income&quot;]) g = g.map(plt.scatter) # Customizing the PairGrid diagonals g = sns.PairGrid(df, vars=[&quot;Fair_Mrkt_Rent&quot;, &quot;Median_Income&quot;]) g = g.map_diag(plt.hist) g = g.map_offdiag(plt.scatter) # Pairplot sns.pairplot(df, vars=[&quot;Fair_Mrkt_Rent&quot;, &quot;Median_Income&quot;], kind=&#39;reg&#39;, diag_kind=&#39;hist&#39;) # Customizing a pairplot sns.pairplot(df.query(&#39;BEDRMS &lt; 3&#39;),vars=[&quot;Fair_Mrkt_Rent&quot;,&quot;Median_Income&quot;, &quot;UTILITY&quot;],hue=&#39;BEDRMS&#39;, palette=&#39;husl&#39;, plot_kws={&#39;alpha&#39;: 0.5}) . Using JointGrid and jointplot . # Basic JointGrid g = sns.JointGrid(data=df, x=&quot;Tuition&quot;,y=&quot;ADM_RATE_ALL&quot;) g.plot(sns.regplot, sns.distplot) # Advanced JointGrid g = sns.JointGrid(data=df, x=&quot;Tuition&quot;,y=&quot;ADM_RATE_ALL&quot;) g = g.plot_joint(sns.kdeplot) g = g.plot_marginals(sns.kdeplot, shade=True) g = g.annotate(stats.pearsonr) # jointplot() sns.jointplot(data=df, x=&quot;Tuition&quot;,y=&quot;ADM_RATE_ALL&quot;, kind=&#39;hex&#39;) # Customizing a jointplot g = (sns.jointplot(x=&quot;Tuition&quot;, y=&quot;ADM_RATE_ALL&quot;, kind=&#39;scatter&#39;, xlim=(0, 25000), marginal_kws=dict(bins=15,rug=True), data=df.query(&#39;UG &lt; 2500 &amp; Ownership == &quot;Public&quot;&#39;)) .plot_joint(sns.kdeplot)) .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/seaborn-cheatsheet.html",
            "relUrl": "/blog/seaborn-cheatsheet.html",
            "date": " • Jan 19, 2021"
        }
        
    
  
    
        ,"post47": {
            "title": "Java installation on Ubuntu 20.04",
            "content": "Following these instructions: https://www.digitalocean.com/community/tutorials/how-to-install-java-with-apt-on-ubuntu-20-04-fr. . Current configuration . !java --version . openjdk 11.0.9.1 2020-11-04 OpenJDK Runtime Environment (build 11.0.9.1+1-Ubuntu-0ubuntu1.20.04) OpenJDK 64-Bit Server VM (build 11.0.9.1+1-Ubuntu-0ubuntu1.20.04, mixed mode, sharing) . Download Oracle JDK 11 . From https://launchpad.net/~linuxuprising/+archive/ubuntu/java/+packages, I can identify the focal version: . oracle-java11-installer-local - 11.0.9-1~linuxuprising0 (changes file) logix2 2020-10-22 Published Focal Java . I download the given version from Oracle website: https://www.oracle.com/java/technologies/javase-jdk11-downloads.html. Java SE Development Kit 11.0.9 Linux x64 Compressed Archive . And yes you have to login with an oracle account to download it. . Installation via linuxuprising/java . sudo add-apt-repository ppa:linuxuprising/java sudo apt update sudo mkdir -p /var/cache/oracle-jdk11-installer-local/ sudo cp ~/Downloads/jdk-11.0.9_linux-x64_bin.tar.gz /var/cache/oracle-jdk11-installer-local/ sudo apt install oracle-java11-installer-local . After accepting the license agreement, installation is running . Check . $ sudo update-alternatives --config java There are 2 choices for the alternative java (providing /usr/bin/java). Selection Path Priority Status 0 /usr/lib/jvm/java-11-openjdk-amd64/bin/java 1111 auto mode 1 /usr/lib/jvm/java-11-openjdk-amd64/bin/java 1111 manual mode * 2 /usr/lib/jvm/java-11-oracle/bin/java 1091 manual mode . Environment variable . Enter /usr/lib/jvm/java-11-oracle as your JAVA_HOME variable in /etc/environment . $ cat /etc/environment PATH=&quot;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin&quot; JAVA_HOME=&quot;/usr/lib/jvm/java-11-oracle&quot; $ source /etc/environment $ echo $JAVA_HOME /usr/lib/jvm/java-11-oracle .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/java-installation-on-ubuntu-20.04.html",
            "relUrl": "/blog/java-installation-on-ubuntu-20.04.html",
            "date": " • Jan 14, 2021"
        }
        
    
  
    
        ,"post48": {
            "title": "pandas cheatsheet",
            "content": "Data Manipulation with pandas . Transforming Data . Introducing DataFrames . # Exploring a DataFrame: .head() dogs.head() # Exploring a DataFrame: .info() dogs.info() # Exploring a DataFrame: .shape dogs.shape # Exploring a DataFrame: .describe() dogs.describe() # Components of a DataFrame: .values dogs.values # Components of a DataFrame: .columns and .index dogs.columns dogs.index . Sorting and subsetting . #Sorting by multiple variables dogs.sort_values([&quot;weight_kg&quot;, &quot;height_cm&quot;], ascending=[True, False]) #Subsetting based on dates dogs[dogs[&quot;date_of_birth&quot;] &gt; &quot;2015-01-01&quot;] #Subsetting based on multiple conditions is_lab = dogs[&quot;breed&quot;] == &quot;Labrador&quot; is_brown = dogs[&quot;color&quot;] == &quot;Brown&quot; dogs[is_lab &amp; is_brown] dogs[ (dogs[&quot;breed&quot;] == &quot;Labrador&quot;) &amp; (dogs[&quot;color&quot;] == &quot;Brown&quot;) ] #Subsetting using .isin() is_black_or_brown = dogs[&quot;color&quot;].isin([&quot;Black&quot;, &quot;Brown&quot;]) dogs[is_black_or_brown] . New columns . # Adding a new column dogs[&quot;height_m&quot;] = dogs[&quot;height_cm&quot;] / 100 . Aggregating Data . Summary statistics . #Summarizing numerical data dogs[&quot;height_cm&quot;].mean() .median() , .mode() .min() , .max() .var() , .std() .sum() .quantile() #The .agg() method def pct30(column): return column.quantile(0.3) dogs[&quot;weight_kg&quot;].agg(pct30) #Multiple summaries def pct40(column): return column.quantile(0.4) dogs[&quot;weight_kg&quot;].agg([pct30, pct40]) #Cumulative sum dogs[&quot;weight_kg&quot;].cumsum() #Cumulative statistics .cummax() .cummin() .cumprod() . Counting . #Dropping duplicate names vet_visits.drop_duplicates(subset=&quot;name&quot;) #Dropping duplicate pairs unique_dogs = vet_visits.drop_duplicates(subset=[&quot;name&quot;, &quot;breed&quot;]) #Counting unique_dogs[&quot;breed&quot;].value_counts(sort=True) . Grouped summary statistics . #Summaries by group dogs[dogs[&quot;color&quot;] == &quot;Black&quot;][&quot;weight_kg&quot;].mean() dogs[dogs[&quot;color&quot;] == &quot;Brown&quot;][&quot;weight_kg&quot;].mean() #Grouped summaries dogs.groupby(&quot;color&quot;)[&quot;weight_kg&quot;].mean() #Multiple grouped summaries dogs.groupby(&quot;color&quot;)[&quot;weight_kg&quot;].agg([min, max, sum]) #Grouping by multiple variables dogs.groupby([&quot;color&quot;, &quot;breed&quot;])[&quot;weight_kg&quot;].mean() #Many groups, many summaries dogs.groupby([&quot;color&quot;, &quot;breed&quot;])[[&quot;weight_kg&quot;, &quot;height_cm&quot;]].mean() . Pivot tables . #pivot table dogs.pivot_table(values=&quot;weight_kg&quot;,index=&quot;color&quot;) #Different statistics import numpy as np dogs.pivot_table(values=&quot;weight_kg&quot;, index=&quot;color&quot;, aggfunc=np.median) #Multiple statistics dogs.pivot_table(values=&quot;weight_kg&quot;, index=&quot;color&quot;, aggfunc=[np.mean, np.median]) #Pivot on two variables dogs.groupby([&quot;color&quot;, &quot;breed&quot;])[&quot;weight_kg&quot;].mean() dogs.pivot_table(values=&quot;weight_kg&quot;, index=&quot;color&quot;, columns=&quot;breed&quot;) #Filling missing values in pivot tables dogs.pivot_table(values=&quot;weight_kg&quot;, index=&quot;color&quot;, columns=&quot;breed&quot;, fill_value=0) # Summing with pivot tables dogs.pivot_table(values=&quot;weight_kg&quot;, index=&quot;color&quot;, columns=&quot;breed&quot;, fill_value=0, margins=True) . Slicing and Indexing . Explicit indexes . # Setting a column as the index dogs_ind = dogs.set_index(&quot;name&quot;) # Removing an index dogs_ind.reset_index() # Dropping an index dogs_ind.reset_index(drop=True) # Indexes make subsetting simpler dogs[dogs[&quot;name&quot;].isin([&quot;Bella&quot;, &quot;Stella&quot;])] # versus dogs_ind.loc[[&quot;Bella&quot;, &quot;Stella&quot;]] # Multi-level indexes a.k.a. hierarchical indexes dogs_ind3 = dogs.set_index([&quot;breed&quot;, &quot;color&quot;]) # Subset the outer level with a list dogs_ind3.loc[[&quot;Labrador&quot;, &quot;Chihuahua&quot;]] # Subset inner levels with a list of tuples dogs_ind3.loc[[(&quot;Labrador&quot;, &quot;Brown&quot;), (&quot;Chihuahua&quot;, &quot;Tan&quot;)]] # Sorting by index values dogs_ind3.sort_index() # Controlling sort_index dogs_ind3.sort_index(level=[&quot;color&quot;, &quot;breed&quot;], ascending=[True, False]) . Slicing and subsetting with .loc and .iloc . # Sort the index before you slice dogs_srt = dogs.set_index([&quot;breed&quot;, &quot;color&quot;]).sort_index() # Slicing the outer index level dogs_srt.loc[&quot;Chow Chow&quot;:&quot;Poodle&quot;] # Slicing the inner index levels correctly dogs_srt.loc[(&quot;Labrador&quot;, &quot;Brown&quot;):(&quot;Schnauzer&quot;, &quot;Grey&quot;)] # Slicing columns dogs_srt.loc[:, &quot;name&quot;:&quot;height_cm&quot;] # Slice twice dogs_srt.loc[ (&quot;Labrador&quot;, &quot;Brown&quot;):(&quot;Schnauzer&quot;, &quot;Grey&quot;), &quot;name&quot;:&quot;height_cm&quot;] # Dog days dogs = dogs.set_index(&quot;date_of_birth&quot;).sort_index() # Slicing by dates # Get dogs with date_of_birth between 2014-08-25 and 2016-09-16 dogs.loc[&quot;2014-08-25&quot;:&quot;2016-09-16&quot;] # Slicing by partial dates # Get dogs with date_of_birth between 2014-01-01 and 2016-12-31 dogs.loc[&quot;2014&quot;:&quot;2016&quot;] # Subsetting by row/column number print(dogs.iloc[2:5, 1:4]) . Working with pivot tables . # Pivoting the dog pack dogs_height_by_breed_vs_color = dog_pack.pivot_table( &quot;height_cm&quot;, index=&quot;breed&quot;, columns=&quot;color&quot;) # The axis argument dogs_height_by_breed_vs_color.mean(axis=&quot;index&quot;) # Calculating summary stats across columns dogs_height_by_breed_vs_color.mean(axis=&quot;columns&quot;) . Creating and Visualizing DataFrames . Visualizing your data . # Histograms import matplotlib.pyplot as plt dog_pack[&quot;height_cm&quot;].hist(bins=20) # Bar plots avg_weight_by_breed = dog_pack.groupby(&quot;breed&quot;)[&quot;weight_kg&quot;].mean() avg_weight_by_breed.plot(kind=&quot;bar&quot;, title=&quot;Mean Weight by Dog Breed&quot;) # Line plots sully.head() sully.plot(x=&quot;date&quot;, y=&quot;weight_kg&quot;, kind=&quot;line&quot;) # Rotating axis labels sully.plot(x=&quot;date&quot;, y=&quot;weight_kg&quot;, kind=&quot;line&quot;, rot=45) # Scatter plots dog_pack.plot(x=&quot;height_cm&quot;, y=&quot;weight_kg&quot;, kind=&quot;scatter&quot;) # Layering plots dog_pack[dog_pack[&quot;sex&quot;]==&quot;F&quot;][&quot;height_cm&quot;].hist() dog_pack[dog_pack[&quot;sex&quot;]==&quot;M&quot;][&quot;height_cm&quot;].hist() # Add a legend plt.legend([&quot;F&quot;, &quot;M&quot;]) # Transparency dog_pack[dog_pack[&quot;sex&quot;]==&quot;F&quot;][&quot;height_cm&quot;].hist(alpha=0.7) dog_pack[dog_pack[&quot;sex&quot;]==&quot;M&quot;][&quot;height_cm&quot;].hist(alpha=0.7) plt.legend([&quot;F&quot;, &quot;M&quot;]) . Missing values . # Detecting missing values dogs.isna() # Detecting any missing values dogs.isna().any() # Counting missing values dogs.isna().sum() # Plotting missing values import matplotlib.pyplot as plt dogs.isna().sum().plot(kind=&quot;bar&quot;) plt.show() # Removing rows containing missing values dogs.dropna() # Replacing missing values dogs.fillna(0) . Reading and writing CSVs . # CSV to DataFrame import pandas as pd new_dogs = pd.read_csv(&quot;new_dogs.csv&quot;) # DataFrame to CSV new_dogs.to_csv(&quot;new_dogs_with_bmi.csv&quot;) # CSV to dataframe parsing dates, and having date as index climate_change = pd.read_csv(prefix+&#39;climate_change.csv&#39;, parse_dates=[&#39;date&#39;], index_col=&#39;date&#39;) . Joining data with pandas . Data merging basics . Inner join . # Inner join wards_census = wards.merge(census, on=&#39;ward&#39;) # Suffixes wards_census = wards.merge(census, on=&#39;ward&#39;, suffixes=(&#39;_ward&#39;,&#39;_cen&#39;)) . One-to-many relationships . # One-to-many example ward_licenses = wards.merge(licenses, on=&#39;ward&#39;, suffixes=(&#39;_ward&#39;,&#39;_lic&#39;)) . Merging multiple DataFrames . # Single merge grants.merge(licenses, on=[&#39;address&#39;,&#39;zip&#39;]) # Merging multiple tables grants_licenses_ward = grants.merge(licenses, on=[&#39;address&#39;,&#39;zip&#39;]) .merge(wards, on=&#39;ward&#39;, suffixes=(&#39;_bus&#39;,&#39;_ward&#39;)) # Plot Results import matplotlib.pyplot as plt grant_licenses_ward.groupby(&#39;ward&#39;).agg(&#39;sum&#39;).plot(kind=&#39;bar&#39;, y=&#39;grant&#39;) . Merging Tables With Different Join Types . Left join . # Merge with left join movies_taglines = movies.merge(taglines, on=&#39;id&#39;, how=&#39;left&#39;) . Other joins . # Merge with right join tv_movies = movies.merge(tv_genre, how=&#39;right&#39;, left_on=&#39;id&#39;, right_on=&#39;movie_id&#39;) # Merge with outer join family_comedy = family.merge(comedy, on=&#39;movie_id&#39;, how=&#39;outer&#39;, suffixes=(&#39;_fam&#39;, &#39;_com&#39;)) . Merging a table to itself . # Merging a table to itself original_sequels = sequels.merge(sequels, left_on=&#39;sequel&#39;, right_on=&#39;id&#39;, suffixes=(&#39;_org&#39;,&#39;_seq&#39;)) . Merging on indexes . # Setting an index movies = pd.read_csv(&#39;tmdb_movies.csv&#39;, index_col=[&#39;id&#39;]) # Merging on index movies_taglines = movies.merge(taglines, on=&#39;id&#39;, how=&#39;left&#39;) # MultiIndex merge samuel_casts = samuel.merge(casts, on=[&#39;movie_id&#39;,&#39;cast_id&#39;]) # Index merge with left_on and right_on movies_genres = movies.merge(movie_to_genres, left_on=&#39;id&#39;, left_index=True, right_on=&#39;movie_id&#39;, right_index=True) . Advanced Merging and Concatenating . Filtering joins . ########### # semi-join # Step 1 - semi-join genres_tracks = genres.merge(top_tracks, on=&#39;gid&#39;) # Step 2 - semi-join genres[&#39;gid&#39;].isin(genres_tracks[&#39;gid&#39;]) # Step 3 - semi-join genres_tracks = genres.merge(top_tracks, on=&#39;gid&#39;) top_genres = genres[genres[&#39;gid&#39;].isin(genres_tracks[&#39;gid&#39;])] ########### # anti-join # Step 1 - anti-join genres_tracks = genres.merge(top_tracks, on=&#39;gid&#39;, how=&#39;left&#39;, indicator=True) # Step 2 - anti-join gid_list = genres_tracks.loc[genres_tracks[&#39;_merge&#39;] == &#39;left_only&#39;, &#39;gid&#39;] # Step 3 - anti-join genres_tracks = genres.merge(top_tracks, on=&#39;gid&#39;, how=&#39;left&#39;, indicator=True) gid_list = genres_tracks.loc[genres_tracks[&#39;_merge&#39;] == &#39;left_only&#39;,&#39;gid&#39;] non_top_genres = genres[genres[&#39;gid&#39;].isin(gid_list)] . Concatenate DataFrames together vertically . # Basic concatenation pd.concat([inv_jan, inv_feb, inv_mar]) # Ignoring the index pd.concat([inv_jan, inv_feb, inv_mar], ignore_index=True) # Setting labels to original tables pd.concat([inv_jan, inv_feb, inv_mar], ignore_index=False, keys=[&#39;jan&#39;,&#39;feb&#39;,&#39;mar&#39;]) # Concatenate tables with different column names pd.concat([inv_jan, inv_feb], sort=True) # Concatenate tables with different column names pd.concat([inv_jan, inv_feb], join=&#39;inner&#39;) # Append the tables inv_jan.append([inv_feb, inv_mar], ignore_index=True, sort=True) . Verifying integrity . # Validating merges .merge(validate=None) : Checks if merge is of specified type &#39;one_to_one&#39; &#39;one_to_many&#39; &#39;many_to_one&#39; &#39;many_to_many&#39; # Merge validate: one_to_one tracks.merge(specs, on=&#39;tid&#39;, validate=&#39;one_to_one&#39;) # Merge validate: one_to_many albums.merge(tracks, on=&#39;aid&#39;, validate=&#39;one_to_many&#39;) # Verifying concatenations .concat(verify_integrity=False) : Check whether the new concatenated index contains duplicates Default value is False . Merging Ordered and Time-Series Data . Using merge_ordered() . # Merging stock data import pandas as pd pd.merge_ordered(appl, mcd, on=&#39;date&#39;, suffixes=(&#39;_aapl&#39;,&#39;_mcd&#39;)) # Forward fill example pd.merge_ordered(appl, mcd, on=&#39;date&#39;, suffixes=(&#39;_aapl&#39;,&#39;_mcd&#39;), fill_method=&#39;ffill&#39;) . Using merge_asof() . # merge_asof() example pd.merge_asof(visa, ibm, on=&#39;date_time&#39;, suffixes=(&#39;_visa&#39;,&#39;_ibm&#39;)) # merge_asof() example with direction pd.merge_asof(visa, ibm, on=[&#39;date_time&#39;], suffixes=(&#39;_visa&#39;,&#39;_ibm&#39;), direction=&#39;forward&#39;) . Selecting data with .query() . # Querying on a single condition stocks.query(&#39;nike &gt;= 90&#39;) # Querying on a multiple conditions, &quot;and&quot;, &quot;or&quot; stocks.query(&#39;nike &gt; 90 and disney &lt; 140&#39;) stocks.query(&#39;nike &gt; 96 or disney &lt; 98&#39;) # Using .query() to select text stocks_long.query(&#39;stock==&quot;disney&quot; or (stock==&quot;nike&quot; and close &lt; 90)&#39;) . Reshaping data with .melt() . # Example of .melt() social_fin_tall = social_fin.melt(id_vars=[&#39;financial&#39;,&#39;company&#39;]) # Melting with value_vars social_fin_tall = social_fin.melt(id_vars=[&#39;financial&#39;,&#39;company&#39;], value_vars=[&#39;2018&#39;,&#39;2017&#39;]) # Melting with column names social_fin_tall = social_fin.melt(id_vars=[&#39;financial&#39;,&#39;company&#39;], value_vars=[&#39;2018&#39;,&#39;2017&#39;], var_name=[&#39;year&#39;], value_name=&#39;dollars&#39;) .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/pandas-cheatsheet.html",
            "relUrl": "/blog/pandas-cheatsheet.html",
            "date": " • Jan 13, 2021"
        }
        
    
  
    
        ,"post49": {
            "title": "matplotlib cheatsheet",
            "content": "Introduction to Data Visualization with Matplotlib . matplotlib cheatsheet in pdf . pdf lecture in github . Introduction to Matplotlib . Introduction to data visualization with Matplotlib . # Introducing the pyplot interface import matplotlib.pyplot as plt fig, ax = plt.subplots() plt.show() # Adding data to axes ax.plot(seattle_weather[&quot;MONTH&quot;], seattle_weather[&quot;MLY-TAVG-NORMAL&quot;]) plt.show() . Customizing your plots . # Adding markers ax.plot(seattle_weather[&quot;MONTH&quot;], seattle_weather[&quot;MLY-PRCP-NORMAL&quot;], marker=&quot;o&quot;) plt.show() # Choosing markers ax.plot(seattle_weather[&quot;MONTH&quot;], seattle_weather[&quot;MLY-PRCP-NORMAL&quot;], marker=&quot;v&quot;) plt.show() . markers . # Setting the linestyle fig, ax = plt.subplots() ax.plot(seattle_weather[&quot;MONTH&quot;], seattle_weather[&quot;MLY-TAVG-NORMAL&quot;], marker=&quot;v&quot;, linestyle=&quot;--&quot;) plt.show() . line style . # Eliminating lines with linestyle fig, ax = plt.subplots() ax.plot(seattle_weather[&quot;MONTH&quot;], seattle_weather[&quot;MLY-TAVG-NORMAL&quot;], marker=&quot;v&quot;, linestyle=&quot;None&quot;) plt.show() # Choosing color fig, ax = plt.subplots() ax.plot(seattle_weather[&quot;MONTH&quot;], seattle_weather[&quot;MLY-TAVG-NORMAL&quot;], marker=&quot;v&quot;, linestyle=&quot;--&quot;, color=&quot;r&quot;) plt.show() # Customizing the axes labels ax.set_xlabel(&quot;Time (months)&quot;) plt.show() # Setting the y axis label ax.set_xlabel(&quot;Time (months)&quot;) ax.set_ylabel(&quot;Average temperature (Fahrenheit degrees)&quot;) plt.show() # Adding a title ax.set_title(&quot;Weather in Seattle&quot;) plt.show() . Small multiples . # Small multiples with plt.subplots fig, ax = plt.subplots(3, 2) plt.show() # Adding data to subplots ax.shape (3, 2) ax[0, 0].plot(seattle_weather[&quot;MONTH&quot;],seattle_weather[&quot;MLY-PRCP-NORMAL&quot;],color=&#39;b&#39;) plt.show() # Subplots with data fig, ax = plt.subplots(2, 1) ax[0].plot(seattle_weather[&quot;MONTH&quot;], seattle_weather[&quot;MLY-PRCP-NORMAL&quot;],color=&#39;b&#39;) ax[0].plot(seattle_weather[&quot;MONTH&quot;], seattle_weather[&quot;MLY-PRCP-25PCTL&quot;],linestyle=&#39;--&#39;, color=&#39;b&#39;) ax[0].plot(seattle_weather[&quot;MONTH&quot;], seattle_weather[&quot;MLY-PRCP-75PCTL&quot;],linestyle=&#39;--&#39;, color=&#39;b&#39;) ax[1].plot(austin_weather[&quot;MONTH&quot;], austin_weather[&quot;MLY-PRCP-NORMAL&quot;],color=&#39;r&#39;) ax[1].plot(austin_weather[&quot;MONTH&quot;], austin_weather[&quot;MLY-PRCP-25PCTL&quot;],linestyle=&#39;--&#39;, color=&#39;r&#39;) ax[1].plot(austin_weather[&quot;MONTH&quot;], austin_weather[&quot;MLY-PRCP-75PCTL&quot;],linestyle=&#39;--&#39;, color=&#39;r&#39;) ax[0].set_ylabel(&quot;Precipitation (inches)&quot;) ax[1].set_ylabel(&quot;Precipitation (inches)&quot;) ax[1].set_xlabel(&quot;Time (months)&quot;) plt.show() # Sharing the y-axis range fig, ax = plt.subplots(2, 1, sharey=True) . Plotting time-series . Plotting time-series data . # DateTimeIndex climate_change.index DatetimeIndex([&#39;1958-03-06&#39;, &#39;1958-04-06&#39;, &#39;1958-05-06&#39;, &#39;1958-06-06&#39;, dtype=&#39;datetime64[ns]&#39;, name=&#39;date&#39;, length=706, freq=None) # Plotting time-series data import matplotlib.pyplot as plt fig, ax = plt.subplots() ax.plot(climate_change.index, climate_change[&#39;co2&#39;]) ax.set_xlabel(&#39;Time&#39;) ax.set_ylabel(&#39;CO2 (ppm)&#39;) plt.show() # Zooming in on a decade sixties = climate_change[&quot;1960-01-01&quot;:&quot;1969-12-31&quot;] fig, ax = plt.subplots() ax.plot(sixties.index, sixties[&#39;co2&#39;]) ax.set_xlabel(&#39;Time&#39;) ax.set_ylabel(&#39;CO2 (ppm)&#39;) plt.show() # Zooming in on one year sixty_nine = climate_change[&quot;1969-01-01&quot;:&quot;1969-12-31&quot;] fig, ax = plt.subplots() ax.plot(sixty_nine.index, sixty_nine[&#39;co2&#39;]) ax.set_xlabel(&#39;Time&#39;) ax.set_ylabel(&#39;CO2 (ppm)&#39;) plt.show() . Plotting time-series with different variables . # Plotting two time-series together import matplotlib.pyplot as plt fig, ax = plt.subplots() ax.plot(climate_change.index, climate_change[&quot;co2&quot;]) ax.plot(climate_change.index, climate_change[&quot;relative_temp&quot;]) ax.set_xlabel(&#39;Time&#39;) ax.set_ylabel(&#39;CO2 (ppm) / Relative temperature&#39;) plt.show() # Using twin axes fig, ax = plt.subplots() ax.plot(climate_change.index, climate_change[&quot;co2&quot;]) ax.set_xlabel(&#39;Time&#39;) ax.set_ylabel(&#39;CO2 (ppm)&#39;) ax2 = ax.twinx() ax2.plot(climate_change.index, climate_change[&quot;relative_temp&quot;]) ax2.set_ylabel(&#39;Relative temperature (Celsius)&#39;) plt.show() # Separating variables by color fig, ax = plt.subplots() ax.plot(climate_change.index, climate_change[&quot;co2&quot;], color=&#39;blue&#39;) ax.set_xlabel(&#39;Time&#39;) ax.set_ylabel(&#39;CO2 (ppm)&#39;, color=&#39;blue&#39;) ax2 = ax.twinx() ax2.plot(climate_change.index, climate_change[&quot;relative_temp&quot;], color=&#39;red&#39;) ax2.set_ylabel(&#39;Relative temperature (Celsius)&#39;, color=&#39;red&#39;) plt.show() # Coloring the ticks fig, ax = plt.subplots() ax.plot(climate_change.index, climate_change[&quot;co2&quot;], color=&#39;blue&#39;) ax.set_xlabel(&#39;Time&#39;) ax.set_ylabel(&#39;CO2 (ppm)&#39;, color=&#39;blue&#39;) ax.tick_params(&#39;y&#39;, colors=&#39;blue&#39;) ax2 = ax.twinx() ax2.plot(climate_change.index, climate_change[&quot;relative_temp&quot;], color=&#39;red&#39;) ax2.set_ylabel(&#39;Relative temperature (Celsius)&#39;, color=&#39;red&#39;) ax2.tick_params(&#39;y&#39;, colors=&#39;red&#39;) plt.show() # A function that plots time-series def plot_timeseries(axes, x, y, color, xlabel, ylabel): axes.plot(x, y, color=color) axes.set_xlabel(xlabel) axes.set_ylabel(ylabel, color=color) axes.tick_params(&#39;y&#39;, colors=color) # Using our function fig, ax = plt.subplots() plot_timeseries(ax, climate_change.index, climate_change[&#39;co2&#39;],&#39;blue&#39;, &#39;Time&#39;, &#39;CO2 (ppm)&#39;) ax2 = ax.twinx() plot_timeseries(ax, climate_change.index,climate_change[&#39;relative_temp&#39;],&#39;red&#39;, &#39;Time&#39;, &#39;Relative temperature (Celsius)&#39;) plt.show() . Annotating time-series data . # Annotation fig, ax = plt.subplots() plot_timeseries(ax, climate_change.index, climate_change[&#39;co2&#39;], &#39;blue&#39;, &#39;Time&#39;, &#39;CO2 (ppm)&#39;) ax2 = ax.twinx() plot_timeseries(ax2, climate_change.index, climate_change[&#39;relative_temp&#39;], &#39;red&#39;, &#39;Time&#39;, &#39;Relative temperature (Celsius)&#39;) ax2.annotate(&quot;&gt;1 degree&quot;, xy=[pd.TimeStamp(&quot;2015-10-06&quot;), 1]) plt.show() # Positioning the text ax2.annotate(&quot;&gt;1 degree&quot;, xy=(pd.Timestamp(&#39;2015-10-06&#39;), 1), xytext=(pd.Timestamp(&#39;2008-10-06&#39;), -0.2)) # Adding arrows to annotation ax2.annotate(&quot;&gt;1 degree&quot;, xy=(pd.Timestamp(&#39;2015-10-06&#39;), 1), xytext=(pd.Timestamp(&#39;2008-10-06&#39;), -0.2), arrowprops={}) # Customizing arrow properties ax2.annotate(&quot;&gt;1 degree&quot;, xy=(pd.Timestamp(&#39;2015-10-06&#39;), 1), xytext=(pd.Timestamp(&#39;2008-10-06&#39;), -0.2), arrowprops={&quot;arrowstyle&quot;:&quot;-&gt;&quot;, &quot;color&quot;:&quot;gray&quot;}) . Customizing annotations . Quantitative comparisons and statistical visualizations . Quantitative comparisons: bar-charts . # Olympic medals: visualizing the data medals = pd.read_csv(&#39;medals_by_country_2016.csv&#39;, index_col=0) fig, ax = plt.subplots() ax.bar(medals.index, medals[&quot;Gold&quot;]) plt.show() # Interlude: rotate the tick labels fig, ax = plt.subplots() ax.bar(medals.index, medals[&quot;Gold&quot;]) ax.set_xticklabels(medals.index, rotation=90) ax.set_ylabel(&quot;Number of medals&quot;) plt.show() # Olympic medals: visualizing the other medals : stacked bar chart fig, ax = plt.subplots ax.bar(medals.index, medals[&quot;Gold&quot;]) ax.bar(medals.index, medals[&quot;Silver&quot;], bottom=medals[&quot;Gold&quot;]) ax.set_xticklabels(medals.index, rotation=90) ax.set_ylabel(&quot;Number of medals&quot;) plt.show() # Olympic medals: visualizing all three fig, ax = plt.subplots ax.bar(medals.index, medals[&quot;Gold&quot;]) ax.bar(medals.index, medals[&quot;Silver&quot;], bottom=medals[&quot;Gold&quot;]) ax.bar(medals.index, medals[&quot;Bronze&quot;], bottom=medals[&quot;Gold&quot;] + medals[&quot;Silver&quot;]) ax.set_xticklabels(medals.index, rotation=90) ax.set_ylabel(&quot;Number of medals&quot;) plt.show() # Adding a legend fig, ax = plt.subplots ax.bar(medals.index, medals[&quot;Gold&quot;], label=&quot;Gold&quot;) ax.bar(medals.index, medals[&quot;Silver&quot;], bottom=medals[&quot;Gold&quot;], label=&quot;Silver&quot;) ax.bar(medals.index, medals[&quot;Bronze&quot;], bottom=medals[&quot;Gold&quot;] + medals[&quot;Silver&quot;], label=&quot;Bronze&quot;) ax.set_xticklabels(medals.index, rotation=90) ax.set_ylabel(&quot;Number of medals&quot;) ax.legend() plt.show() . Quantitative comparisons: histograms . # Introducing histograms fig, ax = plt.subplots() ax.hist(mens_rowing[&quot;Height&quot;]) ax.hist(mens_gymnastic[&quot;Height&quot;]) ax.set_xlabel(&quot;Height (cm)&quot;) ax.set_ylabel(&quot;# of observations&quot;) plt.show() # Labels are needed ax.hist(mens_rowing[&quot;Height&quot;], label=&quot;Rowing&quot;) ax.hist(mens_gymnastic[&quot;Height&quot;], label=&quot;Gymnastics&quot;) ax.set_xlabel(&quot;Height (cm)&quot;) ax.set_ylabel(&quot;# of observations&quot;) ax.legend() plt.show() # Customizing histograms: setting the number of bins ax.hist(mens_rowing[&quot;Height&quot;], label=&quot;Rowing&quot;, bins=5) ax.hist(mens_gymnastic[&quot;Height&quot;], label=&quot;Gymnastics&quot;, bins=5) ax.set_xlabel(&quot;Height (cm)&quot;) ax.set_ylabel(&quot;# of observations&quot;) ax.legend() plt.show() # Customizing histograms: setting bin boundaries ax.hist(mens_rowing[&quot;Height&quot;], label=&quot;Rowing&quot;, bins=[150, 160, 170, 180, 190, 200, 210]) ax.hist(mens_gymnastic[&quot;Height&quot;], label=&quot;Gymnastics&quot;, bins=[150, 160, 170, 180, 190, 200, 210]) ax.set_xlabel(&quot;Height (cm)&quot;) ax.set_ylabel(&quot;# of observations&quot;) ax.legend() plt.show() # Customizing histograms: transparency ax.hist(mens_rowing[&quot;Height&quot;], label=&quot;Rowing&quot;, bins=[150, 160, 170, 180, 190, 200, 210], histtype=&quot;step&quot;) ax.hist(mens_gymnastic[&quot;Height&quot;], label=&quot;Gymnastics&quot;, bins=[150, 160, 170, 180, 190, 200, 210], histtype=&quot;step&quot;) ax.set_xlabel(&quot;Height (cm)&quot;) ax.set_ylabel(&quot;# of observations&quot;) ax.legend() plt.show() . Statistical plotting . # Adding error bars to bar charts fig, ax = plt.subplots() ax.bar(&quot;Rowing&quot;,mens_rowing[&quot;Height&quot;].mean(), yerr=mens_rowing[&quot;Height&quot;].std()) ax.bar(&quot;Gymnastics&quot;,mens_gymnastics[&quot;Height&quot;].mean(), yerr=mens_gymnastics[&quot;Height&quot;].std()) ax.set_ylabel(&quot;Height (cm)&quot;) plt.show() # Adding error bars to plots fig, ax = plt.subplots() ax.errorbar(seattle_weather[&quot;MONTH&quot;], seattle_weather[&quot;MLY-TAVG-NORMAL&quot;], yerr=seattle_weather[&quot;MLY-TAVG-STDDEV&quot;]) ax.errorbar(austin_weather[&quot;MONTH&quot;], austin_weather[&quot;MLY-TAVG-NORMAL&quot;], yerr=austin_weather[&quot;MLY-TAVG-STDDEV&quot;]) ax.set_ylabel(&quot;Temperature (Fahrenheit)&quot;) plt.show() # Adding boxplots fig, ax = plt.subplots() ax.boxplot([mens_rowing[&quot;Height&quot;], mens_gymnastics[&quot;Height&quot;]]) ax.set_xticklabels([&quot;Rowing&quot;, &quot;Gymnastics&quot;]) ax.set_ylabel(&quot;Height (cm)&quot;) plt.show() . Quantitative comparisons: scatter plots . # Introducing scatter plots fig, ax = plt.subplots() ax.scatter(climate_change[&quot;co2&quot;], climate_change[&quot;relative_temp&quot;]) ax.set_xlabel(&quot;CO2 (ppm)&quot;) ax.set_ylabel(&quot;Relative temperature (Celsius)&quot;) plt.show() # Customizing scatter plots eighties = climate_change[&quot;1980-01-01&quot;:&quot;1989-12-31&quot;] nineties = climate_change[&quot;1990-01-01&quot;:&quot;1999-12-31&quot;] fig, ax = plt.subplots() ax.scatter(eighties[&quot;co2&quot;], eighties[&quot;relative_temp&quot;], color=&quot;red&quot;, label=&quot;eighties&quot;) ax.scatter(nineties[&quot;co2&quot;], nineties[&quot;relative_temp&quot;], color=&quot;blue&quot;, label=&quot;nineties&quot;) ax.legend() ax.set_xlabel(&quot;CO2 (ppm)&quot;) ax.set_ylabel(&quot;Relative temperature (Celsius)&quot;) plt.show() # Encoding a third variable by color fig, ax = plt.subplots() ax.scatter(climate_change[&quot;co2&quot;], climate_change[&quot;relative_temp&quot;], c=climate_change.index) ax.set_xlabel(&quot;CO2 (ppm)&quot;) ax.set_ylabel(&quot;Relative temperature (Celsius)&quot;) plt.show() . Sharing visualizations with others . Preparing your figures to share with others . # Choosing a style plt.style.use(&quot;ggplot&quot;) fig, ax = plt.subplots() ax.plot(seattle_weather[&quot;MONTH&quot;], seattle_weather[&quot;MLY-TAVG-NORMAL&quot; ax.plot(austin_weather[&quot;MONTH&quot;], austin_weather[&quot;MLY-TAVG-NORMAL&quot;]) ax.set_xlabel(&quot;Time (months)&quot;) ax.set_ylabel(&quot;Average temperature (Fahrenheit degrees)&quot;) plt.show() # Back to the default plt.style.use(&quot;default&quot;) . available styles . # The &quot;bmh&quot; style plt.style.use(&quot;bmh&quot;) fig, ax = plt.subplots() ax.plot(seattle_weather[&quot;MONTH&quot;], seattle_weather[&quot;MLY-TAVG-NORMAL&quot; ax.plot(austin_weather[&quot;MONTH&quot;], austin_weather[&quot;MLY-TAVG-NORMAL&quot;]) ax.set_xlabel(&quot;Time (months)&quot;) ax.set_ylabel(&quot;Average temperature (Fahrenheit degrees)&quot;) plt.show() # Seaborn styles plt.style.use(&quot;seaborn-colorblind&quot;) fig, ax = plt.subplots() ax.plot(seattle_weather[&quot;MONTH&quot;], seattle_weather[&quot;MLY-TAVG-NORMAL&quot; ax.plot(austin_weather[&quot;MONTH&quot;], austin_weather[&quot;MLY-TAVG-NORMAL&quot;]) ax.set_xlabel(&quot;Time (months)&quot;) ax.set_ylabel(&quot;Average temperature (Fahrenheit degrees)&quot;) plt.show() . Saving your visualizations . # Saving the figure to file fig, ax = plt.subplots() ax.bar(medals.index, medals[&quot;Gold&quot;]) ax.set_xticklabels(medals.index, rotation=90) ax.set_ylabel(&quot;Number of medals&quot;) fig.savefig(&quot;gold_medals.png&quot;) # Different file formats fig.savefig(&quot;gold_medals.jpg&quot;) fig.savefig(&quot;gold_medals.jpg&quot;, quality=50) fig.savefig(&quot;gold_medals.svg&quot;) # Resolution fig.savefig(&quot;gold_medals.png&quot;, dpi=300) # Size fig.set_size_inches([5, 3]) # Another aspect ratio fig.set_size_inches([3, 5]) . Automating figures from data . # Getting unique values of a column sports = summer_2016_medals[&quot;Sport&quot;].unique() # Bar-chart of heights for all sports fig, ax = plt.subplots() for sport in sports: sport_df = summer_2016_medals[summer_2016_medals[&quot;Sport&quot;] == spor ax.bar(sport, sport_df[&quot;Height&quot;].mean(), yerr=sport_df[&quot;Height&quot;].std()) ax.set_ylabel(&quot;Height (cm)&quot;) ax.set_xticklabels(sports, rotation=90) plt.show() .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/matplotlib-cheatsheet.html",
            "relUrl": "/blog/matplotlib-cheatsheet.html",
            "date": " • Jan 13, 2021"
        }
        
    
  
    
        ,"post50": {
            "title": "How To Install Packages from the Jupyter Notebook",
            "content": "This is directly from https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/. . Here are my own experimentations following this article detailed explanations. . Quick Fix: How To Install Packages from the Jupyter Notebook . . import sys !conda install --yes --prefix {sys.prefix} matplotlib . Collecting package metadata (current_repodata.json): done Solving environment: done # All requested packages already installed. . The Details: Why is Installation from Jupyter so Messy? . How your operating system locates executables . !echo $PATH . /home/explore/gems/bin:/home/explore/miniconda3/envs/pytorch/bin:/home/explore/miniconda3/condabin:/home/explore/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin . !type python . python is /home/explore/miniconda3/envs/pytorch/bin/python . You can optionally add the -a tag to see all available versions of the command in your current shell environment; for example: . !type -a python . python is /home/explore/miniconda3/envs/pytorch/bin/python python is /usr/bin/python . !type -a conda . conda is /home/explore/miniconda3/condabin/conda . How Python locates packages . import sys sys.path . [&#39;/home/explore/git/guillaume/blog/_notebooks&#39;, &#39;/home/explore/miniconda3/envs/pytorch/lib/python38.zip&#39;, &#39;/home/explore/miniconda3/envs/pytorch/lib/python3.8&#39;, &#39;/home/explore/miniconda3/envs/pytorch/lib/python3.8/lib-dynload&#39;, &#39;&#39;, &#39;/home/explore/miniconda3/envs/pytorch/lib/python3.8/site-packages&#39;, &#39;/home/explore/miniconda3/envs/pytorch/lib/python3.8/site-packages/IPython/extensions&#39;, &#39;/home/explore/.ipython&#39;] . By default, the first place Python looks for a module is an empty path, meaning the current working directory. If the module is not found there, it goes down the list of locations until the module is found. You can find out which location has been used using the __path__ attribute of an imported module: . import numpy numpy.__path__ . [&#39;/home/explore/miniconda3/envs/pytorch/lib/python3.8/site-packages/numpy&#39;] . by printing the sys.path variables for each of the available python executables in my path, using Jupyter&#39;s delightful ability to mix Python and bash commands in a single code block: . paths = !type -a python for path in set(paths): path = path.split()[-1] print(path) !{path} -c &quot;import sys; print(sys.path)&quot; print() . /usr/bin/python [&#39;&#39;, &#39;/usr/lib/python2.7&#39;, &#39;/usr/lib/python2.7/plat-x86_64-linux-gnu&#39;, &#39;/usr/lib/python2.7/lib-tk&#39;, &#39;/usr/lib/python2.7/lib-old&#39;, &#39;/usr/lib/python2.7/lib-dynload&#39;, &#39;/home/explore/.local/lib/python2.7/site-packages&#39;, &#39;/usr/local/lib/python2.7/dist-packages&#39;, &#39;/usr/local/lib/python2.7/dist-packages/PyCapture2-0.0.0-py2.7-linux-x86_64.egg&#39;, &#39;/usr/lib/python2.7/dist-packages&#39;] /home/explore/miniconda3/envs/pytorch/bin/python [&#39;&#39;, &#39;/home/explore/miniconda3/envs/pytorch/lib/python38.zip&#39;, &#39;/home/explore/miniconda3/envs/pytorch/lib/python3.8&#39;, &#39;/home/explore/miniconda3/envs/pytorch/lib/python3.8/lib-dynload&#39;, &#39;/home/explore/miniconda3/envs/pytorch/lib/python3.8/site-packages&#39;] . pip install will install in the Python in the same path: . !type pip . pip is /home/explore/miniconda3/envs/pytorch/bin/pip . conda install will install in the active conda envt . !conda env list . # conda environments: # base /home/explore/miniconda3 d059 /home/explore/miniconda3/envs/d059 datacamp /home/explore/miniconda3/envs/datacamp deeplearning_specialization /home/explore/miniconda3/envs/deeplearning_specialization deeplearning_specialization_keras /home/explore/miniconda3/envs/deeplearning_specialization_keras deeplearning_specialization_tf1 /home/explore/miniconda3/envs/deeplearning_specialization_tf1 drl_handson /home/explore/miniconda3/envs/drl_handson fastai /home/explore/miniconda3/envs/fastai gan /home/explore/miniconda3/envs/gan gan_tensorflow /home/explore/miniconda3/envs/gan_tensorflow mit_6002x /home/explore/miniconda3/envs/mit_6002x pytorch * /home/explore/miniconda3/envs/pytorch squeezebox /home/explore/miniconda3/envs/squeezebox . The reason both pip and conda default to the conda pytorch environment is that this is the Python environment I used to launch the notebook. . How Jupyter executes code: Jupyter Kernels . !jupyter kernelspec list . Available kernels: python2 /home/explore/.local/share/jupyter/kernels/python2 python3 /home/explore/miniconda3/envs/pytorch/share/jupyter/kernels/python3 . !cat /home/explore/miniconda3/envs/pytorch/share/jupyter/kernels/python3/kernel.json . { &#34;argv&#34;: [ &#34;/home/explore/miniconda3/envs/pytorch/bin/python&#34;, &#34;-m&#34;, &#34;ipykernel_launcher&#34;, &#34;-f&#34;, &#34;{connection_file}&#34; ], &#34;display_name&#34;: &#34;Python 3&#34;, &#34;language&#34;: &#34;python&#34; } . If you&#39;d like to create a new kernel, you can do so using the jupyter ipykernel command; for example, I created the above kernels for my primary conda environments using the following as a template: . $ source activate myenv $ python -m ipykernel install --user --name myenv --display-name &quot;Python (myenv)&quot; . The Root of the Issue . The root of the issue is this: the shell environment is determined when the Jupyter notebook is launched, while the Python executable is determined by the kernel, and the two do not necessarily match. In other words, there is no guarantee that the python, pip, and conda in your $PATH will be compatible with the python executable used by the notebook. . Recall that the python in your path can be determined using . !type python . python is /home/explore/miniconda3/envs/pytorch/bin/python . The Python executable being used in the notebook can be determined using . sys.executable . &#39;/home/explore/miniconda3/envs/pytorch/bin/python&#39; . Notewhen the 2 differs, boom! .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/installing-python-packages-from-jupyter.html",
            "relUrl": "/blog/installing-python-packages-from-jupyter.html",
            "date": " • Jan 13, 2021"
        }
        
    
  
    
        ,"post51": {
            "title": "Hello nbdev",
            "content": "Resources . Everything is under nbdev website. . 3 resources worth to be mentioning: . nbdev tutorial video on youtube; 1 year old but seems still valid | nbdev tutorial page | nbdev github repo | . What I plan to do is to watch the video part, and keep note of my progress in this blog entry. . Walkthrough tutorial . repo creation . As suggested by Jeremy, I start by creating a github repo named hello_nbdev from a nbdev template. . It is just about clicking this link: https://github.com/fastai/nbdev_template/generate. If I am logged in github it will show the proper page. . github pages . Documentation will be hosted at github (can be hosted anywhere but github seems a straightforward option) and to do that we have to setup github pages: . Settings &gt; Options &gt; Github pages &gt; Source &gt; Master (branch) &gt; /docs (folder) &gt; Save . And when done . Now we can insert this doc url as our repo website setting: . repo home &gt; &lt;&gt; code &gt; about (edit repo details) &gt; Website . Edit settings.ini . Everything is in this file. . Just edit directly from github. . lib_name = nbdev_template # For Enterprise Git add variable repo_name and company name # repo_name = analytics # company_name = nike user = fastai # description = A description of your project # keywords = some keywords # author = Your Name # author_email = email@example.com # copyright = Your Name or Company Name . to . lib_name = hello_nbdev user = castorfou description = A tutorial walkthrough with nbdev keywords = fastai nbdev tutorial author = Guillaume Ramelet author_email = guillaume.ramelet@gmail.com copyright = Guillaume R. . and commit changes . Clone repo . ~/git/guillaume$ git clone git@github.com:castorfou/hello_nbdev.git Cloning into &#39;hello_nbdev&#39;... remote: Enumerating objects: 106, done. remote: Counting objects: 100% (106/106), done. remote: Compressing objects: 100% (94/94), done. remote: Total 106 (delta 7), reused 81 (delta 4), pack-reused 0 Receiving objects: 100% (106/106), 1.02 MiB | 2.45 MiB/s, done. Resolving deltas: 100% (7/7), done. . Setup nbdev python environment . It is not specifically mentionned in the video. For this walkthrough I will use my existing fastai environment. . ~/git/guillaume$ conda activate fastai ~/git/guillaume$ nbdev_ nbdev_build_docs nbdev_diff_nbs nbdev_test_nbs nbdev_build_lib nbdev_fix_merge nbdev_trust_nbs nbdev_bump_version nbdev_install_git_hooks nbdev_update_lib nbdev_clean_nbs nbdev_nb2md nbdev_upgrade nbdev_conda_package nbdev_new nbdev_detach nbdev_read_nbs . As expected nbdev is already integrated in it. . Otherwise my guess is that I have to run conda install -c fastai nbdev under my python env. . Install git hooks . (fastai) ~/git/guillaume/hello_nbdev$ nbdev_install_git_hooks Executing: git config --local include.path ../.gitconfig Success: hooks are installed and repo&#39;s .gitconfig is now trusted . deal with conflicts . If needed in case of conflict, Jeremy explains one can call nbdev_fix_merge filename.ipynb and it will use the standard conflict marker to help you identify and fix the conflict. . Open 00_core.ipynb . create lib (we start with a core module) . Just following Jeremy&#39;s instructions. . Create say_hello function | Use it (example) | Test it (assert) | . build_lib . We can call nbdev_build_lib from anywhere in the repo. . (fastai) ~/git/guillaume/hello_nbdev$ nbdev_build_lib Converted 00_core.ipynb. Converted index.ipynb. . and it creates files, under hello_nbdev . hello_nbdev$ ls hello_nbdev/ core.py __init__.py _nbdev.py __pycache__ . Module Documentation . There are 2 levels of documentation. Documentation for your library that will be in index.ipynb and documentation for your modules that will be directly created from your code/notebooks 00_core.ipynb, etc . And to generate this documentation it will be just a matter of calling nbdev_build_docs. . Library documentation into index.ipynb . create doc . Documentation (what will be puclished) is in index.ipynb. . This is an actual documentation. Documentation won&#39;t be written in markdown. It will be executed as code and rendered as such. How great is that. . To make it happen we have to import our lib just freshly generated. . And now we can use all the part of our lib to explain how it works and why it is great. . say_hello(&quot;Guillaume&quot;) . &#39;Hello Guillaume!&#39; . build_docs . We have to call nbdev_build_docs from our repo root. . (fastai) ~/git/guillaume/hello_nbdev$ nbdev_build_docs converting: /home/explore/git/guillaume/hello_nbdev/00_core.ipynb converting /home/explore/git/guillaume/hello_nbdev/index.ipynb to README.md . commit to publish docs . Here is the list of files to be pushed: . git status Changes to be committed: modified: 00_core.ipynb new file: 00_core.py new file: Makefile modified: README.md new file: docs/_config.yml modified: docs/_data/sidebars/home_sidebar.yml new file: docs/_data/topnav.yml new file: docs/core.html new file: docs/index.html modified: docs/sidebar.json new file: hello_nbdev/__init__.py new file: hello_nbdev/_nbdev.py new file: hello_nbdev/core.py modified: index.ipynb new file: index.py . init.py . Just add from .core import * to __init__.py . !cat /home/explore/git/guillaume/hello_nbdev/hello_nbdev/__init__.py . __version__ = &#34;0.0.1&#34; from .core import * . So that we can easily use hello_nbdev without mentioning core module . commit and push . (fastai) ~/git/guillaume/hello_nbdev$ git commit -m &#39;initial commit&#39; [master 3484db7] initial commit 15 files changed, 520 insertions(+), 31 deletions(-) create mode 100644 00_core.py create mode 100644 Makefile create mode 100644 docs/_config.yml create mode 100644 docs/_data/topnav.yml create mode 100644 docs/core.html create mode 100644 docs/index.html create mode 100644 hello_nbdev/__init__.py create mode 100644 hello_nbdev/_nbdev.py create mode 100644 hello_nbdev/core.py create mode 100644 index.py (fastai) ~/git/guillaume/hello_nbdev$ git push Enumerating objects: 30, done. Counting objects: 100% (30/30), done. Delta compression using up to 12 threads Compressing objects: 100% (19/19), done. Writing objects: 100% (21/21), 4.87 KiB | 2.44 MiB/s, done. Total 21 (delta 7), reused 0 (delta 0) remote: Resolving deltas: 100% (7/7), completed with 5 local objects. remote: remote: GitHub found 1 vulnerability on castorfou/hello_nbdev&#39;s default branch (1 low). To find out more, visit: remote: https://github.com/castorfou/hello_nbdev/security/dependabot/docs/Gemfile.lock/nokogiri/open remote: To github.com:castorfou/hello_nbdev.git 3aec9f4..3484db7 master -&gt; master . And documentation is ready . https://castorfou.github.io/hello_nbdev/ . . Going further . Now that we have a 1st simple example up and running, we can go further with: . classes | autoreload tip | launch nbdev_build_lib from jupyter | run tests in parallel | . Classes . Following tutorial, we can create class HelloSayer and document our methods by calling show_doc(HelloSayer.say). . We can decide to add entries into index.ipynb if this is something worth having at the library level. . autoreload . By adding these lines . %load_ext autoreload %autoreload 2 . your notebook automatically reads in the new modules as soon as the python file changes . launch nbdev scripts directly from jupyter . Make it your last cell . from nbdev.export import notebook2script; notebook2script() . Converted 00_core.ipynb. Converted index.ipynb. . run tests in parallel . Just run nbdev_test_nbs . If your notebook starts with _, it will be excluded from the test list. . Jekyll to view documentation locally . Jekyll is the web server to properly render documentation. This is what is used at github pages. . Installation and setup . From https://jekyllrb.com/docs/installation/ubuntu/, . sudo apt-get install ruby-full build-essential zlib1g-dev . Install variables to use gem: . echo &#39;# Install Ruby Gems to ~/gems&#39; &gt;&gt; ~/.bashrc echo &#39;export GEM_HOME=&quot;$HOME/gems&quot;&#39; &gt;&gt; ~/.bashrc echo &#39;export PATH=&quot;$HOME/gems/bin:$PATH&quot;&#39; &gt;&gt; ~/.bashrc source ~/.bashrc . Install Jekyll and Builder: . gem install jekyll bundler . Setup our lib to use Jekyll . From our docs folder, launch bundle install . (fastai) ~/git/guillaume/hello_nbdev/docs$ bundle install Fetching gem metadata from https://rubygems.org/......... Using concurrent-ruby 1.1.7 .... Bundle complete! 4 Gemfile dependencies, 90 gems now installed. Use `bundle info [gemname]` to see where a bundled gem is installed. . Use it . From repo root, launch make docs_serve . (fastai) ~/git/guillaume/hello_nbdev$ make docs_serve cd docs &amp;&amp; bundle exec jekyll serve Configuration file: /home/explore/git/guillaume/hello_nbdev/docs/_config.yml Source: /home/explore/git/guillaume/hello_nbdev/docs Destination: /home/explore/git/guillaume/hello_nbdev/docs/_site Incremental build: disabled. Enable with --incremental Generating... GitHub Metadata: No GitHub API authentication could be found. Some fields may be missing or have incorrect data. done in 0.098 seconds. /home/explore/gems/gems/pathutil-0.16.2/lib/pathutil.rb:502: warning: Using the last argument as keyword parameters is deprecated Auto-regeneration: enabled for &#39;/home/explore/git/guillaume/hello_nbdev/docs&#39; Server address: http://127.0.0.1:4000/hello_nbdev// Server running... press ctrl-c to stop. . It is available locally at http://127.0.0.1:4000/hello_nbdev/ . . Skipped . I have not gone through pypi publication and console_scripts. . I don&#39;t have the need for the moment, if I need that I will add an entry here. .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/nbdev_tutorial.html",
            "relUrl": "/blog/nbdev_tutorial.html",
            "date": " • Jan 12, 2021"
        }
        
    
  
    
        ,"post52": {
            "title": "About my datacamp learning process",
            "content": "Datacamp . I started learning with Datacamp in March 2019. This is a great resource and I recommend all datascience newcomers to give it a shot. . What I like are the consistent courses content. There is an overall logic between all courses. And content is just incredible: more than 300 interactive courses. OK maybe you won&#39;t find all of them super useful but at least you can pick what is of interest for you. Following my learning process it takes me about 8 hours to complete a course. . . Career tracks are a smart way to help you build a 1st tour in your datascience journey. I followed python programmer (old version), data scientist with python (old version) and machine learning scientist with python tracks. Mileage may vary but it is about 20 courses per track. Updated versions of tracks are now online and this is a mix between courses, projects and skills assessments. I have tested one project but it is a little bit too basic for me. . . There is a nice and smooth progress tracking system, and as in a game you earn XP for each achivement. . . Selecting courses . A natural way to select courses is to browse through courses from career tracks. And I will complete courses from new version of career tracks. Or when I need to learn on a new domain, I just search for relevant courses (search engine is very good). . I have 2 ways to track these courses: . bookmarks in Datacamp | . entries in ITP (individual training plan, a big excel list of learning items I plan to follow) | . Learning process . Starting a project . As an example I will use . . which is a project from the new Data Scientist career track and which is in my ITP: . . Git repo - data-scientist-skills . In my data-scientist-skills github repo, I have 2 folders: . Other datacamp courses - where I keep lectures (pdf slides) from datacamp courses | python-sandbox - where I keep notebooks and data from datacamp exercises | . creation of Data Manipulation with pandas folder under Other datacamp courses | creation of data-manipulation-with-pandas folder under python-sandbox | copy of python-sandbox/_1project-template/ into python-sandbox/data-manipulation-with-pandas | . Datacamp project template . In this project template, . data_from_datacamp will store all data needed to launch datacamp exercises | exports_py will contain exports of notebooks in txt/py format (usefull to search on code patterns) | start_env.sh start_env.bat to launch jupyter notebook from the right conda env | downloadfromFileIO.py to download data files from my local notebooks (using in the background file.io) | uploadfromdatacamp.py to upload data files from datacamp | uploadfromdatacamp_examples.py some examples to transfer dataframes, dataseries, lists, ... | . Projects structure . After initialisation, I have the following structure and content: . . On your left lectures (one per chapter) and final certificate. . On your right notebooks. . Notebooks for exercises . Just run the jupyter notebook environment by calling start_env.sh. . Get the chapter title: . . And name the notebook accordingly: . . Then enter interactive instructions. I copy paste instructions using copy selection as markdown firefox add-on. . . Here in this example, if I want to follow instructions locally I need to have homelessness dataframe. . I can use the following code from uploadfromdatacamp_examples.py . ##### Dataframe ################### #upload and download from downloadfromFileIO import saveFromFileIO &quot;&quot;&quot; à executer sur datacamp: (apres copie du code uploadfromdatacamp.py) uploadToFileIO(homelessness) &quot;&quot;&quot; tobedownloaded=&quot;&quot;&quot; {pandas.core.frame.DataFrame: {&#39;homelessness.csv&#39;: &#39;https://file.io/vTM1t2ehXds4&#39;}} &quot;&quot;&quot; prefixToc=&#39;1.1&#39; prefix = saveFromFileIO(tobedownloaded, prefixToc=prefixToc, proxy=&quot;&quot;) #initialisation import pandas as pd homelessness = pd.read_csv(prefix+&#39;homelessness.csv&#39;,index_col=0) . Before executing this cell, I have to copy/paste/execute uploadfromdatacamp.py content on datacamp server. And call . uploadToFileIO(homelessness) . Then get the results last line . In [2]: uploadToFileIO(homelessness) {&quot;success&quot;:true,&quot;key&quot;:&quot;vTM1t2ehXds4&quot;,&quot;link&quot;:&quot;https://file.io/vTM1t2ehXds4&quot;,&quot;expiry&quot;:&quot;14 days&quot;} {pandas.core.frame.DataFrame: {&#39;homelessness.csv&#39;: &#39;https://file.io/vTM1t2ehXds4&#39;}} . and copy it in tobedownloaded variable. . Update prefixTOC to the good value (exercise 1.1 is the 1st one in first chapter) which is used as a prefix in data files. And update local variable name and csv file. . Run the cell . Here is the result . Téléchargements à lancer {&#39;pandas.core.frame.DataFrame&#39;: {&#39;homelessness.csv&#39;: &#39;https://file.io/vTM1t2ehXds4&#39;}} % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 2528 0 2528 0 0 4870 0 --:--:-- --:--:-- --:--:-- 4870 . And homelessness is available to be used. . Files downloaded are in data_from_datacamp folder. . . And running again the cell won&#39;t download file from file.io, but will read the cached file. (delete file to force download) . Full content of this notebook example at the bottom . keep content in git . ~/git/guillaume/data-scientist-skills$ git add . ~/git/guillaume/data-scientist-skills$ git commit -m &#39;start of data manipulation in pandas course&#39; [master c8696ce] start of data manipulation in pandas course 45 files changed, 9010 insertions(+) create mode 100644 Other datacamp courses/Data Manipulation with pandas/chapter1.pdf create mode 100644 python-sandbox/data-manipulation-with-pandas/.ipynb_checkpoints/chapter1 - Transforming Data-checkpoint.ipynb create mode 100644 python-sandbox/data-manipulation-with-pandas/__pycache__/downloadfromFileIO.cpython-37.pyc create mode 100644 python-sandbox/data-manipulation-with-pandas/chapter1 - Transforming Data.ipynb create mode 100644 python-sandbox/data-manipulation-with-pandas/data_from_datacamp/.empty_dir.txt create mode 100644 python-sandbox/data-manipulation-with-pandas/data_from_datacamp/chapter1 - Transforming Data-Exercise1.1_3277903540843719836.lock create mode 100644 python-sandbox/data-manipulation-with-pandas/data_from_datacamp/chapter1 - Transforming Data-Exercise1.1_homelessness.csv create mode 100644 python-sandbox/data-manipulation-with-pandas/downloadfromFileIO.py create mode 100644 python-sandbox/data-manipulation-with-pandas/exports_py/.empty_dir.txt create mode 100644 python-sandbox/data-manipulation-with-pandas/exports_py/Untitled.py create mode 100644 python-sandbox/data-manipulation-with-pandas/exports_py/Untitled.txt create mode 100644 python-sandbox/data-manipulation-with-pandas/exports_py/chapter1 - Transforming Data.py create mode 100644 python-sandbox/data-manipulation-with-pandas/start_env.bat create mode 100755 python-sandbox/data-manipulation-with-pandas/start_env.sh create mode 100644 python-sandbox/data-manipulation-with-pandas/uploadfromdatacamp.py create mode 100644 python-sandbox/data-manipulation-with-pandas/uploadfromdatacamp_examples.py ~/git/guillaume/data-scientist-skills$ git push Enumerating objects: 43, done. Counting objects: 100% (43/43), done. Delta compression using up to 12 threads Compressing objects: 100% (38/38), done. Writing objects: 100% (40/40), 5.75 MiB | 3.85 MiB/s, done. Total 40 (delta 8), reused 1 (delta 0) remote: Resolving deltas: 100% (8/8), completed with 3 local objects. To github.com:castorfou/data-scientist-skills.git 89f60e5..c8696ce master -&gt; master . Update progress in ITP . Datacamp is giving instant progress . . So I regularly report this progress (here 0.18/4=5%) in ITP. . keep certificates . I download and keep certificates with lectures. . . Notebook example : Introducing DataFrames . Inspecting a DataFrame | Python . Inspecting a DataFrame . When you get a new DataFrame to work with, the first thing you need to do is explore it and see what it contains. There are several useful methods and attributes for this. . .head() returns the first few rows (the “head” of the DataFrame). | .info() shows information on each of the columns, such as the data type and number of missing values. | .shape returns the number of rows and columns of the DataFrame. | .describe() calculates a few summary statistics for each column. | . homelessness is a DataFrame containing estimates of homelessness in each U.S. state in 2018. The individual column is the number of homeless individuals not part of a family with children. The family_members column is the number of homeless individuals part of a family with children. The state_pop column is the state&#39;s total population. . pandas is imported for you. . init . ##### Dataframe ################### #upload and download from downloadfromFileIO import saveFromFileIO &quot;&quot;&quot; à executer sur datacamp: (apres copie du code uploadfromdatacamp.py) uploadToFileIO(homelessness) &quot;&quot;&quot; tobedownloaded=&quot;&quot;&quot; {pandas.core.frame.DataFrame: {&#39;homelessness.csv&#39;: &#39;https://file.io/vTM1t2ehXds4&#39;}} &quot;&quot;&quot; prefixToc=&#39;1.1&#39; prefix = saveFromFileIO(tobedownloaded, prefixToc=prefixToc, proxy=&quot;&quot;) #initialisation import pandas as pd homelessness = pd.read_csv(prefix+&#39;homelessness.csv&#39;,index_col=0) . Téléchargements à lancer {&#39;pandas.core.frame.DataFrame&#39;: {&#39;homelessness.csv&#39;: &#39;https://file.io/vTM1t2ehXds4&#39;}} % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 2528 0 2528 0 0 4870 0 --:--:-- --:--:-- --:--:-- 4870 . code . Print the head of the homelessness DataFrame. . print(homelessness.head()) . region state individuals family_members state_pop 0 East South Central Alabama 2570.0 864.0 4887681 1 Pacific Alaska 1434.0 582.0 735139 2 Mountain Arizona 7259.0 2606.0 7158024 3 West South Central Arkansas 2280.0 432.0 3009733 4 Pacific California 109008.0 20964.0 39461588 . Print information about the column types and missing values in homelessness. . print(homelessness.info()) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 51 entries, 0 to 50 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 region 51 non-null object 1 state 51 non-null object 2 individuals 51 non-null float64 3 family_members 51 non-null float64 4 state_pop 51 non-null int64 dtypes: float64(2), int64(1), object(2) memory usage: 2.4+ KB None . Print the number of rows and columns in homelessness. . print(homelessness.shape) . (51, 5) . Print some summary statistics that describe the homelessness DataFrame. . print(homelessness.describe()) . individuals family_members state_pop count 51.000000 51.000000 5.100000e+01 mean 7225.784314 3504.882353 6.405637e+06 std 15991.025083 7805.411811 7.327258e+06 min 434.000000 75.000000 5.776010e+05 25% 1446.500000 592.000000 1.777414e+06 50% 3082.000000 1482.000000 4.461153e+06 75% 6781.500000 3196.000000 7.340946e+06 max 109008.000000 52070.000000 3.946159e+07 . .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/Datacamp.html",
            "relUrl": "/blog/Datacamp.html",
            "date": " • Jan 7, 2021"
        }
        
    
  
    
        ,"post53": {
            "title": "Auto export python code from jupyter notebooks",
            "content": "This hack comes from https://github.com/jupyter/notebook/blob/master/docs/source/extending/savehooks.rst. . jupyter_notebook_config.py . Here is the code: . # Based off of https://github.com/jupyter/notebook/blob/master/docs/source/extending/savehooks.rst import io import os from notebook.utils import to_api_path _script_exporter = None _html_exporter = None def script_post_save(model, os_path, contents_manager, **kwargs): &quot;&quot;&quot;convert notebooks to Python script after save with nbconvert replaces `ipython notebook --script` &quot;&quot;&quot; from nbconvert.exporters.script import ScriptExporter from nbconvert.exporters.html import HTMLExporter if model[&#39;type&#39;] != &#39;notebook&#39;: return global _script_exporter if _script_exporter is None: _script_exporter = ScriptExporter(parent=contents_manager) log = contents_manager.log global _html_exporter if _html_exporter is None: _html_exporter = HTMLExporter(parent=contents_manager) log = contents_manager.log # save .py file base, ext = os.path.splitext(os_path) script, resources = _script_exporter.from_filename(os_path) # si le sous rep eports_py existe, on ecrit dedans, sinon on ecrit à la racine sous_rep=&#39;&#39; repertoire=os.path.dirname(base) if os.path.exists(repertoire+&#39;/exports_py&#39;): sous_rep=&#39;/exports_py&#39; basename = os.path.basename(base) script_fname = repertoire+ sous_rep+&#39;/&#39;+basename+resources.get(&#39;output_extension&#39;, &#39;.txt&#39;) log.info(&quot;base: {}, basename: {}, sous_rep: {}, repertoire: {}&quot;.format(base, basename, sous_rep, repertoire)) log.info(&quot;script_fname: {}&quot;.format(script_fname)) #script_fname = base + resources.get(&#39;output_extension&#39;, &#39;.txt&#39;) log.info(&quot;Saving script /%s&quot;, to_api_path(script_fname, contents_manager.root_dir)) with io.open(script_fname, &#39;w&#39;, encoding=&#39;utf-8&#39;) as f: f.write(script) &quot;&quot;&quot; # save html base, ext = os.path.splitext(os_path) script, resources = _html_exporter.from_filename(os_path) script_fname = base + resources.get(&#39;output_extension&#39;, &#39;.txt&#39;) log.info(&quot;Saving html /%s&quot;, to_api_path(script_fname, contents_manager.root_dir)) with io.open(script_fname, &#39;w&#39;, encoding=&#39;utf-8&#39;) as f: f.write(script) &quot;&quot;&quot; c.FileContentsManager.post_save_hook = script_post_save . In this version, if a subfolder exports_py exists, .py version will be exported in it. Oherwise it will be exported in the notebook folder. . Maybe in a later version it would be good to export only of this subfolder exists. (for example I don&#39;t need these py files when creating such a blog entry, even if my .gitignore won&#39;t publish .py files) . And to remove the creation of Untitled.txt files when notebooks are just being created (and not yet named). . deployment . Just save/merge this jupyter_notebook_config.py file (download) to your jupyter home directory. . According to Config file and command line options in jupyter documentation, it is located at ~/.jupyter . And in windows it is at C: Users &lt;yourID&gt; .jupyter . This will be valid for all your conda environments. . test . ~/.jupyter$ cp ~/git/guillaume/blog/files/jupyter_notebook_config.py . . Restart Jupyter notebook server and click save on any notebook: . .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/jupyter-export-notebook-as-py.html",
            "relUrl": "/blog/jupyter-export-notebook-as-py.html",
            "date": " • Jan 5, 2021"
        }
        
    
  
    
        ,"post54": {
            "title": "From cron to anacron",
            "content": "Current config . I run generate_plots.sh daily at 9:30 AM. However what happens if my PC is off at that time, will have to wait another uptime at 9:30 AM. . Solution is to move from cron to anacron. . From https://www.putorius.net/cron-vs-anacron.html: . . Anacron as user . Anacron is already setup on ubuntu. Actually cron.daily is managed by anacron therefore everything under /etc/cron.daily is run even if your system was off at the time by crontab. . But it is true for root, and has to be setup for users. . I will follow these recommandations: https://askubuntu.com/a/235090 . .anacron folders . Create a .anacron folder in your home directory and in it two subfolders, etc and spool . !mkdir -p ~/.anacron/{etc,spool} . anacrontab . Create a new file ~/.anacron/etc/anacrontab with the following content: . # ~/.anacron/etc/anacrontab: configuration file for anacron # See anacron(8) and anacrontab(5) for details. SHELL=/bin/bash PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/home/explore/miniconda3/bin:/home/explore/miniconda3/condabin:/home/explore/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin # period delay job-identifier command 1 10 squeezebox ~/git/guillaume/squeezebox/generate_plots.sh . start anacron . Add the following line to your crontab using crontab -e: . @hourly /usr/sbin/anacron -s -t $HOME/.anacron/etc/anacrontab -S $HOME/.anacron/spool . And remove squeezebox entry from crontab. . !crontab -l . # NVIDIA SDK Manager updater # NVIDIA SDK Manager updater 0 12 */7 * * /bin/bash /home/explore/.nvsdkm/.updater/updater.sh #30 9 * * * ~/git/guillaume/squeezebox/generate_plots.sh @hourly /usr/sbin/anacron -s -t $HOME/.anacron/etc/anacrontab -S $HOME/.anacron/spool .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/from-cron-to-anacron.html",
            "relUrl": "/blog/from-cron-to-anacron.html",
            "date": " • Dec 13, 2020"
        }
        
    
  
    
        ,"post55": {
            "title": "Repo with 2 remote-urls",
            "content": "from github . I have just created this empty repo: https://github.com/castorfou/data-scientist-skills . from existing local repo connected to gitlab . Add the new remote url to github, name it github . (base) guillaume@LL11LPC0PQARQ:~/git/data-scientist-skills$ git remote add github https://github.com/castorfou/data-scientist-skills.git . Or this is possible to use ssh protocol: git remote add origin git@github.com:castorfou/data-scientist-skills.git . Push repo to this new remote url: git push -u github . (base) guillaume@LL11LPC0PQARQ:~/git/data-scientist-skills$ git push -u github Username for &#39;https://github.com&#39;: castorfou Password for &#39;https://castorfou@github.com&#39;: Counting objects: 4736, done. Delta compression using up to 8 threads. Compressing objects: 100% (3171/3171), done. Writing objects: 100% (4736/4736), 630.97 MiB | 9.08 MiB/s, done. Total 4736 (delta 1549), reused 4538 (delta 1458) remote: Resolving deltas: 100% (1549/1549), done. To https://github.com/castorfou/data-scientist-skills.git * [new branch] master -&gt; master Branch &#39;master&#39; set up to track remote branch &#39;master&#39; from &#39;github&#39;. . for new local repo . Clone the new repo: git clone git@github.com:castorfou/data-scientist-skills.git . And I want to have same names for same remotes: git remote rename origin github . So now it is quite easy to update from different remote repo: . cat refresh_from_github.sh #!/bin/bash git fetch github git pull .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/share-github-repo-on-2-pc.html",
            "relUrl": "/blog/share-github-repo-on-2-pc.html",
            "date": " • Dec 8, 2020"
        }
        
    
  
    
        ,"post56": {
            "title": "Push large files to github: git-lfs",
            "content": "Where is the problem . I am currently following deep learning specialization from Andrew Ng on coursera. . In the course 4 about CNNs, there are some pre-trained yolo models that we use to do object detection. And these models come as large .h5 files. . Because I run all programming assignments locally and keep everything (lectures + codes) on my local repo, when I pushed to github I got this error: . (base) explore@explore-ThinkPad-P53:~/git/guillaume/deeplearning_specialization$ git push Enumerating objects: 247, done. Counting objects: 100% (247/247), done. Delta compression using up to 12 threads Compressing objects: 100% (239/239), done. Writing objects: 100% (242/242), 707.06 MiB | 4.74 MiB/s, done. Total 242 (delta 6), reused 0 (delta 0) remote: Resolving deltas: 100% (6/6), completed with 3 local objects. remote: warning: File notebooks/C4W3/nb_images/pred_video.mp4 is 85.44 MB; this is larger than GitHub&#39;s recommended maximum file size of 50.00 MB remote: warning: File notebooks/C4W3/nb_images/road_video.mp4 is 81.71 MB; this is larger than GitHub&#39;s recommended maximum file size of 50.00 MB remote: error: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com. remote: error: Trace: 2d1944991c30279b831124b51e4aac57a17a860f2ef789b4e32801fb65282244 remote: error: See http://git.io/iEPt8g for more information. remote: error: File notebooks/C4W2/ResNet50.h5 is 270.32 MB; this exceeds GitHub&#39;s file size limit of 100.00 MB remote: error: File notebooks/C4W3/model_data/yolo.h5 is 194.69 MB; this exceeds GitHub&#39;s file size limit of 100.00 MB To github.com:castorfou/deeplearning_specialization.git ! [remote rejected] master -&gt; master (pre-receive hook declined) . Solution: git-lfs . As explained in https://github.com/git-lfs/git-lfs/wiki/Tutorial, there is (always) a way to do it properly. . First it is a matter of installing git-lfs: . sudo apt-get install git-lfs . Then to setup git lfs . git lfs install . And then to &quot;migrate&quot; big files to lfs: . git lfs migrate import --include=&quot;*.mp4&quot; . git lfs migrate import --include=&quot;*.h5&quot; . And now to git push . (base) explore@explore-ThinkPad-P53:~/git/guillaume/deeplearning_specialization$ git push Uploading LFS objects: 100% (25/25), 954 MB | 37 MB/s, done. Enumerating objects: 311, done. Counting objects: 100% (311/311), done. Delta compression using up to 12 threads Compressing objects: 100% (273/273), done. Writing objects: 100% (276/276), 60.49 MiB | 5.59 MiB/s, done. Total 276 (delta 18), reused 0 (delta 0) remote: Resolving deltas: 100% (18/18), completed with 16 local objects. To github.com:castorfou/deeplearning_specialization.git d0d2dc2..004fa09 master -&gt; master .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/push-big-files-to-github.html",
            "relUrl": "/blog/push-big-files-to-github.html",
            "date": " • Dec 2, 2020"
        }
        
    
  
    
        ,"post57": {
            "title": "Open Jupyter Notebook with http launch instead of redirect file",
            "content": "Where is the problem? . Default configuration when launching jupyter notebook is to create a redirect file. . Here is the explanation from config file ~/.jupyter/jupyter_notebook_config.py. . ## Disable launching browser by redirect file # # For versions of notebook &gt; 5.7.2, a security feature measure was added that # prevented the authentication token used to launch the browser from being # visible. This feature makes it difficult for other users on a multi-user # system from running code in your Jupyter session as you. # # However, some environments (like Windows Subsystem for Linux (WSL) and # Chromebooks), launching a browser using a redirect file can lead the browser # failing to load. This is because of the difference in file structures/paths # between the runtime and the browser. # # Disabling this setting to False will disable this behavior, allowing the # browser to launch by using a URL and visible token (as before). #c.NotebookApp.use_redirect_file = True . And when launching jupyter notebook from WSL . (xgboost) guillaume@LL11LPC0PQARQ:~/git/d059-vld-ic$ jupyter notebook [I 13:09:57.346 NotebookApp] The port 8888 is already in use, trying another port. [I 13:09:57.370 NotebookApp] [jupyter_nbextensions_configurator] enabled 0.4.1 [I 13:09:57.371 NotebookApp] Serving notebooks from local directory: /mnt/d/git/d059-vld-ic [I 13:09:57.372 NotebookApp] The Jupyter Notebook is running at: [I 13:09:57.373 NotebookApp] http://localhost:8889/?token=c3f77aea548937f5f563e0306d982d4332d26b0ed623e662 [I 13:09:57.373 NotebookApp] or http://127.0.0.1:8889/?token=c3f77aea548937f5f563e0306d982d4332d26b0ed623e662 [I 13:09:57.374 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 13:10:00.384 NotebookApp] To access the notebook, open this file in a browser: file:///home/guillaume/.local/share/jupyter/runtime/nbserver-828-open.html Or copy and paste one of these URLs: http://localhost:8889/?token=c3f77aea548937f5f563e0306d982d4332d26b0ed623e662 or http://127.0.0.1:8889/?token=c3f77aea548937f5f563e0306d982d4332d26b0ed623e662 . . Solution . As given in https://stackoverflow.com/questions/57679894/how-to-change-jupyter-launch-from-file-to-url, . update jupyter config file to change #c.NotebookApp.use_redirect_file = True to c.NotebookApp.use_redirect_file = False . .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/open-jupyter-from-http-link.html",
            "relUrl": "/blog/open-jupyter-from-http-link.html",
            "date": " • Oct 21, 2020"
        }
        
    
  
    
        ,"post58": {
            "title": "Use git with github (ssh) behind corporate proxy",
            "content": "Configuration . . I use 2 kinds of repo. gitlab for internal/corporate projects, hosted inside my company. github for public/pet projects and as a blogging platform. 3 days a week I am inside company, 4 days a week outside. . Green lines are the natural path to collaborate. . When outside I don&#39;t have proxy configuration or firewall, and I can directly access github. I cannot access to gitlab but I don&#39;t want to address it now, this is why it is set as a black line. (if this is really needed I have a vpn access and this is as being inside) . When inside, I use internal proxy. I can directly access gitlab. But I want to access github in a transparent way. And yes from both Windows and Linux (WSL). This is the red line. . Setup the red line . Here is the https://stackoverflow.com/questions/21318535/how-to-setup-corkscrew-to-connect-to-github-through-draconian-proxy discussion. . 1st step is to install workscrew: sudo apt install corkscrew . Then I create 2 ssh config files: . (base) guillaume@LL11LPC0PQARQ:~/proxy_files$ cat ssh_config_noproxy Host github.com IdentityFile ~/.ssh/id_rsa_gmail Host gitlab.&lt;mycompany&gt;.com IdentityFile ~/.ssh/id_rsa (base) guillaume@LL11LPC0PQARQ:~/proxy_files$ cat ssh_config_proxy Host github.com ProxyCommand /usr/bin/corkscrew &lt;my_proxy_hostname&gt; &lt;my_proxy_port&gt; %h %p IdentityFile ~/.ssh/id_rsa_gmail Host gitlab.&lt;mycompany&gt;.com IdentityFile ~/.ssh/id_rsa . It is just a matter of linking appropriate files when I am in or out of corporate network. . As in my_ip.sh: . # Set Proxy function setproxy() { echo &quot;Calling setproxy&quot; export {http,https,ftp}_proxy=&quot;http://proxy_ip:80&quot; export {HTTP,HTTPS,FTP}_PROXY=&quot;http://proxy_ip:80&quot; #proxy for wget ln -sf ~/proxy_files/.wgetrc_proxy ~/.wgetrc #proxy for apt #sudo ln -sf ~/proxy_files/apt_proxy.conf /etc/apt/apt.conf.d/proxy.conf #proxy for conda ln -sf ~/proxy_files/.condarc_proxy ~/.condarc #proxy for git git config --global http.proxy http://proxy_ip:80 ln -sf ~/proxy_files/ssh_config_proxy ~/.ssh/config } # Unset Proxy function unsetproxy() { echo &quot;Calling unsetproxy&quot; unset {http,https,ftp}_proxy unset {HTTP,HTTPS,FTP}_PROXY #no proxy for wget ln -sf ~/proxy_files/.wgetrc_noproxy ~/.wgetrc #no proxy for apt #sudo rm -f /etc/apt/apt.conf.d/proxy.conf #no proxy for conda ln -sf ~/proxy_files/.condarc_noproxy ~/.condarc #no proxy for git git config --global --unset http.proxy ln -sf ~/proxy_files/ssh_config_noproxy ~/.ssh/config } . July-21: Update following change of entreprise network . Now that we are behing GlobalProxy protection, we get some transparent proxy (good!) with no ssh access (bad). . $ ssh -Tvv git@github.com OpenSSH_7.6p1 Ubuntu-4ubuntu0.3, OpenSSL 1.0.2n 7 Dec 2017 debug1: Reading configuration data /home/guillaume/.ssh/config debug1: /home/guillaume/.ssh/config line 1: Applying options for github.com debug1: Reading configuration data /etc/ssh/ssh_config debug1: /etc/ssh/ssh_config line 19: Applying options for * debug1: Executing proxy command: exec /bin/nc -X 5 -x 192.168.50.202:1080 github.com 22 debug1: permanently_drop_suid: 1000 debug1: identity file /home/guillaume/.ssh/id_rsa_gmail type 0 debug1: key_load_public: No such file or directory debug1: identity file /home/guillaume/.ssh/id_rsa_gmail-cert type -1 debug1: Local version string SSH-2.0-OpenSSH_7.6p1 Ubuntu-4ubuntu0.3 ssh_exchange_identification: Connection closed by remote host . First step is to get rid of socks5 configuration . Revert socks5 for git . Just by commenting ProxyCommand in ssh for github . $ cat .ssh/config Host github.com IdentityFile ~/.ssh/id_rsa_gmail #ProxyCommand /bin/nc -X 5 -x 192.168.50.202:1080 %h %p Host gitlab.michelin.com IdentityFile ~/.ssh/id_rsa . Second step is to migrate remote-url from ssh to https: . from git ssh to git https . $ git remote set-url origin https://githib.com/castorfou/mit_600.2x.git $ git remote -v origin https://github.com/castorfou/mit_600.2x.git (fetch) origin https://github.com/castorfou/mit_600.2x.git (push) . This allows to fetch and pull updates . git fetch git pull . Lastly to setup passwordless access to github . Github token to access passwordless using https . From https://clarusway.com/passwordless-usage-of-private-git-repositories/ . To Generate token in github: . (profile &gt; Settings &gt; Developer settings &gt; Personal access tokens) | Generate new token | Select repo section | . Integrate into git config: . copy token into .git/config remote url ( from url = https://github.com/castorfou/guillaume_blog.git to url = https://mytoken@github.com/castorfou/guillaume_blog.git) | .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/github-ssh-behind-proxy.html",
            "relUrl": "/blog/github-ssh-behind-proxy.html",
            "date": " • Oct 21, 2020"
        }
        
    
  
    
        ,"post59": {
            "title": "edX MIT 6.00.2x Introduction to Computational Thinking and Data Science",
            "content": "MIT courses at edX . Last summer I have been following a 1st MIT course on python programming. Not that I would need this knowledge but as for Polytechnique courses, I like their way to explain knowledge foundations. Teachers from these schools tend to go back to deep roots, and provide clear and somtimes illuminating examples to help us understand concepts. . About 6 years ago I have completed a Probability introduction from Ecole Polythechnique. That was great. I had always been hermeticly closed to probability and statistics. For a reason I don&#39;t understand, it is not being teached in CPGE (which is a two-or-three-year intensive full-time course preparing top high school graduates for the entrance examination of French engineering and business schools, this is just after high school). It means last time I was exposed to probability was in high school, and probably in engineering school as well but on a light way. . That would be great to give back a look to these courses. . MIT 6.00.1x - Introduction to Computer Science and Programming Using Python . Unfortunately I registered in September when only a couple of weeks were left to complete this 9-week course. And because I didn&#39;t upgrade to a Verified Certificate, I lost access to materials and progress. It cut when my progress was about 38%. . I like Eric Grimson&#39;s style. He is calm and has his own way to explain some advanced subjects. . Next session is planned on Jan 27, 2021. That would be a good idea to complete this course. . MIT 6.00.2x - Introduction to Computational Thinking and Data Science . For this one I have registered on time. And I have purchased the Verified Certificate. . Here is the full course program and dates: . . 1st lectures are interesting. As said before I like to be back to roots of problems. And on that matter I expect to get a full overview. . Lecture 4 should be released in the coming at the end of October. cannot wait to resume these sessions. . As a matter of comparaison with gan specialization from coursera+openAI, I like better the interactions with students offered by openAI. They use slack as a platform to support these interactions and I think it is a smart move. .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/MIT-edx-Introduction-Computational-Thinking-and-Data-Science.html",
            "relUrl": "/blog/MIT-edx-Introduction-Computational-Thinking-and-Data-Science.html",
            "date": " • Oct 20, 2020"
        }
        
    
  
    
        ,"post60": {
            "title": "GAN Specialization course 2 week 3 - Apply and certificate",
            "content": "Notes . tbd later . Certificate . . About next steps . For the moment I am hesitating to follow last course for gans. This is a deep dive into gan, with lots of theory (papers) associated to it. My 1st goal was to know better about it and this is met. My 2nd one was to figure out how to use this kind of generative networks for other area such as tabular data + prescription issues. I am not sure this is applicable (or not yet). . Maybe it is better for now to resume my learning sessions with Jeremy Howard on fastai v2. .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/gan-course2-week3-certificate.html",
            "relUrl": "/blog/gan-course2-week3-certificate.html",
            "date": " • Oct 15, 2020"
        }
        
    
  
    
        ,"post61": {
            "title": "GAN Specialization course 2 week 2 - Disadvantages and Bias",
            "content": "Notes . .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/gan-course2-week2-disadventages-bias.html",
            "relUrl": "/blog/gan-course2-week2-disadventages-bias.html",
            "date": " • Oct 9, 2020"
        }
        
    
  
    
        ,"post62": {
            "title": "GAN Specialization course 2 week 1 - evaluations on GANs",
            "content": "Features extraction, inception v3, embeddings, FID (Fr&#233;chet Inception Distance), Sampling and Truncation, Precision and Recall . .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/gan-course2-week1-evaluations-on-gans.html",
            "relUrl": "/blog/gan-course2-week1-evaluations-on-gans.html",
            "date": " • Oct 9, 2020"
        }
        
    
  
    
        ,"post63": {
            "title": "GAN Specialization course 1 week 4 - conditional generation, controllable generation",
            "content": "Conditional generation, controllable generation, disentanglement of Z-space . . End of course . Next one is a 3 week course named: Build Better Generative Adversarial Networks (GANs) . Here is my certificate . .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/gan-specialization-week4-conditional_generation-controllable-generation.html",
            "relUrl": "/blog/gan-specialization-week4-conditional_generation-controllable-generation.html",
            "date": " • Oct 7, 2020"
        }
        
    
  
    
        ,"post64": {
            "title": "GAN Specialization course 1 week 3 - mode collapse, vanishing gradient, wasserstein loss",
            "content": "Mode collapse, Vanishing gradient . Very good explanation about why it happens. Flat region when discriminator is learning faster (it has an easier job) than generator. . . Earth mover&#39;s distance. Wasserstein loss. 1-L continuous condition . .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/gan-specialization-week3-mode_collapse-W_loss.html",
            "relUrl": "/blog/gan-specialization-week3-mode_collapse-W_loss.html",
            "date": " • Oct 7, 2020"
        }
        
    
  
    
        ,"post65": {
            "title": "Variables traces using show_guts decorator",
            "content": "show_guts decorator . Adaptaton from https://stackoverflow.com/questions/24165374/printing-a-functions-local-variable-names-and-values . Update to python 3. . import sys import threading def show_guts(f): sentinel = object() gutsdata = threading.local() gutsdata.captured_locals = None gutsdata.tracing = False def trace_locals(frame, event, arg): if event.startswith(&#39;c_&#39;): # C code traces, no new hook return if event == &#39;call&#39;: # start tracing only the first call if gutsdata.tracing: return None gutsdata.tracing = True return trace_locals if event == &#39;line&#39;: # continue tracing return trace_locals # event is either exception or return, capture locals, end tracing gutsdata.captured_locals = frame.f_locals.copy() return None def wrapper(*args, **kw): # preserve existing tracer, start our trace old_trace = sys.gettrace() sys.settrace(trace_locals) retval = sentinel try: retval = f(*args, **kw) finally: # reinstate existing tracer, report, clean up sys.settrace(old_trace) for key, val in gutsdata.captured_locals.items(): print(&#39;{}: {!r}&#39;.format(key, val)) if retval is not sentinel: print(&#39;Returned: {!r}&#39;.format(retval)) gutsdata.captured_locals = None gutsdata.tracing = False return retval return wrapper . use example . import torch from torch import nn from tqdm.auto import tqdm from torchvision import transforms from torchvision.utils import make_grid from torchvision.datasets import CelebA from torch.utils.data import DataLoader import matplotlib.pyplot as plt . @show_guts def get_score(current_classifications, original_classifications, target_indices, other_indices, penalty_weight): &#39;&#39;&#39; Function to return the score of the current classifications, penalizing changes to other classes with an L2 norm. Parameters: current_classifications: the classifications associated with the current noise original_classifications: the classifications associated with the original noise target_indices: the index of the target class other_indices: the indices of the other classes penalty_weight: the amount that the penalty should be weighted in the overall score &#39;&#39;&#39; # Steps: 1) Calculate the change between the original and current classifications (as a tensor) # by indexing into the other_indices you&#39;re trying to preserve, like in x[:, features]. # 2) Calculate the norm (magnitude) of changes per example. # 3) Multiply the mean of the example norms by the penalty weight. # This will be your other_class_penalty. # Make sure to negate the value since it&#39;s a penalty! # 4) Take the mean of the current classifications for the target feature over all the examples. # This mean will be your target_score. #### START CODE HERE #### change_original_classification = (current_classifications[:,other_indices] - original_classifications[:,other_indices]) # Calculate the norm (magnitude) of changes per example and multiply by penalty weight other_class_penalty = - torch.mean(torch.norm(change_original_classification, dim=1) * penalty_weight) # Take the mean of the current classifications for the target feature target_score = torch.mean(current_classifications) #### END CODE HERE #### return target_score + other_class_penalty . rows = 10 current_class = torch.tensor([[1] * rows, [2] * rows, [3] * rows, [4] * rows]).T.float() original_class = torch.tensor([[1] * rows, [2] * rows, [3] * rows, [4] * rows]).T.float() # Must be 3 assert get_score(current_class, original_class, [1, 3] , [0, 2], 0.2).item() == 3 . current_classifications: tensor([[1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.]]) original_classifications: tensor([[1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.], [1., 2., 3., 4.]]) target_indices: [1, 3] other_indices: [0, 2] penalty_weight: 0.2 change_original_classification: tensor([[0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.]]) other_class_penalty: tensor(-0.) target_score: tensor(2.5000) Returned: tensor(2.5000) . AssertionError Traceback (most recent call last) &lt;ipython-input-5-c7e77f2e4ae1&gt; in &lt;module&gt; 4 5 # Must be 3 -&gt; 6 assert get_score(current_class, original_class, [1, 3] , [0, 2], 0.2).item() == 3 AssertionError: .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/decorator-trace-variables.html",
            "relUrl": "/blog/decorator-trace-variables.html",
            "date": " • Oct 7, 2020"
        }
        
    
  
    
        ,"post66": {
            "title": "Conda activate from bash scripts",
            "content": "Can&#39;t execute conda activate from bash script . Good description of the problem in conda github. . Calling conda activate from a bash script will raise some errors: . CommandNotFoundError: Your shell has not been properly configured to use &#39;conda activate&#39;. To initialize your shell, run $ conda init &lt;SHELL_NAME&gt; Currently supported shells are: - bash - fish - tcsh - xonsh - zsh - powershell See &#39;conda init --help&#39; for more information and options. . source ~/your_conda/etc/profile.d/conda.sh . It is just a matter of sourcing the conda bash settings before calling conda activate. . In m case I have installed conda in ~/miniconda3, I just have to call source ~/miniconda3/etc/profile.d/conda.sh . Example to run my blogging environment . #!/bin/bash source ~/miniconda3/etc/profile.d/conda.sh cd ~/git/guillaume/guillaume_blog/_notebooks conda activate fastai jupyter notebook .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/conda-activate-from-bash-script.html",
            "relUrl": "/blog/conda-activate-from-bash-script.html",
            "date": " • Oct 7, 2020"
        }
        
    
  
    
        ,"post67": {
            "title": "GAN Specialization course 1 week 2 - Deep Convolutional GAN",
            "content": "My notes . .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/gan-specialization-course1-week2-Deep_convolutional_GAN.html",
            "relUrl": "/blog/gan-specialization-course1-week2-Deep_convolutional_GAN.html",
            "date": " • Oct 6, 2020"
        }
        
    
  
    
        ,"post68": {
            "title": "GAN Specialization course 1 week 1 - intro to GAN",
            "content": "My notes . . .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/gan-specialization-course1-week1-intro_to_gan.html",
            "relUrl": "/blog/gan-specialization-course1-week1-intro_to_gan.html",
            "date": " • Oct 6, 2020"
        }
        
    
  
    
        ,"post69": {
            "title": "Generative Adversarial Networks (GANs) Specialization from Coursera",
            "content": "Coursera . This specialization comes in 3 courses. . . Build Basic Generative Adversarial Networks . Build Better Generative Adversarial Networks . Apply Generative Adversarial Networks . env installation . I am just getting the version from coursera to be sure I have the same behaviour. . import torch . print(torch.__version__) . 1.4.0 . So I can now create a gan environment with appropriate lib versions. . conda create -n gan python=3.7 conda activate gan conda install -c pytorch pytorch=1.4.0 conda install jupyter matplotlib conda install -c conda-forge tqdm conda install -c pytorch torchvision . git settings . echo &quot;# gan_specialization from coursera&quot; &gt;&gt; README.md git init git add README.md git commit -m &quot;first commit&quot; git branch -M master git remote add origin git@github.com:castorfou/gan_specialization.git git push -u origin master . I am pushing notebooks to github . Intro to PyTorch . I have exported Intro to Pytorch notebook from coursera lab. . To run it on my machine. . Intro to GAN using tensorflow . Versions of python seems incompatible between each other. (3.7 for pytorch, 3.6 for tensorflow=1.10) . I create a new python environment: . conda create -n gan_tensorflow python=3.6 conda activate gan_tensorflow conda install jupyter conda install -c conda-forge requests pip install tensorflow-gpu==1.15 .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/gan-pytorch-coursera.html",
            "relUrl": "/blog/gan-pytorch-coursera.html",
            "date": " • Oct 1, 2020"
        }
        
    
  
    
        ,"post70": {
            "title": "Use fingerprint to authenticate on Ubuntu, and passwordless on some apps",
            "content": "Fingerprint authentication . . Just by activating Fingerprint login, quite surprisingly it has been working directly. . Passwordless commands . Because I have changed my password for a quite complex one, I am interested to launch some sudo commands without prompt of password. . How to run sudo commands without password . Use visudo to update /etc/sudoers. I understand there is some syntax check to avoid mistake when editing this file. You don&#39;t want to be left with a defective sudo system. . I have just added this line. explore is my username. I can add additional commands after a comma (e.g. /bin/systemctl restart httpd.service, /bin/kill) . explore ALL = NOPASSWD: /usr/bin/apt .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/fingerprint-authentication-sudoers.html",
            "relUrl": "/blog/fingerprint-authentication-sudoers.html",
            "date": " • Oct 1, 2020"
        }
        
    
  
    
        ,"post71": {
            "title": "Upgrade ubuntu LTS 18.04 to 20.04",
            "content": "Standard upgrade process . As a LTS user, I want to keep using these long term support version. . !cat /etc/issue . Ubuntu 18.04.5 LTS n l . A good way to do it is by using do-release-upgrade tool. Full explanation at: 18.04 to 20.04. . sudo do-release-upgrade Checking for a new Ubuntu release There is no development version of an LTS available. To upgrade to the latest non-LTS develoment release set Prompt=normal in /etc/update-manager/release-upgrades. . Waiting for blockers to be fixed . There is a last blocker before releasing Ubuntu 20.04.1 LTS. . . Expected around 1st of October 2020. . (2020-09-28) blockers are fixed, upgrade in progress . Unfortunately the upgrade process went uneventful. Nothing broke, nothing to learn ;) . It took minutes to do the upgrade. . . Ubuntu releases-code names .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/upgrade-ubuntu-18.04-to-20.04.html",
            "relUrl": "/blog/upgrade-ubuntu-18.04-to-20.04.html",
            "date": " • Sep 28, 2020"
        }
        
    
  
    
        ,"post72": {
            "title": "Multiple subplots and animations with matplotlib",
            "content": "Subplots . What I want it to display multiple plots, with a given max rows. And to display my plots depending only on these parameters. . from fastai.tabular.all import * %matplotlib inline # fastai v1 backward compatibility import matplotlib.pyplot as plt import torch import torch.nn as nn import numpy as np . def my_hidden_f(x): return 4*x**3+2*x**2-12*x+5+10*torch.rand(x.shape) n=100 time = torch.ones(n,1) time[:,0].uniform_(-3.14,3.14) speed=my_hidden_f(time) plt.scatter(time[:,0], speed) plt.scatter(tensor(-1.5), my_hidden_f(tensor([-1.5])), color=&#39;red&#39;) def f(t, params): a,b,c,d = params return a*(t**3) + (b*t**2) + c*t + d def mse(preds, targets): return ((preds-targets)**2).mean() def show_preds(preds, ax=None): if ax is None: ax=plt.subplots()[1] ax.scatter(time, speed) ax.scatter(time, to_np(preds), color=&#39;red&#39;) ax.set_ylim(-50,150) lr = 1e-4 def apply_step(params, prn=True): preds = f(time, params) loss = mse(preds, speed) loss.backward() params.data -= lr * params.grad.data params.grad = None if prn: print(loss.item()) return preds . params = torch.randn(4).requires_grad_() #nbr of iterations max_iter = 1000 #nbr of curves visible nbr_graph = 4 #max number of curves on one row max_columns = 5 #nbr of rows max_rows = (nbr_graph-1) // max_columns + 1 #nbr of iter per plot graph_iteration = max_iter //(nbr_graph-1) _,axs = plt.subplots(nrows=max_rows,ncols=max_columns,figsize=(3*max_columns,3*max_rows)) i=-1 ax_index= ((i+1) // graph_iteration ) // (max_columns), ((i+1) // graph_iteration ) % (max_columns) if (max_rows ==1): ax_index= ((i+1) // graph_iteration ) % (max_columns) show_preds(apply_step(params, prn=False), axs[ax_index]) axs[ax_index].set_title(&#39;iter 0&#39;) for i in range(max_iter): preds=apply_step(params, prn=False) if ((i+1) % graph_iteration == 0): ax_index= ((i+1) // graph_iteration ) // (max_columns), ((i+1) // graph_iteration ) % (max_columns) if (max_rows ==1): ax_index= ((i+1) // graph_iteration ) % (max_columns) show_preds(preds, axs[ax_index]) axs[ax_index].set_title(&#39;iter &#39;+str(i+1)) plt.tight_layout() . Animation . import . %matplotlib inline # fastai v1 backward compatibility import matplotlib.pyplot as plt import torch import torch.nn as nn import numpy as np def tensor(*argv): return torch.tensor(argv) # TEST assert torch.all(tensor(1,2) == torch.tensor([1,2])), &#39;Backward compatibility with fastai v1&#39; . function and plot . n=100 x = torch.ones(n,1) x.uniform_(-3.14,3.14) def my_function(x, a): return ((torch.cat((x**3, x**2, x, torch.ones(n,1) ), 1))@a).reshape((n)) a=tensor(4., 2., -12., 5.) y = my_function(x, a) a = tensor(-1.,-2., 6., -8) y_hat = my_function(x, a) plt.scatter(x[:,0], y) plt.scatter(x[:,0],y_hat); def mse(y_hat, y): return ((y_hat-y)**2).mean() . gradient descent . a = nn.Parameter(a); a def update(): y_hat = my_function(x, a) loss = mse(y, y_hat) if t % 10 == 0: print(loss) loss.backward() with torch.no_grad(): a.sub_(lr * a.grad) a.grad.zero_() lr = 1e-3 for t in range(100): update() . tensor(1967.0251, grad_fn=&lt;MeanBackward0&gt;) tensor(559.2718, grad_fn=&lt;MeanBackward0&gt;) tensor(365.7207, grad_fn=&lt;MeanBackward0&gt;) tensor(282.6393, grad_fn=&lt;MeanBackward0&gt;) tensor(245.4054, grad_fn=&lt;MeanBackward0&gt;) tensor(227.3450, grad_fn=&lt;MeanBackward0&gt;) tensor(217.3324, grad_fn=&lt;MeanBackward0&gt;) tensor(210.7267, grad_fn=&lt;MeanBackward0&gt;) tensor(205.5912, grad_fn=&lt;MeanBackward0&gt;) tensor(201.1171, grad_fn=&lt;MeanBackward0&gt;) . animation . from matplotlib import animation, rc rc(&#39;animation&#39;, html=&#39;jshtml&#39;) a = nn.Parameter(tensor(-1.,1)) a=tensor(4., 2., -12., 5.) y = my_function(x, a) a = tensor(-1.,-2., 6., -8) y_hat = my_function(x, a) a = nn.Parameter(a); a fig = plt.figure() plt.scatter(x[:,0], y, c=&#39;orange&#39;) line = plt.scatter(x[:,0], y_hat.detach()) plt.close() def animate(i): line.set_offsets(np.c_[x[:,0], (my_function(x,a)).detach()]) update() return line, animation.FuncAnimation(fig, animate, np.arange(0, 300), interval=5) . &lt;/input&gt; Once Loop Reflect",
            "url": "https://castorfou.github.io/guillaume_blog/blog/Matplotlib-multiple-subplots-and-animations.html",
            "relUrl": "/blog/Matplotlib-multiple-subplots-and-animations.html",
            "date": " • Sep 26, 2020"
        }
        
    
  
    
        ,"post73": {
            "title": "Fastai book Deep Learning for Coders with fastai and Pytorch",
            "content": "Paper version of fastai book . Of course the 1st tep is to purchase this great book: . . I have liked what Jeremy Howard said about why this is important to purchase it (in video 1). Fastai is offering full access to the book as notebooks. So that we can run all codes from them. . Get notebook version of fastai book . ~/git/guillaume$ git clone https://github.com/fastai/fastbook.git . . I have now a perfect combo between paper book and notebooks. . Video courses based on fastai book . Rachel Thomas and Jeremy Howard have done some great videos about learning fastai (and pytorch). . They tend to do it every year. But this year is quite special due to fastai book. . Here are all the 1st 7 videos: https://course.fast.ai/videos/?lesson=1 . Fastai forums . This is the natural source of information and interactions with other students. . There is a category Part 1 (2020) which seems perfect: https://forums.fast.ai/c/part1-v4/46 . Personal git organization . ~/git/guillaume$ ll fastai/ &lt;-- from https://github.com/fastai/fastai fastai_experiments/ &lt;-- I will keep all experiments I will do here fastbook/ &lt;-- from https://github.com/fastai/fastbook.git guillaume_blog/ &lt;-- from git@github.com:castorfou/guillaume_blog.git . fastai_experiments likely to have 1 notebook per chapter or video. . update jupyter to include extensions (toc, ...) . conda install -c conda-forge jupyter_contrib_nbextensions . I like table of content, others are quite usefull as well (scratchpad, ExecuteTime...). . Install some libraries to run book examples . conda install -c fastai fastbook . It will install graphviz, nbdev, and other libraries. . And most of them can be loaded by calling from utils import * . Launch jupyter notebook and start expermenting . cd ~/git/guillaume conda activate fastai jupyter notebook . And launch several tabs at: blog entries, fastai experiments, fastai courses and fastai videos. . And I keep track of progress with git. .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/fastai-book.html",
            "relUrl": "/blog/fastai-book.html",
            "date": " • Sep 24, 2020"
        }
        
    
  
    
        ,"post74": {
            "title": "Setup ubuntu box with fastai",
            "content": "Install miniconda . Get miniconda Linux installer. . Check sha256sum: sha256sum Miniconda3-latest-Linux-x86_64.sh . Run install: ./Miniconda3-latest-Linux-x86_64.sh -p $HOME/miniconda3 . Install fastai . conda create -n fastai python=3.8 conda activate fastai conda install -c fastai -c pytorch fastai . Install jupyter within fastai environment . conda activate fastai conda install jupyter . Test fastai installation (valid for v1) . With fastai v1, there was an easy way to check installation: . conda activate fastai python -m fastai.utils.show_install . get git repo to learn from fastai . From git folder, . git clone https://github.com/fastai/fastai . Test fastai v2 installation . From python environment: . from fastai.vision.all import * . From jupyter notebook . from fastai.vision.all import * . Install nvidia drivers for ubuntu . I tried by downloading a driver from nvidia website. But I was unable to install it (nvidia-drm-drv.c:662:44: error: &#39;DRIVER_PRIME&#39; undeclared here (not in a function); did you mean &#39;DRIVER_PCI_DMA&#39;?) . sudo ubuntu-drivers autoinstall . then rebooting fixed the issue. . Run courses from fastai github repo . just run fastai/dev_nbs/course/lesson1-pets.ipynb . And everything is just fined ;) . install nbdev . This is for rendering reasons: To get a prettier result with hyperlinks to source code and documentation, install nbdev: pip install nbdev . !pip install nbdev . Collecting nbdev Downloading nbdev-1.0.18-py3-none-any.whl (57 kB) |████████████████████████████████| 57 kB 1.5 MB/s eta 0:00:01 Requirement already satisfied: fastcore&gt;=1.0.5 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (1.0.13) Requirement already satisfied: packaging in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (20.4) Requirement already satisfied: jupyter-client in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (6.1.6) Requirement already satisfied: nbconvert&lt;6 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (5.6.1) Requirement already satisfied: nbformat&gt;=4.4.0 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (5.0.7) Collecting fastscript&gt;=1.0.0 Downloading fastscript-1.0.0-py3-none-any.whl (11 kB) Requirement already satisfied: pyyaml in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (5.3.1) Requirement already satisfied: pip in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (20.2.2) Requirement already satisfied: ipykernel in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (5.3.4) Requirement already satisfied: pyparsing&gt;=2.0.2 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from packaging-&gt;nbdev) (2.4.7) Requirement already satisfied: six in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from packaging-&gt;nbdev) (1.15.0) Requirement already satisfied: pyzmq&gt;=13 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jupyter-client-&gt;nbdev) (19.0.2) Requirement already satisfied: tornado&gt;=4.1 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jupyter-client-&gt;nbdev) (6.0.4) Requirement already satisfied: python-dateutil&gt;=2.1 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jupyter-client-&gt;nbdev) (2.8.1) Requirement already satisfied: traitlets in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jupyter-client-&gt;nbdev) (4.3.3) Requirement already satisfied: jupyter-core&gt;=4.6.0 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jupyter-client-&gt;nbdev) (4.6.3) Requirement already satisfied: pandocfilters&gt;=1.4.1 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert&lt;6-&gt;nbdev) (1.4.2) Requirement already satisfied: pygments in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert&lt;6-&gt;nbdev) (2.7.1) Requirement already satisfied: mistune&lt;2,&gt;=0.8.1 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert&lt;6-&gt;nbdev) (0.8.4) Requirement already satisfied: defusedxml in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert&lt;6-&gt;nbdev) (0.6.0) Requirement already satisfied: bleach in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert&lt;6-&gt;nbdev) (3.2.1) Requirement already satisfied: jinja2&gt;=2.4 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert&lt;6-&gt;nbdev) (2.11.2) Requirement already satisfied: testpath in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert&lt;6-&gt;nbdev) (0.4.4) Requirement already satisfied: entrypoints&gt;=0.2.2 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert&lt;6-&gt;nbdev) (0.3) Requirement already satisfied: jsonschema!=2.5.0,&gt;=2.4 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbformat&gt;=4.4.0-&gt;nbdev) (3.0.2) Requirement already satisfied: ipython-genutils in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbformat&gt;=4.4.0-&gt;nbdev) (0.2.0) Requirement already satisfied: ipython&gt;=5.0.0 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from ipykernel-&gt;nbdev) (7.18.1) Requirement already satisfied: decorator in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from traitlets-&gt;jupyter-client-&gt;nbdev) (4.4.2) Requirement already satisfied: webencodings in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from bleach-&gt;nbconvert&lt;6-&gt;nbdev) (0.5.1) Requirement already satisfied: MarkupSafe&gt;=0.23 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jinja2&gt;=2.4-&gt;nbconvert&lt;6-&gt;nbdev) (1.1.1) Requirement already satisfied: pyrsistent&gt;=0.14.0 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.4.0-&gt;nbdev) (0.17.3) Requirement already satisfied: attrs&gt;=17.4.0 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.4.0-&gt;nbdev) (20.2.0) Requirement already satisfied: setuptools in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.4.0-&gt;nbdev) (49.6.0.post20200814) Requirement already satisfied: backcall in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from ipython&gt;=5.0.0-&gt;ipykernel-&gt;nbdev) (0.2.0) Requirement already satisfied: pexpect&gt;4.3; sys_platform != &#34;win32&#34; in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from ipython&gt;=5.0.0-&gt;ipykernel-&gt;nbdev) (4.8.0) Requirement already satisfied: pickleshare in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from ipython&gt;=5.0.0-&gt;ipykernel-&gt;nbdev) (0.7.5) Requirement already satisfied: jedi&gt;=0.10 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from ipython&gt;=5.0.0-&gt;ipykernel-&gt;nbdev) (0.17.2) Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from ipython&gt;=5.0.0-&gt;ipykernel-&gt;nbdev) (3.0.7) Requirement already satisfied: ptyprocess&gt;=0.5 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from pexpect&gt;4.3; sys_platform != &#34;win32&#34;-&gt;ipython&gt;=5.0.0-&gt;ipykernel-&gt;nbdev) (0.6.0) Requirement already satisfied: parso&lt;0.8.0,&gt;=0.7.0 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jedi&gt;=0.10-&gt;ipython&gt;=5.0.0-&gt;ipykernel-&gt;nbdev) (0.7.0) Requirement already satisfied: wcwidth in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0-&gt;ipython&gt;=5.0.0-&gt;ipykernel-&gt;nbdev) (0.2.5) Installing collected packages: fastscript, nbdev Successfully installed fastscript-1.0.0 nbdev-1.0.18 .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/Setup-ubuntu-box-with-fastai.html",
            "relUrl": "/blog/Setup-ubuntu-box-with-fastai.html",
            "date": " • Sep 23, 2020"
        }
        
    
  
    
        ,"post75": {
            "title": "Fastai on WSL 2 with Cuda",
            "content": "This is based on what is explained in https://forums.fast.ai/t/fastai-on-wsl-2-ubuntu-0-7-0-or-any-version/76651 . install update of nvidia drivers . Based on Deep Learning Course Forums Platform: Windows 10 using WSL2 w/GPU fastai users . create nvidia account | download quadro driver from https://developer.nvidia.com/cuda/wsl/download (460.15) | install | . (xgboost) guillaume@LL11LPC0PQARQ:~/git/guillaume_blog$ nvidia-smi.exe Mon Sep 21 16:00:46 2020 +--+ | NVIDIA-SMI 460.15 Driver Version: 460.15 CUDA Version: 11.1 | |-+-+-+ | GPU Name TCC/WDDM | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 Quadro M1000M WDDM | 00000000:01:00.0 On | N/A | | N/A 59C P0 N/A / N/A | 905MiB / 4096MiB | 0% Default | +-+-+-+ . install of WSL2 and convert existing images . Open a PowerShell window as an Administrator . Run wsl --set-default-version 2 . update KB . . --set-default-version 2 is not a valid option. KB4566116 should be installed . This can be downloaded from Catalog Microsoft Update . update kernel version . If you see this message after running the command: WSL 2 requires an update to its kernel component. For information please visit https://aka.ms/wsl2kernel. You still need to install the MSI Linux kernel update package. . Download from https://docs.microsoft.com/en-us/windows/wsl/install-win10#step-4download-the-linux-kernel-update-package . set default WSL to be version 2 . PS C: WINDOWS system32&gt; wsl --set-default-version 2 . convert existing images . PS C: WINDOWS system32&gt; wsl --list --verbose NAME STATE VERSION * Ubuntu-18.04 Running 1 PS C: WINDOWS system32&gt; wsl --set-version Ubuntu-18.04 2 La conversion est en cours. Cette opération peut prendre quelques minutes... Pour plus d’informations sur les différences de clés avec WSL 2, visitez https://aka.ms/wsl2 La conversion est terminée. . It took a while (~1 hour) for my unique ubuntu image. . And at the end it has worked. . PS C: WINDOWS system32&gt; wsl --list --verbose NAME STATE VERSION * Ubuntu-18.04 Stopped 2 . install of nvidia drivers under ubuntu . [Installation instructions)(https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=Ubuntu&amp;target_version=1804&amp;target_type=deblocal) . wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin sudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600 wget https://developer.download.nvidia.com/compute/cuda/11.0.3/local_installers/cuda-repo-ubuntu1804-11-0-local_11.0.3-450.51.06-1_amd64.deb sudo dpkg -i cuda-repo-ubuntu1804-11-0-local_11.0.3-450.51.06-1_amd64.deb sudo apt-key add /var/cuda-repo-ubuntu1804-11-0-local/7fa2af80.pub sudo apt-get update sudo apt-get -y install cuda . !cat /usr/local/cuda/version.txt . CUDA Version 11.0.228 . !/usr/local/cuda/samples/4_Finance/BlackScholes/BlackScholes . [/usr/local/cuda/samples/4_Finance/BlackScholes/BlackScholes] - Starting... CUDA error at ../../common/inc/helper_cuda.h:777 code=35(cudaErrorInsufficientDriver) &#34;cudaGetDeviceCount(&amp;device_count)&#34; . There is an error when launching CUDA samples. Googling that error maybe my video card is running on low driver version? . I have posted on nvidia (cuda+wsl) forum: https://forums.developer.nvidia.com/t/cuda-sample-throwing-error/142537/18 . (update 09-22: it is not possible to have cuda on wsl2 if not in Windows Insider build from Dev Channel. (20145 or higher)) . . because I am in version 1909 (18363.1049), it won&#39;t work for me. ;( . cuda for WSL . Here is a link that could be interesting: https://docs.nvidia.com/cuda/wsl-user-guide/index.html . According to this, I should not have installed cuda but cuda-toolkit. Do not choose the cuda, cuda-11-0, or cuda-drivers meta-packages under WSL 2 since these packages will result in an attempt to install the Linux NVIDIA driver under WSL 2. . Is it causing my issue? . apt-get install -y cuda-toolkit-11-0 . !/usr/local/cuda/bin/nvcc --version . nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2020 NVIDIA Corporation Built on Wed_Jul_22_19:09:09_PDT_2020 Cuda compilation tools, release 11.0, V11.0.221 Build cuda_11.0_bu.TC445_37.28845127_0 . install a new distro (ubuntu 20.04) . Because I cannot use windows store, I have to manually install https://docs.microsoft.com/fr-fr/windows/wsl/install-manual . Installation by just launching Ubuntu_2004.2020.424.0_x64.appx. . I have now 2 distros, . PS C: WINDOWS system32&gt; wsl --list -v NAME STATE VERSION * Ubuntu-18.04 Running 2 Ubuntu-20.04 Running 2 . WSL2 and network . There is a change of network architecture between WSL 1 and WSL 2. In WSL 2, a new network interface is available: . Carte Ethernet vEthernet (WSL) : Suffixe DNS propre à la connexion. . . : Adresse IPv6 de liaison locale. . . . .: Adresse IPv4. . . . . . . . . . . . . .: 192.168.81.193 Masque de sous-réseau. . . . . . . . . : 255.255.255.240 Passerelle par défaut. . . . . . . . . : . Revert image to WSL1 to get back network access . PS C: WINDOWS system32&gt; wsl --set-version Ubuntu-18.04 1 La conversion est en cours. Cette opération peut prendre quelques minutes... La conversion est terminée. . PS C: WINDOWS system32&gt; wsl --list -v NAME STATE VERSION * Ubuntu-18.04 Stopped 1 Ubuntu-20.04 Stopped 2 . access to linux files from windows . For running state distros: . Files are available at wsl$ . . For stopped state distros: . Files are available at C: Users &lt;users&gt; AppData Local Packages CanonicalGroupLimited.Ubuntu* LocalState rootfs . some usefull wsl commands . List distros . !wsl --list --verbose . NAME STATE VERSION * Ubuntu-18.04 Running 1 Ubuntu-20.04 Stopped 2 . Stop a distro . !wsl --terminate Ubuntu-18.04 . Update dns settings . as explained here . just switch from generateResolvConf = true to generateResolvConf = false in /etc/wsl.conf and edit /etc/resolv.conf . But still have issues, mainly I think linked to Symantec Endpoint protection. . Workaround network issue with WSL2 . https://github.com/sakai135/wsl-vpnkit . installation setup . vpnkit . install docker for windows . install genisoimage in ubuntu (from http://archive.ubuntu.com/ubuntu/pool/main/c/cdrkit/genisoimage_1.1.11-3.1ubuntu1_amd64.deb) . install vpnkit . isoinfo -i /mnt/c/Program Files/Docker/Docker/resources/wsl/docker-for-wsl.iso -R -x /containers/services/vpnkit-tap-vsockd/lower/sbin/vpnkit-tap-vsockd &gt; ./vpnkit-tap-vsockd chmod +x vpnkit-tap-vsockd sudo mv vpnkit-tap-vsockd /sbin/vpnkit-tap-vsockd sudo chown root:root /sbin/vpnkit-tap-vsockd . npiperelay . install unzip in ubuntu (from http://archive.ubuntu.com/ubuntu/pool/main/u/unzip/unzip_6.0-25ubuntu1_amd64.deb) . download npiprelay (from https://github.com/jstarks/npiperelay/releases/download/v0.1.0/npiperelay_windows_amd64.zip ) . install npiprelay . unzip npiperelay_windows_amd64.zip npiperelay.exe rm npiperelay_windows_amd64.zip mkdir -p /mnt/c/bin mv npiperelay.exe /mnt/c/bin/ sudo ln -s /mnt/c/bin/npiperelay.exe /usr/local/bin/npiperelay.exe . socat . install socat in ubuntu (from http://archive.ubuntu.com/ubuntu/pool/main/s/socat/socat_1.7.3.3-2_amd64.deb) . Configure DNS for WSL . Disable WSL from generating and overwriting /etc/resolv.conf. . sudo tee /etc/wsl.conf &lt;&lt;EOL [network] generateResolvConf = false EOL . Manually set DNS servers to use when not running this script. 1.1.1.1 is provided as an example. . sudo tee /etc/resolv.conf &lt;&lt;EOL nameserver 1.1.1.1 EOL . wsl-vpnkit . from https://github.com/sakai135/wsl-vpnkit/archive/refs/heads/main.zip . unzip ~/git/wsl-vpnkit-main.zip -d ~/Applications/wsl-vpnkit . execution . ~/Applications/wsl-vpnkit/wsl-vpnkit-main$ sudo ./wsl-vpnkit . proxychains . I can now use proxychains and everything works beautifully ;) . clean from local deb install to ubuntu repo . ~/git$ ls *.deb genisoimage_1.1.11-3.1ubuntu1_amd64.deb proxychains_3.1-8.1_all.deb libproxychains3_3.1-8.1_amd64.deb socat_1.7.3.3-2_amd64.deb net-tools_1.60+git20180626.aebd88e-1ubuntu1_amd64.deb unzip_6.0-25ubuntu1_amd64.deb . sudo apt remove genisoimage net-tools socat unzip sudo proxychains apt install genisoimage libproxychains3 net-tools proxychains socat unzip . I don&#39;t exactly see how to do it with proxychains. .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/Windows10-fastai-wsl2-cuda.html",
            "relUrl": "/blog/Windows10-fastai-wsl2-cuda.html",
            "date": " • Sep 21, 2020"
        }
        
    
  
    
        ,"post76": {
            "title": "Autodetect Home / Office network + Proxy",
            "content": "IP detection . Command to get IP address is as follow: . IP=`ifconfig | grep &#39;inet &#39;| grep -v &#39;127.0.0.1&#39; | cut -d: -f2 | awk &#39;{ print $2}&#39;` . I can then check how IP is setup: . empty: no network attached, in that case nothing to do | HOME_IP=192.168.1.241: based on MAC I give fixed IP to my computers (out of DHCP scope) | S8_IP=192.168.: hotspot from samsung is using 192.168. addresses | OFFICE_IP=10.: office network uses 10. addresses | . Detect if variable IP is set: . if [ -z &quot;$IP&quot; ]; then echo &quot;Not connected to any network&quot; fi . Network detection and proxy settings . Depending on my network, I have to set or unset proxy. . Here is the 1st version: . (xgboost) guillaume@LL11LPC0PQARQ:~$ cat my_ip.sh #!/bin/bash IP=`ifconfig | grep &#39;inet &#39;| grep -v &#39;127.0.0.1&#39; | cut -d: -f2 | awk &#39;{ print $2}&#39;` HOME_IP=192.168.1.241 OFFICE_IP=10. S8_IP=192.168. # Set Proxy function setproxy() { echo &quot;Calling setproxy&quot; export {http,https,ftp}_proxy=&quot;http://proxy_ip:80&quot; export {HTTP,HTTPS,FTP}_PROXY=&quot;http://proxy_ip:80&quot; } # Unset Proxy function unsetproxy() { echo &quot;Calling unsetproxy&quot; unset {http,https,ftp}_proxy unset {HTTP,HTTPS,FTP}_PROXY } if [ -z &quot;$IP&quot; ]; then echo &quot;Not connected to any network&quot; else echo &quot;Connected and IP address is: $IP&quot; if [[ &quot;$IP&quot; == &quot;$HOME_IP&quot; ]]; then echo &quot;Connected at home from freebox pop --&gt; no proxy&quot; unsetproxy else if [[ &quot;$IP&quot; == &quot;$S8_IP&quot;* ]]; then echo &quot;Connected with mobile phone --&gt; no proxy&quot; unsetproxy fi if [[ &quot;$IP&quot; == &quot;$OFFICE_IP&quot;* ]]; then echo &quot;Connected from Office --&gt; proxy&quot; setproxy fi fi fi . Call this script: source . If I want these environment variables to be available from parent shell, I have to call my script with source. . (xgboost) guillaume@LL11LPC0PQARQ:~$ source my_ip.sh Connected and IP address is: 10.xxx.xxx.xxx 192.168.1.241 Connected from Office --&gt; proxy Calling setproxy . And I will auto launch this script each time I open a terminal by adding source my_ip.sh at the end of .bashrc . git and keep dot configuration files: config . Another great practice from Jeremy Howard: From https://developer.atlassian.com/blog/2016/02/best-way-to-store-dotfiles-git-bare-repo/ and https://www.atlassian.com/git/tutorials/dotfiles . I will create a blog entry about that later. . config add .bashrc my_ip.sh config commit -m &#39;detect network and set proxy&#39; config push . wget: proxy / no proxy . I store proxy conf files under ~/proxy_files/ . For wget: 2 files . $ cat proxy_files/.wgetrc_noproxy use_proxy=no $ cat proxy_files/.wgetrc_proxy use_proxy=yes http_proxy=proxy_ip:80 https_proxy=proxy_ip:80 . And enabling proxy for wget: ln -sf ~/proxy_files/.wgetrc_proxy ~/.wgetrc . Disabling proxy for wget: ln -sf ~/proxy_files/.wgetrc_noproxy ~/.wgetrc . So the updated functions setproxy and unsetproxy are: . # Set Proxy function setproxy() { echo &quot;Calling setproxy&quot; export {http,https,ftp}_proxy=&quot;http://proxy_ip:80&quot; export {HTTP,HTTPS,FTP}_PROXY=&quot;http://proxy_ip:80&quot; #proxy for wget ln -sf ~/proxy_files/.wgetrc_proxy ~/.wgetrc } # Unset Proxy function unsetproxy() { echo &quot;Calling unsetproxy&quot; unset {http,https,ftp}_proxy unset {HTTP,HTTPS,FTP}_PROXY #no proxy for wget ln -sf ~/proxy_files/.wgetrc_noproxy ~/.wgetrc } . apt-get: proxy / no proxy . I store proxy conf files under ~/proxy_files/ . For apt, 1 file . $ cat proxy_files/apt_proxy.conf Acquire { HTTP::proxy &quot;http://proxy_ip:80&quot;; HTTPS::proxy &quot;http://proxy_ip:80&quot;; } . And enabling proxy for apt: sudo ln -sf ~/proxy_files/apt_proxy.conf /etc/apt/apt.conf.d/proxy.conf . Disabling proxy for wget: sudo rm -f /etc/apt/apt.conf.d/proxy.conf . . Refactor to avoid password request each time it is launched . for the moment I have just commented out these lines . So the updated functions setproxy and unsetproxy are: . # Set Proxy function setproxy() { echo &quot;Calling setproxy&quot; export {http,https,ftp}_proxy=&quot;http://proxy_ip:80&quot; export {HTTP,HTTPS,FTP}_PROXY=&quot;http://proxy_ip:80&quot; #proxy for wget ln -sf ~/proxy_files/.wgetrc_proxy ~/.wgetrc #proxy for apt #sudo ln -sf ~/proxy_files/apt_proxy.conf /etc/apt/apt.conf.d/proxy.conf } # Unset Proxy function unsetproxy() { echo &quot;Calling unsetproxy&quot; unset {http,https,ftp}_proxy unset {HTTP,HTTPS,FTP}_PROXY #no proxy for wget ln -sf ~/proxy_files/.wgetrc_noproxy ~/.wgetrc #no proxy for apt #sudo rm -f /etc/apt/apt.conf.d/proxy.conf } . Sept-21 2020: IP detection to be changed after WSL2 . With WSL2, IP address is from 172 network. . This looks like a virtual internal address. More detail at that address: https://github.com/microsoft/WSL/issues/4150. . . to update IP detection . Oct-21 2020: Use git with github (ssh) behind corporate proxy . Here is the new configuration explained in my blog entry Use git with github (ssh) behind corporate proxy . It is just a matter of linking appropriate files when I am in or out of corporate network. . As in my_ip.sh: . # Set Proxy function setproxy() { echo &quot;Calling setproxy&quot; export {http,https,ftp}_proxy=&quot;http://proxy_ip:80&quot; export {HTTP,HTTPS,FTP}_PROXY=&quot;http://proxy_ip:80&quot; #proxy for wget ln -sf ~/proxy_files/.wgetrc_proxy ~/.wgetrc #proxy for apt #sudo ln -sf ~/proxy_files/apt_proxy.conf /etc/apt/apt.conf.d/proxy.conf #proxy for conda ln -sf ~/proxy_files/.condarc_proxy ~/.condarc #proxy for git git config --global http.proxy http://proxy_ip:80 ln -sf ~/proxy_files/ssh_config_proxy ~/.ssh/config } # Unset Proxy function unsetproxy() { echo &quot;Calling unsetproxy&quot; unset {http,https,ftp}_proxy unset {HTTP,HTTPS,FTP}_PROXY #no proxy for wget ln -sf ~/proxy_files/.wgetrc_noproxy ~/.wgetrc #no proxy for apt #sudo rm -f /etc/apt/apt.conf.d/proxy.conf #no proxy for conda ln -sf ~/proxy_files/.condarc_noproxy ~/.condarc #no proxy for git git config --global --unset http.proxy ln -sf ~/proxy_files/ssh_config_noproxy ~/.ssh/config } .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/autodetect-home-office-network-and-proxy-settings.html",
            "relUrl": "/blog/autodetect-home-office-network-and-proxy-settings.html",
            "date": " • Sep 15, 2020"
        }
        
    
  
    
        ,"post77": {
            "title": "Fast read Excel files with pandas",
            "content": "Problem description . initial settings . measure_time decorator . from functools import wraps from time import time def measure_time(func): @wraps(func) def _time_it(*args, **kwargs): start = int(round(time() * 1000)) try: return func(*args, **kwargs) finally: end_ = int(round(time() * 1000)) - start print(f&quot;Total execution time: {end_ if end_ &gt; 0 else 0} ms&quot;) return _time_it . read big excel file with pandas . big_excel_file = root_data+&#39;/pandas-caching/big_excel_file.xlsx&#39; . @measure_time def load_excel(file): dataframe = pd.read_excel(file) return dataframe . dataframe = load_excel(big_excel_file) . Total execution time: 36196 ms . . Waouh, 36 sec to read this file! . read converted csv file (turned to csv from excel using excel) . csv_file = root_data+&#39;/pandas-caching/big_csv_file_turned_from_excel.csv&#39; . @measure_time def load_csv(file): dataframe = pd.read_csv(file, sep=&#39;;&#39;, decimal=&#39;,&#39;) return dataframe . df_csv = load_csv(csv_file) . Total execution time: 836 ms . . Much better, 0.8 sec! . Caching library . import os def read_CachedXLS(filename, forceReload = False, **options): &quot;&quot;&quot; Part d&#39;un fichier excel natif (filename). Si le dataframe caché correspondant n&#39;existe pas encore, alors sauve le dataframe caché au format csv dans le rep source. (s&#39;il existe et si forceReload==True, alors écrase le dataframe caché existant par une nouvelle version) Lit le dataframe caché correspondant avec les **options et retourne le dataframe. Examples -- &gt;&gt;&gt; filename = &#39;/mnt/z/data/Stam-CC/ExportData 25625.xlsx&#39; forceReload = False option={&#39;dayfirst&#39;:True, &#39;parse_dates&#39;:[&#39;Fecha de Medida&#39;, &#39;Fecha de Fabricacion&#39;], &#39;sheetname&#39;:0} getCachedXLSRaw(filename, forceRelead, **option).info() Parameters - filename : string Emplacement du fichier XLS. Avec l&#39;extension. Format complet Ex: &#39;/mnt/z/data/Stam-CC/ExportData 25625.xlsx&#39; forceReload : boolean, optional, default value = False Si forceReload == True, le fichier sera relu et sauvé même s&#39;il existe déjà en cache options : **keyword args, optional Arguments de lecture du fichier XLS : https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html Ex: sheetname=1 Returns - dataframe Dataframe correspondant &quot;&quot;&quot; #split pour ne garder que le nom sans le chemin de filename : Stam-CC/ExportData 25625 --&gt; ExportData 25625 dataframe_filename = os.path.dirname(filename)+&#39;/&#39;+os.path.basename(filename)+&#39;.csv&#39; #bug de pandas.to_csv quand il y a des espaces ? dataframe_filename = dataframe_filename.replace(&quot; &quot;, &quot;_&quot;) dataframe=[] xls_toget = False #print(dataframe_filename) if (forceReload and os.path.exists(dataframe_filename)): print(&quot;Cached file &quot;+dataframe_filename+&quot; déjà existant mais forceReload=True - FORCE RELOAD&quot;) xls_toget = True if (not os.path.exists(dataframe_filename)): print(&quot;Cached file &quot;+dataframe_filename+&quot; inexistant - read_CachedXLS&quot;) xls_toget = True if (xls_toget): dataframe = pd.read_excel(filename, **options) dataframe.to_csv(dataframe_filename) else: print(&quot;Cached file &quot;+dataframe_filename+&quot; existe en cache, relecture&quot;) #index_col pour ignorer les n° de lignes excel options[&#39;sep&#39;]=&#39;,&#39; options[&#39;decimal&#39;]=&#39;.&#39; options[&#39;skiprows&#39;]=0 options.pop(&#39;sheet_name&#39;) dataframe = pd.read_csv(dataframe_filename,**options) return dataframe . option={&#39;sheet_name&#39;:0} read_CachedXLS(big_excel_file, **option) print(&quot;et voila&quot;) . Cached file /mnt/z/data//pandas-caching/big_excel_file.xlsx.csv existe en cache, relecture et voila .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/fast-read-excel-pandas.html",
            "relUrl": "/blog/fast-read-excel-pandas.html",
            "date": " • Sep 14, 2020"
        }
        
    
  
    
        ,"post78": {
            "title": "Git push to github without password",
            "content": "By default everytime I push to github, I have a prompt asking for password. . (xgboost) guillaume@LL11LPC0PQARQ:~/git/guillaume_blog$ git push Username for &#39;https://github.com&#39;: castorfou Password for &#39;https://castorfou@github.com&#39;: Everything up-to-date . Would be great if I could leverage ssh keys to authenticate. . Update remote from https to ssh . From https://stackoverflow.com/questions/14762034/push-to-github-without-a-password-using-ssh-key, . For example, a GitHub project like Git will have an HTTPS URL: https://github.com/&lt;Username&gt;/&lt;Project&gt;.git And the SSH one: git@github.com:&lt;Username&gt;/&lt;Project&gt;.git You can do: git remote set-url origin git@github.com:&lt;Username&gt;/&lt;Project&gt;.git to change the URL. . In my case I have . (xgboost) guillaume@LL11LPC0PQARQ:~/git/guillaume_blog$ git remote -v origin https://github.com/castorfou/guillaume_blog.git (fetch) origin https://github.com/castorfou/guillaume_blog.git (push) . I have just to modify: . git remote set-url origin git@github.com:castorfou/guillaume_blog.git . Results . It looks like working: . (xgboost) guillaume@LL11LPC0PQARQ:~/git/guillaume_blog$ git push Counting objects: 4, done. Delta compression using up to 8 threads. Compressing objects: 100% (4/4), done. Writing objects: 100% (4/4), 1.59 KiB | 812.00 KiB/s, done. Total 4 (delta 2), reused 0 (delta 0) remote: Resolving deltas: 100% (2/2), completed with 2 local objects. To github.com:castorfou/guillaume_blog.git 4013108..ae78b99 master -&gt; master . Drawback: doesn&#39;t work behing a firewall . . To find a solution to use a proxy . Here are 2 ways to be tested: https://stackoverflow.com/questions/1728934/accessing-a-git-repository-via-ssh-behind-a-firewall https://stackoverflow.com/questions/18604719/how-to-configure-git-to-clone-repo-from-github-behind-a-proxy-server?noredirect=1&amp;lq=1 .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/git-commit-without-password.html",
            "relUrl": "/blog/git-commit-without-password.html",
            "date": " • Sep 11, 2020"
        }
        
    
  
    
        ,"post79": {
            "title": "Blog from jupyter notebook",
            "content": "That will be great if I can simply write blog entries using Jupyter Notebook. . I usually paste inner images into jupyter cells. But this feature is not available yet into fastpages. So for the moment I won&#39;t include images into these posts. . That way I could simply use markdown and insert images . And directly see rendered impact before commiting and pushing to my blog. . get local repo from github . As I am behind a proxy most of my time when working from office, the easiest way for me is to work from WSL. . WSL . I won&#39;t detail how to install WSL on Windows. . I use ubuntu images (18.04) on my PC. . set unset proxy in WSL . I have just added some bash commands at the end of my .bashrc file. . # Set Proxy function setproxy() { export {http,https,ftp}_proxy=&quot;http://&lt;my proxy ip address&gt;:80&quot; export {HTTP,HTTPS,FTP}_PROXY=&quot;http://&lt;my proxy ip address&gt;:80&quot; } # Unset Proxy function unsetproxy() { unset {http,https,ftp}_proxy unset {HTTP,HTTPS,FTP}_PROXY } . git clone castorfou.github.io . I keep most of my local repos under ~/git/ . cd ~/git setproxy git clone https://github.com/castorfou/castorfou.github.io.git . create a blog entry with Jupyter Notebook . commit and push to github . (base) guillaume@LL11LPC0PQARQ:~$ cd git (base) guillaume@LL11LPC0PQARQ:~/git$ cd castorfou.github.io/ (base) guillaume@LL11LPC0PQARQ:~/git/castorfou.github.io$ git add . (base) guillaume@LL11LPC0PQARQ:~/git/castorfou.github.io$ git commit -m &#39;new blog entry: blog from jupyter&#39; [master 6b7460a] new blog entry: blog from jupyter 4 files changed, 516 insertions(+) create mode 100644 _posts/.ipynb_checkpoints/2020-09-10-blog-from-jupyter-checkpoint.ipynb create mode 100644 _posts/2020-09-10-blog-from-jupyter.ipynb create mode 100644 _posts/2020-09-10-blog-from-jupyter.py create mode 100644 _posts/Untitled.txt (base) guillaume@LL11LPC0PQARQ:~/git/castorfou.github.io$ git push fatal: unable to access &#39;https://github.com/castorfou/castorfou.github.io.git/&#39;: gnutls_handshake() failed: The TLS connection was non-properly terminated. . error: gnutls_handshake() failed: The TLS connection was non-properly terminated. . Just googling this error gives some insight: https://github.community/t/unable-to-push-to-repo-gnutls-handshake-failed/885 . It is likely some local firewell issue. . . To be fixed later . switch to mobile wifi without need of proxy . (base) guillaume@LL11LPC0PQARQ:~/git/castorfou.github.io$ unsetproxy (base) guillaume@LL11LPC0PQARQ:~/git/castorfou.github.io$ git push Username for &#39;https://github.com&#39;: castorfou Password for &#39;https://castorfou@github.com&#39;: Counting objects: 7, done. Delta compression using up to 8 threads. Compressing objects: 100% (6/6), done. Writing objects: 100% (7/7), 141.26 KiB | 10.09 MiB/s, done. Total 7 (delta 1), reused 0 (delta 0) remote: Resolving deltas: 100% (1/1), completed with 1 local object. remote: remote: GitHub found 3 vulnerabilities on castorfou/castorfou.github.io&#39;s default branch (2 high, 1 moderate). To find out more, visit: remote: https://github.com/castorfou/castorfou.github.io/network/alerts remote: To https://github.com/castorfou/castorfou.github.io.git 6adeb02..6b7460a master -&gt; master . check entries into blog . double entries . Double entries: one for the notebook (.ipynb) and one for the auto python export (.py). I will have to update my jupyter settings to avoid this python file creation. In the meantime I can just delete the python file, and commit. . . Change settings of jupyter + .gitignore to avoid these double entries . cannot open notebook into browser . Clicking just ask me to download the notebook, it doesn&#39;t display it into the browser. . checking .gitignore . Just by looking into .gitignore, there is an interesting entry: . *.swp ~* *~ _site .sass-cache .jekyll-cache .jekyll-metadata vendor _notebooks/.ipynb_checkpoints . Wait what is in this last line. . Let&#39;s create _notebooks directory and move my notebook in that directory. . notebooks from _notebooks not rendered . No entries, I guess there is some additional settings to do... . . Why notebooks are not rendered by Jekyl . test entry from md using local repo . There is no problem with that. . Creating a local md file in _poststhen pushing to github is creating the right entry blog. . following fastpages troubleshooting guide . upgrade fastpages . Try the automated upgrade as described in https://github.com/fastai/fastpages/blob/master/_fastpages_docs/UPGRADE.md . Unfortunately I don&#39;t see . I have to follow the manual upgrade. . manual fastpages upgrade . I am surprised because the 1st step from manual upgrade is to copy the fastpages repo. It is what I did 2 days ago. I doubt having an outdated version of fastpages. . fastai forum: fastpages category . I will browse through nbdev &amp; faspages category in fastai forums. I should see people with the same issue. . I have created an entry, into fastai forums: Fastpages - cannot see build process of GitHub Actions . And quite immediately Hamel Hussain answered guiding to the write direction: . I misread the Settings instruction: my github repo should explicitely NOT include my github username and I did exactly the opposite. . . I have to create a new repo: guillaume_blog . nothing visible from Actions tab . And another surprising subject: at github in Actions tab. I have a kind of default page. I expect something like an execution journal of Actions. . Page build failure . Received a notification by email: . The page build failed for the master branch with the following error: . Page build failed. For more information, see https://docs.github.com/github/working-with-github-pages/troubleshooting-jekyll-build-errors-for-github-pages-sites#troubleshooting-build-errors. . For information on troubleshooting Jekyll see: . https://docs.github.com/articles/troubleshooting-jekyll-builds . If you have any questions you can submit a request on the Contact GitHub page at https://support.github.com/contact?repo_id=293820308&amp;page_build_id=202240535 . Move to another repo . repo creation . It was just a matter of creating a new repo: . actions monitoring . Monitoring is effective . merge pull request . actions around ssh keys . Following the steps: . Create keys using ssh utility | Enter Secret Key | Enter Deploy Key | . merge PR . There are conflicts to be fixed before that. . And it works: https://castorfou.github.io/guillaume_blog/ . Get local repo . cd ~/git unsetproxy git clone https://github.com/castorfou/guillaume_blog.git .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/blog-from-jupyter-sans-images.html",
            "relUrl": "/blog/blog-from-jupyter-sans-images.html",
            "date": " • Sep 10, 2020"
        }
        
    
  
    
        ,"post80": {
            "title": "Blogging from github",
            "content": "Blogging from github . fastai and fastpages . I am a big fan of fastai’s spirit and even more of their leaders: . Jeremy Howards | Rachel Thomas | Sylvain Gugger who is know at huggingface. | . They are commited to beautiful ideas, and are inspiring people. I like their courses. I like their softwares. For sure I will discuss about fastai. They have created fastpages. It turns github into a blogging platform. I don’t have the full detail but it is explained in fastpages github repo It is based on github actions, and by just creating a repo from a fastpages template https://github.com/fastai/fast_template/generate and giving a couple of settings, you are ready to go. . And here I have to thank Hamel Husain. He is from github company and I think he is behing github actions and helped fastai to release fastpages. I don’t know Hamel but he looks like a humble, terribly skilled guy, with tons of energy. Thanks Hamel. . my blog . My main audience is the future me. (maybe not entirely true otherwise I would have written in French) In 1 year, I want to turn back to this blog and I would like to see all the learning peaces I went through. I want this platform to be as easy as possible. . fastpages . For the moment it cannot be easier. I have setup the about page. And each blog entry is just a new markdown page into _posts. github _posts: . . By commiting this page, there are internal actions being run automatically (through github actions magic) and after a couple of minutes the new blog pages are generated (using Jekyl and ruby if I am not wrong). For the moment I use github web interface. But I guess it is easier to have a local repo of my blog, create new entries and when satisfied git push to github. (to be tested later) . github accounts . For a reason I used my personal github account (guillaume.ramelet@gmail.com) and not my professional one (guillaume.ramelet@michelin.com). I will see later if I have to move to another account. I had some troubles to setup actions into github. For a reason I thought it was available only for organization account. So I have turned my michelin github account to an organization, and I cannot login anymore. To be fixed later. . markdown . Ok I am not a huge fan of markdown. I use it as a basic text system specially within notebooks. But it is not as easy to insert images. Currently I screenshot what I want to share, insert into images folder of my repo and reference this image from my blog post using markdown language. I definitely have to improve my practice of markdown, and there are multiple cheatsheets to be used. . jupyter . There are options within fastpages to blog from jupyter notebooks. I have to do it. My intent will be to use this place to share my knowledge. Today most of my knowledge comes from experiences I make within jupyter. If I could directly blog from that it will be great. . comments . OK as the sole reader this is maybe a minor concern but there is no commenting system associated with fastpages. I cannot get any feedback from these entries. Would love to get advices, create discussions within that blog. Not for today. .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/blogging-from-github.html",
            "relUrl": "/blog/blogging-from-github.html",
            "date": " • Sep 9, 2020"
        }
        
    
  
    
        ,"post81": {
            "title": "Becoming a datascientist",
            "content": "March 2019 . This journey has started about 1 year ago. . No wait, that was dormant for a long time before that. I guess I have to go back to my studying time: at that time my days were full of maths and computers. And my days were flying as crazy. It happens to me (to you?) when you’re just in a middle of something you like very much. 10 hours looks like 1. And the opposite is true as well. . 2000 - 2004: software development . Most of my days and weekends at that time were dedicated to code in Java and bash. Java mainly for server-side developpement in J2EE at Unilog Management. Bash from time to time to automate some tasks on my personal PC. At that time it was mainly about learning what is an operating system. I had started with LFS (Linux From Scratch). And in 2002 with Gentoo which was a much more powerful way to mimic LFS. . 2004 - 2009: project management . Strange period. I don’t remember exactly why but I had a shift in my professional orientation. I moved away from software development and turned into a project manager. In 2005 I entered into Michelin company. And sofwtare technical matters at that time were considered as unimportant (and embarrassing subjects) Fortunately in 2009 I have been started my agile journey. A lot to learn, and it was less about software than human relations and empathy. It was like a start from scratch. . 2009 - 2015: agile journey . Quite a new world for me. I had some basic knowledge by following Jono Bacon. At that time he was a community manager|release leader at Ubuntu. And was reporting progress using burndown charts. In 2009 I launched a project to create an employee portal (closed to what netvibes and igoogle were at that time). Using standard java portal technologies and more importantly using agile approach. A lot to learn about Agile, Scrum, and endless discussions about how to introduce Agile into a non-Agile organization. In 2010 I started another more ambitious project, with many colleagues (~30 persons) and a vague vision. It was about to create a product lifecycle management solution for semi-finished products. . 2015 - 2017: lean journey . In 2015, I met lean approaches for office. I was immediately convinced there was powerful and deep roots within lean. And it could bring a lot to people and organizations. I turned into a lean coach, to work with teams identifying what they could improve, how they could work better, with more pleasure. . 2017 - 2019: Welcome to USA! . Nice opportunity at that time to move from Clermont-Ferrand (France) to Greenville, South Carolina (USA). I have loved every part of it. Except maybe that 2 years were too short to make a full tour of this amazing country. It is crazy to think how different we are when we look like the same. . 2019 - : back to France and turning as a datascientist . Sept 2019 - back to France and for 4 months to prepare for a complete new position: datascientist for Manufacturing within Michelin. I spent many days to learn from various sources specially datacamp and Andrew Ng. That was just the beginning. My intent was to move away from project management, team leadership and focus about what I can do by myself. I wanted to return to math domains without giving up an IT landscape. My colleague Francois Deheeger told me about data science and Artificial Intelligence. That looked as interesting as terrifying. I was in. I was not afraid to learn a new language, and to restart my career from scratch. .",
            "url": "https://castorfou.github.io/guillaume_blog/blog/becoming-datascientist.html",
            "relUrl": "/blog/becoming-datascientist.html",
            "date": " • Sep 8, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "About . This blog . This is more a journal where I am adding entries about my (baby steps) learnings. It is likely to be centered around python, git, data-science, … I have been strongly inspired by Rachel Thomas explaining why I should blog. Specially when starting such a journey to turn a datascientist. . My intent would be to regularly add entries to this blog. Ideally at least once a week. Maybe only short ones, the point being to stick on this frequent activity. If it takes days to write posts I am pretty sure I won’t do it. Those entries are personnal thoughts and not those of my employer Michelin. . Me . I am Guillaume Ramelet. I am 44 (in 2020). Father of 3. And have been working for a French tire company for 15+ years. I am French and as you can read English is not my mother tongue (but using English is a good exercise, isn’t it) .",
          "url": "https://castorfou.github.io/guillaume_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  

  

  

  

  
  

  
      ,"page15": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://castorfou.github.io/guillaume_blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}